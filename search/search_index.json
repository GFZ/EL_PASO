{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ELaborative Particle Analysis from Satellite Observations (EL-PASO)","text":"<p><code>el_paso</code> is a framework for downloading, processing, and saving particle observations from satellite missions. Its main goal is providing a framework for saving satellite data in a standardized way useful for radiation belt modelling.</p> <p>It was developed as an Incubator Project funded by NFDI4Earth.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Capable of handling different input formats (cdf, netcdf, h5, ascii, json)</li> <li>Processing functions most commonly used for analyzing particle measurements are available</li> <li>Metadata associating with the processing are storred alongside data</li> <li>Saving processed data in different standards (e.g. PRBEM) to enable easy loading of processed data</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Examples can be found in the examples folder and include processing scripts for Van Allen Probes, Arase, and POES.</p>"},{"location":"API_reference/download/","title":"Download","text":""},{"location":"API_reference/download/#el_paso.download.download","title":"<code>el_paso.download.download</code>","text":"<p>Download satellite data files within a specified time range and cadence.</p> <p>Examples can be found in the 'examples' and 'tutorials' folder.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>datetime</code> <p>The start of the time range for downloading files. Must be timezone-aware (UTC).</p> required <code>end_time</code> <code>datetime</code> <p>The end of the time range for downloading files. Must be timezone-aware (UTC).</p> required <code>save_path</code> <code>str | Path</code> <p>Directory path where downloaded files will be saved.</p> required <code>file_cadence</code> <code>Literal['daily', 'monthly', 'single_file']</code> <p>Frequency of file downloads. - \"daily\": Download files for each day in the range. - \"monthly\": Download files for each month in the range. - \"single_file\": Download a single file.</p> required <code>download_url</code> <code>str</code> <p>Base URL for downloading files.</p> required <code>file_name_stem</code> <code>str</code> <p>Stem for the file name to be downloaded.</p> required <code>download_arguments_prefixes</code> <code>str</code> <p>Additional arguments to prefix to the download command                                          (used with wget). Defaults to \"\".</p> <code>''</code> <code>download_arguments_suffixes</code> <code>str</code> <p>Additional arguments to suffix to the download command                                          (used with wget). Defaults to \"\".</p> <code>''</code> <code>method</code> <code>Literal['request', 'wget']</code> <p>Download method to use. Either \"request\" (Python requests) or                                            \"wget\" (system wget). Defaults to \"request\".</p> <code>'request'</code> <code>authentification_info</code> <code>tuple[str, str]</code> <p>Tuple of (username, password) for authentication.                                                Defaults to (\"\", \"\").</p> <code>('', '')</code> <code>rename_file_name_stem</code> <code>str | None</code> <p>If provided, rename the downloaded file to this stem.                                           Defaults to None.</p> <code>None</code> <code>skip_existing</code> <code>bool</code> <p>If True, skip downloading files that already exist. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If \"monthly\" cadence or an unsupported cadence is specified.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>el_paso/download.py</code> <pre><code>@timed_function()\ndef download(start_time: datetime,\n             end_time: datetime,\n             save_path: str|Path,\n             file_cadence: Literal[\"daily\", \"monthly\", \"single_file\"],\n             download_url: str,\n             file_name_stem: str,\n             download_arguments_prefixes: str = \"\",\n             download_arguments_suffixes: str = \"\",\n             method:Literal[\"request\", \"wget\"]=\"request\",\n             authentification_info:tuple[str,str]=(\"\",\"\"),\n             rename_file_name_stem: str|None = None,\n             *,\n             skip_existing: bool = True) -&gt; None:\n    \"\"\"Download satellite data files within a specified time range and cadence.\n\n    Examples can be found in the 'examples' and 'tutorials' folder.\n\n    Args:\n        start_time (datetime): The start of the time range for downloading files. Must be timezone-aware (UTC).\n        end_time (datetime): The end of the time range for downloading files. Must be timezone-aware (UTC).\n        save_path (str | Path): Directory path where downloaded files will be saved.\n        file_cadence (Literal[\"daily\", \"monthly\", \"single_file\"]): Frequency of file downloads.\n            - \"daily\": Download files for each day in the range.\n            - \"monthly\": Download files for each month in the range.\n            - \"single_file\": Download a single file.\n        download_url (str): Base URL for downloading files.\n        file_name_stem (str): Stem for the file name to be downloaded.\n        download_arguments_prefixes (str, optional): Additional arguments to prefix to the download command\n                                                     (used with wget). Defaults to \"\".\n        download_arguments_suffixes (str, optional): Additional arguments to suffix to the download command\n                                                     (used with wget). Defaults to \"\".\n        method (Literal[\"request\", \"wget\"], optional): Download method to use. Either \"request\" (Python requests) or\n                                                       \"wget\" (system wget). Defaults to \"request\".\n        authentification_info (tuple[str, str], optional): Tuple of (username, password) for authentication.\n                                                           Defaults to (\"\", \"\").\n        rename_file_name_stem (str | None, optional): If provided, rename the downloaded file to this stem.\n                                                      Defaults to None.\n        skip_existing (bool, optional): If True, skip downloading files that already exist. Defaults to True.\n\n    Raises:\n        NotImplementedError: If \"monthly\" cadence or an unsupported cadence is specified.\n\n    Returns:\n        None\n\n    \"\"\"\n    start_time = enforce_utc_timezone(start_time)\n    end_time = enforce_utc_timezone(end_time)\n\n    save_path = Path(save_path)\n\n    curr_time = start_time\n\n    while True:\n        match method:\n            case \"request\":\n                _requests_download(curr_time,\n                                    save_path,\n                                    download_url,\n                                    file_name_stem,\n                                    authentification_info,\n                                    rename_file_name_stem,\n                                    skip_existing=skip_existing)\n            case \"wget\":\n                _wget_download(curr_time,\n                                save_path,\n                                download_url,\n                                download_arguments_prefixes,\n                                download_arguments_suffixes)\n\n        match file_cadence:\n            case \"daily\":\n                if curr_time &gt; end_time:\n                    break\n                curr_time += timedelta(days=1)\n\n            case \"monthly\":\n                msg = \"Monthly file cadence has not been implemented yet!\"\n                raise NotImplementedError(msg)\n\n            case \"single_file\":\n                break\n\n            case _:\n                msg = \"File cadence must be 'single_file', 'daily', or 'monthly'\"\n                raise NotImplementedError(msg)\n</code></pre>"},{"location":"API_reference/extract_variables_from_files/","title":"Extract variables from files","text":""},{"location":"API_reference/extract_variables_from_files/#el_paso.extract_variables_from_files.extract_variables_from_files","title":"<code>el_paso.extract_variables_from_files.extract_variables_from_files</code>","text":"<p>Extract variable data from files with any file format.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>datetime</code> <p>The start time for data extraction.</p> required <code>end_time</code> <code>datetime</code> <p>The end time for data extraction.</p> required <code>file_cadence</code> <code>Literal['daily', 'monthly', 'single_file']</code> <p>The cadence at which files are organized.</p> required <code>data_path</code> <code>Path or str</code> <p>The directory path where data files are stored.</p> required <code>file_name_stem</code> <code>str</code> <p>The stem of the file name to match files.</p> required <code>extraction_infos</code> <code>Iterable[ExtractionInfo]</code> <p>Information about which variables to extract and how.</p> required <code>pd_read_csv_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to pandas.read_csv.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Variable]</code> <p>dict[str, Variable]: A dictionary mapping result keys to extracted Variable objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no files are found for extraction.</p> Source code in <code>el_paso/extract_variables_from_files.py</code> <pre><code>def extract_variables_from_files(start_time: datetime,\n                                 end_time: datetime,\n                                 file_cadence: Literal[\"daily\", \"monthly\", \"single_file\"],\n                                 data_path: Path|str,\n                                 file_name_stem: str,\n                                 extraction_infos: Iterable[ExtractionInfo],\n                                 pd_read_csv_kwargs: dict[str, Any]|None=None,\n                                 ) -&gt; dict[str, Variable]:\n    \"\"\"Extract variable data from files with any file format.\n\n    Args:\n        start_time (datetime): The start time for data extraction.\n        end_time (datetime): The end time for data extraction.\n        file_cadence (Literal[\"daily\", \"monthly\", \"single_file\"]): The cadence at which files are organized.\n        data_path (Path or str): The directory path where data files are stored.\n        file_name_stem (str): The stem of the file name to match files.\n        extraction_infos (Iterable[ExtractionInfo]): Information about which variables to extract and how.\n        pd_read_csv_kwargs (dict[str, Any], optional): Additional keyword arguments to pass to pandas.read_csv.\n\n    Returns:\n        dict[str, Variable]: A dictionary mapping result keys to extracted Variable objects.\n\n    Raises:\n        ValueError: If no files are found for extraction.\n\n    \"\"\"\n    logger.info(\"Extracting variables ...\")\n\n    if pd_read_csv_kwargs is None:\n        pd_read_csv_kwargs = {}\n\n    start_time = enforce_utc_timezone(start_time)\n    end_time = enforce_utc_timezone(end_time)\n\n    data_path = Path(data_path)\n\n    files_list, _ = _construct_file_list(start_time, end_time, file_cadence, data_path / file_name_stem)\n\n    if len(files_list) == 0:\n        msg = \"No file found to extract variables!\"\n        raise ValueError(msg)\n\n    variable_data = _extract_data_from_files(files_list, extraction_infos, pd_read_csv_kwargs)\n\n    # create variables based on the extraction_infos\n    variables:dict[str, Variable] = {}\n\n    for info in extraction_infos:\n        if info.result_key is None:\n            if isinstance(info.name_or_column, str):\n                dict_key = info.name_or_column\n            else:\n                msg = \"Result key cannot be inferred from a integer column! Please provide a result_key!\"\n                raise ValueError(msg)\n        else:\n            dict_key = info.result_key\n        variables[dict_key] = Variable(original_unit=info.unit, data=variable_data[info.name_or_column])\n        variables[dict_key].metadata.source_files = [path.name for path in files_list]\n\n    return variables\n</code></pre>"},{"location":"API_reference/extract_variables_from_files/#el_paso.extract_variables_from_files.ExtractionInfo","title":"<code>el_paso.extract_variables_from_files.ExtractionInfo</code>  <code>dataclass</code>","text":"<p>Class to store information about the extraction of variables from a file.</p> Source code in <code>el_paso/extract_variables_from_files.py</code> <pre><code>@dataclass(frozen=True, slots=True, eq=False)\nclass ExtractionInfo:\n    \"\"\"Class to store information about the extraction of variables from a file.\"\"\"\n\n    name_or_column: str|int\n    unit: u.UnitBase\n    is_time_dependent: bool = True\n    result_key: str|None = None\n    dependent_variables: list[str]|None = None\n</code></pre>"},{"location":"API_reference/overview/","title":"Overview","text":"<p>This section provides a detailed reference for all modules, classes, and functions in <code>el_paso</code>.</p>"},{"location":"API_reference/overview/#core-classes","title":"Core classes","text":"<p>Variable</p> <p>SavingStrategy</p>"},{"location":"API_reference/overview/#core-functions","title":"Core functions","text":"<p>download</p> <p>extract_variables_from_files</p> <p>save</p>"},{"location":"API_reference/overview/#utilities","title":"Utilities","text":"<p>General utilities</p> <p>Load geomagnetic indices and solar wind parameters</p> <p>Scripts</p> <p>Release mode</p> <p>Units</p>"},{"location":"API_reference/overview/#processing-functions","title":"Processing functions","text":"<p>bin_by_time</p> <p>compute_invariank_K</p> <p>compute_invariank_mu</p> <p>compute_magnetic_field_variables</p> <p>compute_phase_space_density</p> <p>fold_pitch_angles_and_flux</p> <p>convert_string_to_datetime</p>"},{"location":"API_reference/overview/#saving-standards","title":"Saving standards","text":"<p>DataOrgStrategy</p> <p>MonthlyH5Strategy</p> <p>MonthlyNetCDFStrategy</p> <p>SingleFileStrategy</p>"},{"location":"API_reference/overview/#data-standards","title":"Data Standards","text":"<p>DataOrgStandard</p> <p>PRBEMStandard</p>"},{"location":"API_reference/save/","title":"Save","text":""},{"location":"API_reference/save/#el_paso.save","title":"<code>el_paso.save</code>","text":""},{"location":"API_reference/save/#el_paso.save.save","title":"<code>save</code>","text":"<p>Saves variables to files based on the specified saving strategy and time intervals.</p> <p>Parameters:</p> Name Type Description Default <code>variables_dict</code> <code>dict[str, Variable]</code> <p>Dictionary mapping variable names to Variable objects to be saved.</p> required <code>saving_strategy</code> <code>SavingStrategy</code> required <code>start_time</code> <code>datetime</code> <code>None</code> <code>end_time</code> <code>datetime</code> <code>None</code> <code>time_var</code> <code>Variable</code> <code>None</code> <code>append</code> <code>bool</code> <p>Whether to append to existing files if possible. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>UserWarning</code> <p>If saving is attempted but some required variables for an output file are missing.</p> Source code in <code>el_paso/save.py</code> <pre><code>@timed_function()\ndef save(variables_dict: dict[str, Variable],\n         saving_strategy: SavingStrategy,\n         start_time: datetime|None=None,\n         end_time: datetime|None=None,\n         time_var: Variable|None=None,\n         *,\n         append:bool=False) -&gt; None:\n    \"\"\"Saves variables to files based on the specified saving strategy and time intervals.\n\n    Args:\n        variables_dict (dict[str, Variable]):\n            Dictionary mapping variable names to Variable objects to be saved.\n        saving_strategy (SavingStrategy):\n        start_time (datetime):\n        end_time (datetime):\n        time_var (Variable):\n        append (bool, optional):\n            Whether to append to existing files if possible. Defaults to False.\n\n    Returns:\n        None\n\n    Raises:\n        UserWarning:\n            If saving is attempted but some required variables for an output file are missing.\n\n    \"\"\"\n    if start_time is not None and end_time is not None:\n        start_time = enforce_utc_timezone(start_time)\n        end_time = enforce_utc_timezone(end_time)\n\n    time_intervals_to_save = saving_strategy.get_time_intervals_to_save(start_time, end_time)\n\n    for interval_start, interval_end in time_intervals_to_save:\n        for output_file in saving_strategy.output_files:\n            file_path = saving_strategy.get_file_path(interval_start, interval_end, output_file)\n\n            target_variables = saving_strategy.get_target_variables(output_file,\n                                                                    variables_dict,\n                                                                    time_var,\n                                                                    interval_start,\n                                                                    interval_end)\n\n            if target_variables is None:\n                logger.warning(\n                    f\"Saving attempted, but product is missing some required variables for output {output_file.name}!\",\n                    stacklevel=2,\n                )\n            else:\n                data_dict =_get_data_dict_to_save(target_variables)\n                saving_strategy.save_single_file(file_path, data_dict, append=append)\n</code></pre>"},{"location":"API_reference/saving_strategy/","title":"Saving strategy","text":""},{"location":"API_reference/saving_strategy/#el_paso.saving_strategy.SavingStrategy","title":"<code>el_paso.saving_strategy.SavingStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining strategies to save output files with specific time intervals and variables.</p> <p>Attributes:</p> Name Type Description <code>output_files</code> <code>list[OutputFile]</code> <p>List of output files to be managed by the saving strategy.</p> <p>Methods:</p> Name Description <code>get_time_intervals_to_save</code> <p>datetime | None, end_time: datetime | None) -&gt; list[tuple[datetime, datetime]]: Abstract method to determine the time intervals for saving data between start_time and end_time.</p> <code>get_file_path</code> <p>datetime, interval_end: datetime, output_file: OutputFile) -&gt; Path: Abstract method to generate the file path for a given time interval and output file.</p> <code>standardize_variable</code> <p>Variable, name_in_file: str) -&gt; Variable: Abstract method to standardize a variable before saving, possibly renaming or formatting it.</p> <code>get_target_variables</code> <p>OutputFile, variables_dict: dict[str, Variable], time_var: Variable | None,                  start_time: datetime | None, end_time: datetime | None) -&gt; dict[str, Variable] | None: Selects and prepares variables to be saved in the output file, optionally truncating them to a time range.</p> <code>save_single_file</code> <p>Path, dict_to_save: dict[str, Any], *, append: bool = False): Saves the provided dictionary to a file in the specified format (.mat, .pickle, .h5), optionally appending data.</p> <code>append_data</code> <p>Path, dict_to_save: dict[str, Any]) -&gt; dict[str, Any]: Abstract method to append data to an existing file; must be implemented by subclasses.</p> Source code in <code>el_paso/saving_strategy.py</code> <pre><code>class SavingStrategy(ABC):\n    \"\"\"Abstract base class for defining strategies to save output files with specific time intervals and variables.\n\n    Attributes:\n        output_files (list[OutputFile]): List of output files to be managed by the saving strategy.\n\n    Methods:\n        get_time_intervals_to_save(start_time: datetime | None, end_time: datetime | None)\n            -&gt; list[tuple[datetime, datetime]]:\n            Abstract method to determine the time intervals for saving data between start_time and end_time.\n\n        get_file_path(interval_start: datetime, interval_end: datetime, output_file: OutputFile) -&gt; Path:\n            Abstract method to generate the file path for a given time interval and output file.\n\n        standardize_variable(variable: Variable, name_in_file: str) -&gt; Variable:\n            Abstract method to standardize a variable before saving, possibly renaming or formatting it.\n\n        get_target_variables(output_file: OutputFile, variables_dict: dict[str, Variable], time_var: Variable | None,\n                             start_time: datetime | None, end_time: datetime | None) -&gt; dict[str, Variable] | None:\n            Selects and prepares variables to be saved in the output file, optionally truncating them to a time range.\n\n        save_single_file(file_path: Path, dict_to_save: dict[str, Any], *, append: bool = False):\n            Saves the provided dictionary to a file in the specified format (.mat, .pickle, .h5),\n            optionally appending data.\n\n        append_data(file_path: Path, dict_to_save: dict[str, Any]) -&gt; dict[str, Any]:\n            Abstract method to append data to an existing file; must be implemented by subclasses.\n    \"\"\"\n\n    output_files:list[OutputFile]\n    dependency_dict: dict[str,list[str]]\n\n    @abstractmethod\n    def get_time_intervals_to_save(self,\n                                   start_time:datetime|None,\n                                   end_time:datetime|None) -&gt; list[tuple[datetime, datetime]]:\n        \"\"\"Generates a list of time intervals to save between the specified start and end times.\n\n        Args:\n            start_time (datetime | None): The starting datetime for the intervals.\n                                          If None, intervals may start from the earliest available time.\n            end_time (datetime | None): The ending datetime for the intervals.\n                                        If None, intervals may end at the latest available time.\n\n        Returns:\n            list[tuple[datetime, datetime]]: A list of tuples, each representing a time interval (start, end)\n                                             to be saved.\n        \"\"\"\n\n    @abstractmethod\n    def get_file_path(self,\n                      interval_start:datetime,\n                      interval_end:datetime,\n                      output_file:OutputFile) -&gt; Path:\n        \"\"\"Generates a file path for saving variables based on the provided interval and output file information.\n\n        Args:\n            interval_start (datetime): The start of the interval for which the file is being generated.\n            interval_end (datetime): The end of the interval for which the file is being generated.\n            output_file (OutputFile): An OutputFile containing the name of the output file,\n                                      and which variables should be saved in this file.\n\n        Returns:\n            Path: The generated file path where the output data should be saved.\n        \"\"\"\n\n    @abstractmethod\n    def standardize_variable(self, variable:Variable, name_in_file:str) -&gt; Variable:\n        \"\"\"Standardizes the given variable according to the specified name in the file.\n\n        Standardization may include checking of units, dimensions, and size consistency.\n\n        Args:\n            variable (Variable): The variable instance to be standardized.\n            name_in_file (str): The name of the variable as it appears in the file.\n\n        Returns:\n            Variable: The standardized variable instance.\n        \"\"\"\n\n    def get_target_variables(self,\n                             output_file:OutputFile,\n                             variables_dict:dict[str,Variable],\n                             time_var:Variable|None,\n                             start_time:datetime|None,\n                             end_time:datetime|None) -&gt; dict[str,Variable]|None:\n        \"\"\"Retrieves and processes target variables for saving based on the specified output file.\n\n        Parameters:\n            output_file (OutputFile): The output file configuration containing variable names to save.\n            variables_dict (dict[str, Variable]): Dictionary mapping variable names to Variable objects.\n            time_var (Variable | None): The time variable used for truncation, if applicable.\n            start_time (datetime | None): The start time for truncating variables, if specified.\n            end_time (datetime | None): The end time for truncating variables, if specified.\n\n        Returns:\n            dict[str, Variable] | None:\n                - A dictionary of processed Variable objects keyed by their names,\n                    or None if any specified variable name is not found in variables_dict.\n\n        Notes:\n            - If no variable names are specified in output_file, all variables in variables_dict are processed.\n            - Variables are deep-copied before processing.\n            - Each variable is standardized using the `standardize_variable` method.\n            - If a requested variable name is not found, a warning is issued and None is returned.\n        \"\"\"\n        target_variables:dict[str,Variable] = {}\n\n        # if no variables have been specified, we save all of them\n        if len(output_file.names_to_save) == 0:\n            for key, var in variables_dict.items():\n                var_to_save = deepcopy(var)\n\n                if start_time is not None and end_time is not None and time_var is not None:\n                    var_to_save.truncate(time_var, start_time.timestamp(), end_time.timestamp())\n                var_to_save = self.standardize_variable(var_to_save, key)\n\n                target_variables[key] = var_to_save\n\n            return target_variables\n\n        for name_to_save in output_file.names_to_save:\n\n            if name_to_save in variables_dict:\n                var_to_save = deepcopy(variables_dict[name_to_save])\n\n                if start_time is not None and end_time is not None and time_var is not None:\n                    var_to_save.truncate(time_var, start_time.timestamp(), end_time.timestamp())\n                var_to_save = self.standardize_variable(var_to_save, name_to_save)\n\n                target_variables[name_to_save] = var_to_save\n            else:\n                msg = f\"Could not find target variable {name_to_save}!\"\n                warnings.warn(msg, stacklevel=2)\n                if output_file.save_incomplete:\n                    target_variables[name_to_save] = Variable(original_unit=u.dimensionless_unscaled, data=np.array([]))\n                else:\n                    return None\n\n        return target_variables\n\n    def save_single_file(self, file_path:Path, dict_to_save:dict[str,Any], *, append:bool=False) -&gt; None:  # noqa: C901\n        \"\"\"Saves variable data to a single file in one of the supported formats (.mat, .pickle, .h5).\n\n        Parameters:\n            file_path (Path): The path to the file where the dictionary will be saved.\n                              The file extension determines the format.\n            dict_to_save (dict[str, Any]): The dictionary containing variable data to save.\n            append (bool, optional): If True and the file exists, appends data to the existing file (if supported).\n                                     Defaults to False.\n\n        Raises:\n            NotImplementedError: If the file format specified by the file extension is not supported.\n\n        Supported formats:\n            - .mat: Saves using scipy.io.savemat.\n            - .pickle: Saves using pickle.dump.\n            - .h5: Saves using h5py, with each key as a dataset (excluding \"metadata\").\n        \"\"\"\n        logger.info(f\"Saving file {file_path.name}...\")\n\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        format_name = file_path.suffix.lower()\n\n        if file_path.exists() and append:\n            dict_to_save = self.append_data(file_path, dict_to_save)\n\n        if format_name == \".mat\":\n            # Save the dictionary into a .mat file\n            savemat(str(file_path), dict_to_save)\n\n        elif format_name == \".pickle\":\n\n            with file_path.open(\"wb\") as file:\n                pickle.dump(dict_to_save, file)\n\n        elif format_name == \".h5\":\n            with h5py.File(file_path, \"w\") as file:\n\n                for path, value in dict_to_save.items():\n\n                    if path == \"metadata\":\n                        continue\n\n                    path_parts = path.split(\"/\")\n                    groups = path_parts[:-1]\n                    dataset_name = path_parts[-1]\n\n                    curr_hierachy = file\n                    for group in groups:\n                        if group not in curr_hierachy:\n                            curr_hierachy = curr_hierachy.create_group(group) # type: ignore[reportUnknownVariableType]\n                        else:\n                            curr_hierachy = typing.cast(\"h5py.Group\", curr_hierachy[group])\n\n                    data_set = curr_hierachy.create_dataset(dataset_name, data=value, compression=\"gzip\", shuffle=True) # type: ignore[reportUnknownMemberType]\n\n                    if path in dict_to_save[\"metadata\"]:\n                        for key, metadata in dict_to_save[\"metadata\"][path].items():\n                            data_set.attrs[key] = metadata\n\n        elif format_name == \".nc\":\n            msg = (\"Encountered format netCDF (.nc). This format has to be implemented by \"\n                   \"each subclass as no general writer exists for it!\")\n            raise NotImplementedError(msg)\n\n        else:\n            msg = f\"The '{format_name}' format is not implemented.\"\n            raise NotImplementedError(msg)\n\n    def append_data(self, file_path:Path, data_dict_to_save:dict[str,Any]) -&gt; dict[str,Any]:\n        \"\"\"Appends variable data from the specified file to the provided dictionary.\n\n        Args:\n            file_path (Path): The path to the file where data should be appended.\n            data_dict_to_save (dict[str, Any]): The dictionary containing data to append.\n\n        Returns:\n            dict[str, Any]: The updated dictionary after appending data.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        msg = \"This has to be overwritten for each Strategy!\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"API_reference/saving_strategy/#el_paso.saving_strategy.SavingStrategy.append_data","title":"<code>append_data</code>","text":"<p>Appends variable data from the specified file to the provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the file where data should be appended.</p> required <code>data_dict_to_save</code> <code>dict[str, Any]</code> <p>The dictionary containing data to append.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The updated dictionary after appending data.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>el_paso/saving_strategy.py</code> <pre><code>def append_data(self, file_path:Path, data_dict_to_save:dict[str,Any]) -&gt; dict[str,Any]:\n    \"\"\"Appends variable data from the specified file to the provided dictionary.\n\n    Args:\n        file_path (Path): The path to the file where data should be appended.\n        data_dict_to_save (dict[str, Any]): The dictionary containing data to append.\n\n    Returns:\n        dict[str, Any]: The updated dictionary after appending data.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    msg = \"This has to be overwritten for each Strategy!\"\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"API_reference/saving_strategy/#el_paso.saving_strategy.SavingStrategy.get_file_path","title":"<code>get_file_path</code>  <code>abstractmethod</code>","text":"<p>Generates a file path for saving variables based on the provided interval and output file information.</p> <p>Parameters:</p> Name Type Description Default <code>interval_start</code> <code>datetime</code> <p>The start of the interval for which the file is being generated.</p> required <code>interval_end</code> <code>datetime</code> <p>The end of the interval for which the file is being generated.</p> required <code>output_file</code> <code>OutputFile</code> <p>An OutputFile containing the name of the output file,                       and which variables should be saved in this file.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The generated file path where the output data should be saved.</p> Source code in <code>el_paso/saving_strategy.py</code> <pre><code>@abstractmethod\ndef get_file_path(self,\n                  interval_start:datetime,\n                  interval_end:datetime,\n                  output_file:OutputFile) -&gt; Path:\n    \"\"\"Generates a file path for saving variables based on the provided interval and output file information.\n\n    Args:\n        interval_start (datetime): The start of the interval for which the file is being generated.\n        interval_end (datetime): The end of the interval for which the file is being generated.\n        output_file (OutputFile): An OutputFile containing the name of the output file,\n                                  and which variables should be saved in this file.\n\n    Returns:\n        Path: The generated file path where the output data should be saved.\n    \"\"\"\n</code></pre>"},{"location":"API_reference/saving_strategy/#el_paso.saving_strategy.SavingStrategy.get_target_variables","title":"<code>get_target_variables</code>","text":"<p>Retrieves and processes target variables for saving based on the specified output file.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>OutputFile</code> <p>The output file configuration containing variable names to save.</p> required <code>variables_dict</code> <code>dict[str, Variable]</code> <p>Dictionary mapping variable names to Variable objects.</p> required <code>time_var</code> <code>Variable | None</code> <p>The time variable used for truncation, if applicable.</p> required <code>start_time</code> <code>datetime | None</code> <p>The start time for truncating variables, if specified.</p> required <code>end_time</code> <code>datetime | None</code> <p>The end time for truncating variables, if specified.</p> required <p>Returns:</p> Type Description <code>dict[str, Variable] | None</code> <p>dict[str, Variable] | None: - A dictionary of processed Variable objects keyed by their names,     or None if any specified variable name is not found in variables_dict.</p> Notes <ul> <li>If no variable names are specified in output_file, all variables in variables_dict are processed.</li> <li>Variables are deep-copied before processing.</li> <li>Each variable is standardized using the <code>standardize_variable</code> method.</li> <li>If a requested variable name is not found, a warning is issued and None is returned.</li> </ul> Source code in <code>el_paso/saving_strategy.py</code> <pre><code>def get_target_variables(self,\n                         output_file:OutputFile,\n                         variables_dict:dict[str,Variable],\n                         time_var:Variable|None,\n                         start_time:datetime|None,\n                         end_time:datetime|None) -&gt; dict[str,Variable]|None:\n    \"\"\"Retrieves and processes target variables for saving based on the specified output file.\n\n    Parameters:\n        output_file (OutputFile): The output file configuration containing variable names to save.\n        variables_dict (dict[str, Variable]): Dictionary mapping variable names to Variable objects.\n        time_var (Variable | None): The time variable used for truncation, if applicable.\n        start_time (datetime | None): The start time for truncating variables, if specified.\n        end_time (datetime | None): The end time for truncating variables, if specified.\n\n    Returns:\n        dict[str, Variable] | None:\n            - A dictionary of processed Variable objects keyed by their names,\n                or None if any specified variable name is not found in variables_dict.\n\n    Notes:\n        - If no variable names are specified in output_file, all variables in variables_dict are processed.\n        - Variables are deep-copied before processing.\n        - Each variable is standardized using the `standardize_variable` method.\n        - If a requested variable name is not found, a warning is issued and None is returned.\n    \"\"\"\n    target_variables:dict[str,Variable] = {}\n\n    # if no variables have been specified, we save all of them\n    if len(output_file.names_to_save) == 0:\n        for key, var in variables_dict.items():\n            var_to_save = deepcopy(var)\n\n            if start_time is not None and end_time is not None and time_var is not None:\n                var_to_save.truncate(time_var, start_time.timestamp(), end_time.timestamp())\n            var_to_save = self.standardize_variable(var_to_save, key)\n\n            target_variables[key] = var_to_save\n\n        return target_variables\n\n    for name_to_save in output_file.names_to_save:\n\n        if name_to_save in variables_dict:\n            var_to_save = deepcopy(variables_dict[name_to_save])\n\n            if start_time is not None and end_time is not None and time_var is not None:\n                var_to_save.truncate(time_var, start_time.timestamp(), end_time.timestamp())\n            var_to_save = self.standardize_variable(var_to_save, name_to_save)\n\n            target_variables[name_to_save] = var_to_save\n        else:\n            msg = f\"Could not find target variable {name_to_save}!\"\n            warnings.warn(msg, stacklevel=2)\n            if output_file.save_incomplete:\n                target_variables[name_to_save] = Variable(original_unit=u.dimensionless_unscaled, data=np.array([]))\n            else:\n                return None\n\n    return target_variables\n</code></pre>"},{"location":"API_reference/saving_strategy/#el_paso.saving_strategy.SavingStrategy.get_time_intervals_to_save","title":"<code>get_time_intervals_to_save</code>  <code>abstractmethod</code>","text":"<p>Generates a list of time intervals to save between the specified start and end times.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>datetime | None</code> <p>The starting datetime for the intervals.                           If None, intervals may start from the earliest available time.</p> required <code>end_time</code> <code>datetime | None</code> <p>The ending datetime for the intervals.                         If None, intervals may end at the latest available time.</p> required <p>Returns:</p> Type Description <code>list[tuple[datetime, datetime]]</code> <p>list[tuple[datetime, datetime]]: A list of tuples, each representing a time interval (start, end)                              to be saved.</p> Source code in <code>el_paso/saving_strategy.py</code> <pre><code>@abstractmethod\ndef get_time_intervals_to_save(self,\n                               start_time:datetime|None,\n                               end_time:datetime|None) -&gt; list[tuple[datetime, datetime]]:\n    \"\"\"Generates a list of time intervals to save between the specified start and end times.\n\n    Args:\n        start_time (datetime | None): The starting datetime for the intervals.\n                                      If None, intervals may start from the earliest available time.\n        end_time (datetime | None): The ending datetime for the intervals.\n                                    If None, intervals may end at the latest available time.\n\n    Returns:\n        list[tuple[datetime, datetime]]: A list of tuples, each representing a time interval (start, end)\n                                         to be saved.\n    \"\"\"\n</code></pre>"},{"location":"API_reference/saving_strategy/#el_paso.saving_strategy.SavingStrategy.save_single_file","title":"<code>save_single_file</code>","text":"<p>Saves variable data to a single file in one of the supported formats (.mat, .pickle, .h5).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the file where the dictionary will be saved.               The file extension determines the format.</p> required <code>dict_to_save</code> <code>dict[str, Any]</code> <p>The dictionary containing variable data to save.</p> required <code>append</code> <code>bool</code> <p>If True and the file exists, appends data to the existing file (if supported).                      Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the file format specified by the file extension is not supported.</p> Supported formats <ul> <li>.mat: Saves using scipy.io.savemat.</li> <li>.pickle: Saves using pickle.dump.</li> <li>.h5: Saves using h5py, with each key as a dataset (excluding \"metadata\").</li> </ul> Source code in <code>el_paso/saving_strategy.py</code> <pre><code>def save_single_file(self, file_path:Path, dict_to_save:dict[str,Any], *, append:bool=False) -&gt; None:  # noqa: C901\n    \"\"\"Saves variable data to a single file in one of the supported formats (.mat, .pickle, .h5).\n\n    Parameters:\n        file_path (Path): The path to the file where the dictionary will be saved.\n                          The file extension determines the format.\n        dict_to_save (dict[str, Any]): The dictionary containing variable data to save.\n        append (bool, optional): If True and the file exists, appends data to the existing file (if supported).\n                                 Defaults to False.\n\n    Raises:\n        NotImplementedError: If the file format specified by the file extension is not supported.\n\n    Supported formats:\n        - .mat: Saves using scipy.io.savemat.\n        - .pickle: Saves using pickle.dump.\n        - .h5: Saves using h5py, with each key as a dataset (excluding \"metadata\").\n    \"\"\"\n    logger.info(f\"Saving file {file_path.name}...\")\n\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    format_name = file_path.suffix.lower()\n\n    if file_path.exists() and append:\n        dict_to_save = self.append_data(file_path, dict_to_save)\n\n    if format_name == \".mat\":\n        # Save the dictionary into a .mat file\n        savemat(str(file_path), dict_to_save)\n\n    elif format_name == \".pickle\":\n\n        with file_path.open(\"wb\") as file:\n            pickle.dump(dict_to_save, file)\n\n    elif format_name == \".h5\":\n        with h5py.File(file_path, \"w\") as file:\n\n            for path, value in dict_to_save.items():\n\n                if path == \"metadata\":\n                    continue\n\n                path_parts = path.split(\"/\")\n                groups = path_parts[:-1]\n                dataset_name = path_parts[-1]\n\n                curr_hierachy = file\n                for group in groups:\n                    if group not in curr_hierachy:\n                        curr_hierachy = curr_hierachy.create_group(group) # type: ignore[reportUnknownVariableType]\n                    else:\n                        curr_hierachy = typing.cast(\"h5py.Group\", curr_hierachy[group])\n\n                data_set = curr_hierachy.create_dataset(dataset_name, data=value, compression=\"gzip\", shuffle=True) # type: ignore[reportUnknownMemberType]\n\n                if path in dict_to_save[\"metadata\"]:\n                    for key, metadata in dict_to_save[\"metadata\"][path].items():\n                        data_set.attrs[key] = metadata\n\n    elif format_name == \".nc\":\n        msg = (\"Encountered format netCDF (.nc). This format has to be implemented by \"\n               \"each subclass as no general writer exists for it!\")\n        raise NotImplementedError(msg)\n\n    else:\n        msg = f\"The '{format_name}' format is not implemented.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"API_reference/saving_strategy/#el_paso.saving_strategy.SavingStrategy.standardize_variable","title":"<code>standardize_variable</code>  <code>abstractmethod</code>","text":"<p>Standardizes the given variable according to the specified name in the file.</p> <p>Standardization may include checking of units, dimensions, and size consistency.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>Variable</code> <p>The variable instance to be standardized.</p> required <code>name_in_file</code> <code>str</code> <p>The name of the variable as it appears in the file.</p> required <p>Returns:</p> Name Type Description <code>Variable</code> <code>Variable</code> <p>The standardized variable instance.</p> Source code in <code>el_paso/saving_strategy.py</code> <pre><code>@abstractmethod\ndef standardize_variable(self, variable:Variable, name_in_file:str) -&gt; Variable:\n    \"\"\"Standardizes the given variable according to the specified name in the file.\n\n    Standardization may include checking of units, dimensions, and size consistency.\n\n    Args:\n        variable (Variable): The variable instance to be standardized.\n        name_in_file (str): The name of the variable as it appears in the file.\n\n    Returns:\n        Variable: The standardized variable instance.\n    \"\"\"\n</code></pre>"},{"location":"API_reference/saving_strategy/#el_paso.saving_strategy.OutputFile","title":"<code>el_paso.saving_strategy.OutputFile</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Represents an output file with its name and a list of variable names to save.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the output file.</p> <code>names_to_save</code> <code>list[str]</code> <p>List of variable names to be saved in the output file.</p> <code>save_incomplete</code> <code>bool</code> <p>If True, allows saving even if some variables are missing.</p> Source code in <code>el_paso/saving_strategy.py</code> <pre><code>class OutputFile(NamedTuple):\n    \"\"\"Represents an output file with its name and a list of variable names to save.\n\n    Attributes:\n        name (str): The name of the output file.\n        names_to_save (list[str]): List of variable names to be saved in the output file.\n        save_incomplete (bool): If True, allows saving even if some variables are missing.\n    \"\"\"\n    name: str\n    names_to_save: list[str]\n    save_incomplete:bool = False\n</code></pre>"},{"location":"API_reference/variable/","title":"Variable","text":""},{"location":"API_reference/variable/#el_paso.variable.Variable","title":"<code>el_paso.variable.Variable</code>","text":"<p>Variable class holding data and metadata.</p> <p>Attributes:</p> Name Type Description <code>_data</code> <code>NDArray[generic]</code> <p>The numerical data of the variable.</p> <code>metadata</code> <code>VariableMetadata</code> <p>An instance of <code>VariableMetadata</code> holding information about the variable.</p> Source code in <code>el_paso/variable.py</code> <pre><code>class Variable:\n    \"\"\"Variable class holding data and metadata.\n\n    Attributes:\n        _data (NDArray[np.generic]): The numerical data of the variable.\n        metadata (VariableMetadata): An instance of `VariableMetadata` holding\n            information about the variable.\n    \"\"\"\n\n    __slots__ = \"_data\", \"metadata\"\n\n    _data:NDArray[np.generic]\n    metadata:VariableMetadata\n\n    def __init__(\n        self,\n        original_unit: u.UnitBase,\n        data:NDArray[np.generic]|None = None,\n        description: str = \"\",\n        processing_notes: str = \"\",\n    ) -&gt; None:\n        \"\"\"Initializes a Variable instance.\n\n        Args:\n            original_unit (u.UnitBase): The original unit of the data.\n            data (NDArray[np.generic] | None): The numerical data. Defaults to an empty\n                numpy array if None.\n            description (str): A description of the variable. Defaults to \"\".\n            processing_notes (str): Notes on how the data was processed. Defaults to \"\".\n        \"\"\"\n        self._data = np.array([], dtype=np.generic) if data is None else data\n\n        self.metadata = VariableMetadata(\n            unit=original_unit,\n            description=description,\n            processing_notes=processing_notes,\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Returns a string representation of the Variable object.\"\"\"\n        return f\"Variable holding {self._data.shape} data points with metadata: {self.metadata}\"\n\n    def convert_to_unit(self, target_unit:u.UnitBase|str) -&gt; None:\n        \"\"\"Converts the data to a given unit.\n\n        Args:\n            target_unit (u.UnitBase | str): The unit the data should be converted to.\n        \"\"\"\n        if isinstance(target_unit, str):\n            target_unit = u.Unit(target_unit)\n\n        if self.metadata.unit != target_unit:\n            data_with_unit = u.Quantity(self._data, self.metadata.unit)\n            self._data = typing.cast(\"NDArray[np.generic]\", data_with_unit.to_value(target_unit)) #type: ignore[reportUnknownMemberType]\n\n            self.metadata.unit = target_unit\n\n    @overload\n    def get_data(self, target_unit:u.UnitBase|str) -&gt; NDArray[np.floating|np.integer]:\n        ...\n\n    @overload\n    def get_data(self, target_unit:None=None) -&gt; NDArray[np.generic]:\n        ...\n\n    def get_data(self, target_unit:u.UnitBase|str|None=None) -&gt; NDArray[np.generic]:\n        \"\"\"Gets the data of the variable.\n\n        Args:\n            target_unit (u.UnitBase | str | None): The unit to convert the data to\n                before returning. If None, the data is returned in its current unit.\n                Defaults to None.\n\n        Returns:\n            NDArray[np.generic]: The data of the variable.\n\n        Raises:\n            TypeError: If `target_unit` is provided and the data is not numeric.\n        \"\"\"\n        if target_unit is None:\n            return self._data\n\n        if isinstance(target_unit, str):\n            target_unit = u.Unit(target_unit)\n\n        if not np.issubdtype(self._data.dtype, np.number):\n            msg = f\"Unit conversion is only supported for numeric types! Encountered for variable {self}.\"\n            raise TypeError(msg)\n\n        return typing.cast(\"NDArray[np.generic]\", u.Quantity(self._data, self.metadata.unit).to_value(target_unit)) #type: ignore[reportUnknownMemberType]\n\n    def set_data(self, data:NDArray[np.generic], unit:Literal[\"same\"]|str|u.UnitBase) -&gt; None:  # noqa: PYI051\n        \"\"\"Sets the data and optionally updates the unit of the variable.\n\n        Args:\n            data (NDArray[np.generic]): The new data array.\n            unit (Literal[\"same\"] | str | u.UnitBase): The unit of the new data.\n                If \"same\", the existing unit is kept. Can be a string representation\n                of a unit or an `astropy.units.UnitBase` object.\n\n        Raises:\n            TypeError: If `unit` is not \"same\", a string, or an `astropy.units.UnitBase` object.\n        \"\"\"\n        self._data = data\n\n        if isinstance(unit, str):\n            if unit != \"same\":\n                self.metadata.unit = u.Unit(unit)\n        elif isinstance(unit, u.UnitBase): #type: ignore[reportUnknownMemberType]\n            self.metadata.unit = unit\n        else:\n            msg = \"unit must be either a str or a astropy unit!\"\n            raise TypeError(msg)\n\n    def transpose_data(self, seq: list[int]|tuple[int,...]) -&gt; None:\n        \"\"\"Transposes the internal data array.\n\n        Args:\n            seq (list[int] | tuple[int, ...]): The axes to transpose to. See\n                `numpy.transpose` for details.\n        \"\"\"\n        self._data = np.transpose(self._data, axes=seq)\n\n    def apply_thresholds_on_data(self, lower_threshold: float = -np.inf, upper_threshold: float = np.inf) -&gt; None:\n        \"\"\"Applies lower and upper thresholds to the data.\n\n        Values outside the thresholds (exclusive) are set to NaN.\n\n        Args:\n            lower_threshold (float): The lower bound for the data. Defaults to\n                negative infinity.\n            upper_threshold (float): The upper bound for the data. Defaults to\n                positive infinity.\n\n        Raises:\n            TypeError: If the data is not numeric.\n        \"\"\"\n        if not np.issubdtype(self._data.dtype, np.number):\n            msg = f\"Thresholds are only supported for numeric types! Encountered for variable {self}.\"\n            raise TypeError(msg)\n        self._data = typing.cast(\"NDArray[np.number]\", self._data)\n\n        self._data = np.where((self._data &gt; lower_threshold) &amp; (self._data &lt; upper_threshold), self._data, np.nan)\n\n    def truncate(self, time_variable:Variable, start_time:float|datetime, end_time:float|datetime) -&gt; None:\n        \"\"\"Truncates the variable's data based on a time variable and a time range.\n\n        Args:\n            time_variable (Variable): A `Variable` object containing the time data.\n            start_time (float | datetime): The start time for truncation. Can be a\n                Unix timestamp (float) or a `datetime` object.\n            end_time (float | datetime): The end time for truncation. Can be a\n                Unix timestamp (float) or a `datetime` object.\n\n        Raises:\n            ValueError: If the length of the variable's data does not match the\n                length of the `time_variable`'s data.\n        \"\"\"\n        if isinstance(start_time, datetime):\n            start_time = enforce_utc_timezone(start_time).timestamp()\n        if isinstance(end_time, datetime):\n            end_time = enforce_utc_timezone(end_time).timestamp()\n\n        if self._data.shape[0] != time_variable.get_data().shape[0]:\n            msg = \"Encountered length missmatch between variable and time variable!\"\n            raise ValueError(msg)\n\n        time_var_data = time_variable.get_data(ep.units.posixtime)\n\n        self._data = self._data[(time_var_data &gt;= start_time) &amp; (time_var_data &lt;= end_time)]\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Computes a hash value for the variable based on its holding data.\n\n        Returns:\n            int: The integer hash value.\n        \"\"\"\n        return hash(self._data.tobytes())\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.__hash__","title":"<code>__hash__</code>","text":"<p>Computes a hash value for the variable based on its holding data.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The integer hash value.</p> Source code in <code>el_paso/variable.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Computes a hash value for the variable based on its holding data.\n\n    Returns:\n        int: The integer hash value.\n    \"\"\"\n    return hash(self._data.tobytes())\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.__init__","title":"<code>__init__</code>","text":"<p>Initializes a Variable instance.</p> <p>Parameters:</p> Name Type Description Default <code>original_unit</code> <code>UnitBase</code> <p>The original unit of the data.</p> required <code>data</code> <code>NDArray[generic] | None</code> <p>The numerical data. Defaults to an empty numpy array if None.</p> <code>None</code> <code>description</code> <code>str</code> <p>A description of the variable. Defaults to \"\".</p> <code>''</code> <code>processing_notes</code> <code>str</code> <p>Notes on how the data was processed. Defaults to \"\".</p> <code>''</code> Source code in <code>el_paso/variable.py</code> <pre><code>def __init__(\n    self,\n    original_unit: u.UnitBase,\n    data:NDArray[np.generic]|None = None,\n    description: str = \"\",\n    processing_notes: str = \"\",\n) -&gt; None:\n    \"\"\"Initializes a Variable instance.\n\n    Args:\n        original_unit (u.UnitBase): The original unit of the data.\n        data (NDArray[np.generic] | None): The numerical data. Defaults to an empty\n            numpy array if None.\n        description (str): A description of the variable. Defaults to \"\".\n        processing_notes (str): Notes on how the data was processed. Defaults to \"\".\n    \"\"\"\n    self._data = np.array([], dtype=np.generic) if data is None else data\n\n    self.metadata = VariableMetadata(\n        unit=original_unit,\n        description=description,\n        processing_notes=processing_notes,\n    )\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.__repr__","title":"<code>__repr__</code>","text":"<p>Returns a string representation of the Variable object.</p> Source code in <code>el_paso/variable.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Returns a string representation of the Variable object.\"\"\"\n    return f\"Variable holding {self._data.shape} data points with metadata: {self.metadata}\"\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.apply_thresholds_on_data","title":"<code>apply_thresholds_on_data</code>","text":"<p>Applies lower and upper thresholds to the data.</p> <p>Values outside the thresholds (exclusive) are set to NaN.</p> <p>Parameters:</p> Name Type Description Default <code>lower_threshold</code> <code>float</code> <p>The lower bound for the data. Defaults to negative infinity.</p> <code>-inf</code> <code>upper_threshold</code> <code>float</code> <p>The upper bound for the data. Defaults to positive infinity.</p> <code>inf</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the data is not numeric.</p> Source code in <code>el_paso/variable.py</code> <pre><code>def apply_thresholds_on_data(self, lower_threshold: float = -np.inf, upper_threshold: float = np.inf) -&gt; None:\n    \"\"\"Applies lower and upper thresholds to the data.\n\n    Values outside the thresholds (exclusive) are set to NaN.\n\n    Args:\n        lower_threshold (float): The lower bound for the data. Defaults to\n            negative infinity.\n        upper_threshold (float): The upper bound for the data. Defaults to\n            positive infinity.\n\n    Raises:\n        TypeError: If the data is not numeric.\n    \"\"\"\n    if not np.issubdtype(self._data.dtype, np.number):\n        msg = f\"Thresholds are only supported for numeric types! Encountered for variable {self}.\"\n        raise TypeError(msg)\n    self._data = typing.cast(\"NDArray[np.number]\", self._data)\n\n    self._data = np.where((self._data &gt; lower_threshold) &amp; (self._data &lt; upper_threshold), self._data, np.nan)\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.convert_to_unit","title":"<code>convert_to_unit</code>","text":"<p>Converts the data to a given unit.</p> <p>Parameters:</p> Name Type Description Default <code>target_unit</code> <code>UnitBase | str</code> <p>The unit the data should be converted to.</p> required Source code in <code>el_paso/variable.py</code> <pre><code>def convert_to_unit(self, target_unit:u.UnitBase|str) -&gt; None:\n    \"\"\"Converts the data to a given unit.\n\n    Args:\n        target_unit (u.UnitBase | str): The unit the data should be converted to.\n    \"\"\"\n    if isinstance(target_unit, str):\n        target_unit = u.Unit(target_unit)\n\n    if self.metadata.unit != target_unit:\n        data_with_unit = u.Quantity(self._data, self.metadata.unit)\n        self._data = typing.cast(\"NDArray[np.generic]\", data_with_unit.to_value(target_unit)) #type: ignore[reportUnknownMemberType]\n\n        self.metadata.unit = target_unit\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.get_data","title":"<code>get_data</code>","text":"<pre><code>get_data\n</code></pre><pre><code>get_data\n</code></pre> <p>Gets the data of the variable.</p> <p>Parameters:</p> Name Type Description Default <code>target_unit</code> <code>UnitBase | str | None</code> <p>The unit to convert the data to before returning. If None, the data is returned in its current unit. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[generic]</code> <p>NDArray[np.generic]: The data of the variable.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>target_unit</code> is provided and the data is not numeric.</p> Source code in <code>el_paso/variable.py</code> <pre><code>def get_data(self, target_unit:u.UnitBase|str|None=None) -&gt; NDArray[np.generic]:\n    \"\"\"Gets the data of the variable.\n\n    Args:\n        target_unit (u.UnitBase | str | None): The unit to convert the data to\n            before returning. If None, the data is returned in its current unit.\n            Defaults to None.\n\n    Returns:\n        NDArray[np.generic]: The data of the variable.\n\n    Raises:\n        TypeError: If `target_unit` is provided and the data is not numeric.\n    \"\"\"\n    if target_unit is None:\n        return self._data\n\n    if isinstance(target_unit, str):\n        target_unit = u.Unit(target_unit)\n\n    if not np.issubdtype(self._data.dtype, np.number):\n        msg = f\"Unit conversion is only supported for numeric types! Encountered for variable {self}.\"\n        raise TypeError(msg)\n\n    return typing.cast(\"NDArray[np.generic]\", u.Quantity(self._data, self.metadata.unit).to_value(target_unit)) #type: ignore[reportUnknownMemberType]\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.set_data","title":"<code>set_data</code>","text":"<p>Sets the data and optionally updates the unit of the variable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[generic]</code> <p>The new data array.</p> required <code>unit</code> <code>Literal['same'] | str | UnitBase</code> <p>The unit of the new data. If \"same\", the existing unit is kept. Can be a string representation of a unit or an <code>astropy.units.UnitBase</code> object.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>unit</code> is not \"same\", a string, or an <code>astropy.units.UnitBase</code> object.</p> Source code in <code>el_paso/variable.py</code> <pre><code>def set_data(self, data:NDArray[np.generic], unit:Literal[\"same\"]|str|u.UnitBase) -&gt; None:  # noqa: PYI051\n    \"\"\"Sets the data and optionally updates the unit of the variable.\n\n    Args:\n        data (NDArray[np.generic]): The new data array.\n        unit (Literal[\"same\"] | str | u.UnitBase): The unit of the new data.\n            If \"same\", the existing unit is kept. Can be a string representation\n            of a unit or an `astropy.units.UnitBase` object.\n\n    Raises:\n        TypeError: If `unit` is not \"same\", a string, or an `astropy.units.UnitBase` object.\n    \"\"\"\n    self._data = data\n\n    if isinstance(unit, str):\n        if unit != \"same\":\n            self.metadata.unit = u.Unit(unit)\n    elif isinstance(unit, u.UnitBase): #type: ignore[reportUnknownMemberType]\n        self.metadata.unit = unit\n    else:\n        msg = \"unit must be either a str or a astropy unit!\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.transpose_data","title":"<code>transpose_data</code>","text":"<p>Transposes the internal data array.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>list[int] | tuple[int, ...]</code> <p>The axes to transpose to. See <code>numpy.transpose</code> for details.</p> required Source code in <code>el_paso/variable.py</code> <pre><code>def transpose_data(self, seq: list[int]|tuple[int,...]) -&gt; None:\n    \"\"\"Transposes the internal data array.\n\n    Args:\n        seq (list[int] | tuple[int, ...]): The axes to transpose to. See\n            `numpy.transpose` for details.\n    \"\"\"\n    self._data = np.transpose(self._data, axes=seq)\n</code></pre>"},{"location":"API_reference/variable/#el_paso.variable.Variable.truncate","title":"<code>truncate</code>","text":"<p>Truncates the variable's data based on a time variable and a time range.</p> <p>Parameters:</p> Name Type Description Default <code>time_variable</code> <code>Variable</code> <p>A <code>Variable</code> object containing the time data.</p> required <code>start_time</code> <code>float | datetime</code> <p>The start time for truncation. Can be a Unix timestamp (float) or a <code>datetime</code> object.</p> required <code>end_time</code> <code>float | datetime</code> <p>The end time for truncation. Can be a Unix timestamp (float) or a <code>datetime</code> object.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of the variable's data does not match the length of the <code>time_variable</code>'s data.</p> Source code in <code>el_paso/variable.py</code> <pre><code>def truncate(self, time_variable:Variable, start_time:float|datetime, end_time:float|datetime) -&gt; None:\n    \"\"\"Truncates the variable's data based on a time variable and a time range.\n\n    Args:\n        time_variable (Variable): A `Variable` object containing the time data.\n        start_time (float | datetime): The start time for truncation. Can be a\n            Unix timestamp (float) or a `datetime` object.\n        end_time (float | datetime): The end time for truncation. Can be a\n            Unix timestamp (float) or a `datetime` object.\n\n    Raises:\n        ValueError: If the length of the variable's data does not match the\n            length of the `time_variable`'s data.\n    \"\"\"\n    if isinstance(start_time, datetime):\n        start_time = enforce_utc_timezone(start_time).timestamp()\n    if isinstance(end_time, datetime):\n        end_time = enforce_utc_timezone(end_time).timestamp()\n\n    if self._data.shape[0] != time_variable.get_data().shape[0]:\n        msg = \"Encountered length missmatch between variable and time variable!\"\n        raise ValueError(msg)\n\n    time_var_data = time_variable.get_data(ep.units.posixtime)\n\n    self._data = self._data[(time_var_data &gt;= start_time) &amp; (time_var_data &lt;= end_time)]\n</code></pre>"},{"location":"API_reference/data_standards/data_org/","title":"DataOrg Standard","text":""},{"location":"API_reference/data_standards/data_org/#el_paso.data_standards.data_org_standard.DataOrgStandard","title":"<code>el_paso.data_standards.data_org_standard.DataOrgStandard</code>","text":"<p>               Bases: <code>DataStandard</code></p> <p>A data standard used historically at the GFZ German Research Centre for Geosciences.</p> <p>This standard defines rules for a set of canonical variable names by converting them to correct units and checking their array dimensions for consistency. It is tailored for compatibility with historical GFZ datasets and internal workflows.</p> Source code in <code>el_paso/data_standards/data_org_standard.py</code> <pre><code>class DataOrgStandard(DataStandard):\n    \"\"\"A data standard used historically at the GFZ German Research Centre for Geosciences.\n\n    This standard defines rules for a set of canonical variable names by converting them\n    to correct units and checking their array dimensions for consistency. It is tailored\n    for compatibility with historical GFZ datasets and internal workflows.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the DataOrgStandard with a ConsistencyCheck object.\"\"\"\n        self.consistency_check = ConsistencyCheck()\n\n    def standardize_variable(self, standard_name: str, variable: ep.Variable) -&gt; ep.Variable:  # noqa: C901, PLR0912, PLR0915\n        \"\"\"Standardizes a variable based on its specified standard name.\n\n        Applies unit conversions and dimension checks to a variable, ensuring its structure\n        conforms to the expectations for its `standard_name`.\n\n        Parameters:\n            standard_name (str): The canonical name of the variable (e.g., 'FEDU', 'xGEO').\n            variable (ep.Variable): The variable to be standardized.\n\n        Returns:\n            ep.Variable: The standardized variable.\n\n        Raises:\n            AssertionError: If the variable's dimensions are incorrect.\n            UnitConversionError: If unit conversion fails.\n        \"\"\"\n        match standard_name:\n            case \"time\":\n                variable.convert_to_unit(u.datenum) # type: ignore[reportUnknownArgumentType]\n                assert_n_dim(variable, 1, standard_name)\n            case \"Flux\":\n                variable.convert_to_unit((u.cm**2 * u.s * u.sr * u.keV)**(-1)) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 3, standard_name)\n                shape = variable.get_data().shape\n                self.consistency_check.check_time_size(shape[0], standard_name)\n                self.consistency_check.check_energy_size(shape[1], standard_name)\n                self.consistency_check.check_pitch_angle_size(shape[2], standard_name)\n\n            case \"alpha_local\"|\"alpha_eq_model\":\n                variable.convert_to_unit(u.radian) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 2, standard_name)\n                shape = variable.get_data().shape\n                self.consistency_check.check_time_size(shape[0], standard_name)\n                self.consistency_check.check_pitch_angle_size(shape[1], standard_name)\n\n            case \"energy_channels\":\n                variable.convert_to_unit(u.MeV) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 2, standard_name)\n                shape = variable.get_data().shape\n                self.consistency_check.check_time_size(shape[0], standard_name)\n                self.consistency_check.check_energy_size(shape[1], standard_name)\n\n            case \"MLT\":\n                variable.convert_to_unit(u.hour) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 1, standard_name)\n                self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n\n            case \"Lstar\"|\"Lm\":\n                variable.convert_to_unit(u.dimensionless_unscaled)\n\n                assert_n_dim(variable, 2, standard_name)\n                shape = variable.get_data().shape\n                self.consistency_check.check_time_size(shape[0], standard_name)\n                self.consistency_check.check_pitch_angle_size(shape[1], standard_name)\n\n            case \"xGEO\":\n                variable.convert_to_unit(u.RE) # type: ignore[reportUnknownArgumentType]\n                assert_n_dim(variable, 2, standard_name)\n                self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n\n            case \"B_eq\"|\"B_local\":\n                variable.convert_to_unit(u.nT) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 1, standard_name)\n                self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n\n            case \"R0\":\n                variable.convert_to_unit(u.RE) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 1, standard_name)\n                self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n\n            case \"density\":\n                variable.convert_to_unit(u.cm**(-3)) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 1, standard_name)\n                self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n            case \"PSD\":\n                variable.convert_to_unit((u.m * u.kg * u.m / u.s)**(-3)) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 3, standard_name)\n                shape = variable.get_data().shape\n                self.consistency_check.check_time_size(shape[0], standard_name)\n                self.consistency_check.check_energy_size(shape[1], standard_name)\n                self.consistency_check.check_pitch_angle_size(shape[2], standard_name)\n\n            case \"InvMu\":\n                variable.convert_to_unit(u.MeV / u.G) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 3, standard_name)\n                shape = variable.get_data().shape\n                self.consistency_check.check_time_size(shape[0], standard_name)\n                self.consistency_check.check_energy_size(shape[1], standard_name)\n                self.consistency_check.check_pitch_angle_size(shape[2], standard_name)\n\n            case \"InvK\":\n                variable.convert_to_unit(u.RE * u.G**0.5) # type: ignore[reportUnknownArgumentType]\n\n                assert_n_dim(variable, 2, standard_name)\n                shape = variable.get_data().shape\n                self.consistency_check.check_time_size(shape[0], standard_name)\n                self.consistency_check.check_pitch_angle_size(shape[1], standard_name)\n\n            case _:\n                msg = f\"Encountered invalid name_in_file: {standard_name}!\"\n                raise ValueError(msg)\n\n        return variable\n</code></pre>"},{"location":"API_reference/data_standards/data_org/#el_paso.data_standards.data_org_standard.DataOrgStandard.__init__","title":"<code>__init__</code>","text":"<p>Initializes the DataOrgStandard with a ConsistencyCheck object.</p> Source code in <code>el_paso/data_standards/data_org_standard.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the DataOrgStandard with a ConsistencyCheck object.\"\"\"\n    self.consistency_check = ConsistencyCheck()\n</code></pre>"},{"location":"API_reference/data_standards/prbem/","title":"PRBEM Standard","text":""},{"location":"API_reference/data_standards/prbem/#el_paso.data_standards.prbem_standard.PRBEMStandard","title":"<code>el_paso.data_standards.prbem_standard.PRBEMStandard</code>","text":"<p>               Bases: <code>DataStandard</code></p> <p>A data standard of the Panel for Radiation Belt Environment Modeling (PRBEM).</p> <p>This class defines and applies a specific set of data standards for variables defined by the PRBEM. It standardizes variables by converting them to canonical units and performing consistency checks on their dimensions and shapes, ensuring they conform to the expected format for each standard name.</p> Source code in <code>el_paso/data_standards/prbem_standard.py</code> <pre><code>class PRBEMStandard(DataStandard):\n    \"\"\"A data standard of the Panel for Radiation Belt Environment Modeling (PRBEM).\n\n    This class defines and applies a specific set of data standards for variables\n    defined by the [PRBEM](https://prbem.github.io/documents/Standard_File_Format.pdf).\n    It standardizes variables by converting them to canonical units and performing\n    consistency checks on their dimensions and shapes, ensuring they conform to the\n    expected format for each standard name.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the PRBEMStandard with a ConsistencyCheck object.\"\"\"\n        self.consistency_check = ConsistencyCheck()\n\n    def standardize_variable(self, standard_name: str, variable: ep.Variable) -&gt; ep.Variable:  # noqa: C901, PLR0912, PLR0915\n        \"\"\"Standardizes a variable based on its specified standard name.\n\n        This method first converts the variable to its canonical unit based on the\n        `standard_name`. It then performs a series of dimension and shape\n        consistency checks to ensure the variable's structure is valid for\n        the given data type.\n\n        Args:\n            standard_name (str): The name of the data standard to apply (e.g.,\n                'FEDU', 'xGEO', 'Lstar').\n            variable (ep.Variable): The variable to be standardized.\n\n        Returns:\n            ep.Variable: The standardized variable with its unit converted and\n                          its consistency validated.\n        \"\"\"\n        if standard_name == \"FEDU\":\n            variable.convert_to_unit((u.cm**2 * u.s * u.sr * u.keV) ** (-1)) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 3, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n            self.consistency_check.check_energy_size(shape[1], standard_name)\n            self.consistency_check.check_pitch_angle_size(shape[2], standard_name)\n\n        elif standard_name == \"FEDO\":\n            variable.convert_to_unit((u.cm**2 * u.s * u.sr * u.keV) ** (-1)) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 2, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n            self.consistency_check.check_energy_size(shape[1], standard_name)\n\n        elif \"alpha\" in standard_name:\n            variable.convert_to_unit(u.deg) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 2, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n            self.consistency_check.check_pitch_angle_size(shape[1], standard_name)\n\n        elif \"energy\" in standard_name:\n            variable.convert_to_unit(u.MeV) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 2, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n            self.consistency_check.check_energy_size(shape[1], standard_name)\n\n        elif \"xGEO\" in standard_name:\n            variable.convert_to_unit(ep.units.RE)\n\n            assert_n_dim(variable, 2, standard_name)\n            self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n\n        elif \"MLT\" in standard_name:\n            variable.convert_to_unit(u.hour) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 1, standard_name)\n            self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n\n        elif \"R0\" in standard_name:\n            variable.convert_to_unit(ep.units.RE)\n\n            assert_n_dim(variable, 1, standard_name)\n            self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n\n        elif \"Lstar\" in standard_name or \"lm\" in standard_name:\n            variable.convert_to_unit(u.dimensionless_unscaled)\n\n            assert_n_dim(variable, 2, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n            self.consistency_check.check_pitch_angle_size(shape[1], standard_name)\n\n        elif \"B_eq\" in standard_name or \"B_local\" in standard_name:\n            variable.convert_to_unit(u.nT) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 1, standard_name)\n            self.consistency_check.check_time_size(variable.get_data().shape[0], standard_name)\n\n        elif \"PSD\" in standard_name:\n            variable.convert_to_unit((u.m * u.kg * u.m / u.s)**(-3)) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 3, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n            self.consistency_check.check_energy_size(shape[1], standard_name)\n            self.consistency_check.check_pitch_angle_size(shape[2], standard_name)\n\n        elif \"inv_mu\" in standard_name:\n            variable.convert_to_unit(u.MeV/u.G) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 3, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n            self.consistency_check.check_energy_size(shape[1], standard_name)\n            self.consistency_check.check_pitch_angle_size(shape[2], standard_name)\n\n        elif \"inv_K\" in standard_name:\n            variable.convert_to_unit(u.RE * u.G**0.5) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 2, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n            self.consistency_check.check_pitch_angle_size(shape[1], standard_name)\n\n        elif \"density\" in standard_name:\n            variable.convert_to_unit(u.cm**(-3)) # type: ignore[reportUnknownArgumentType]\n\n            assert_n_dim(variable, 1, standard_name)\n            shape = variable.get_data().shape\n            self.consistency_check.check_time_size(shape[0], standard_name)\n\n        return variable\n</code></pre>"},{"location":"API_reference/data_standards/prbem/#el_paso.data_standards.prbem_standard.PRBEMStandard.__init__","title":"<code>__init__</code>","text":"<p>Initializes the PRBEMStandard with a ConsistencyCheck object.</p> Source code in <code>el_paso/data_standards/prbem_standard.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the PRBEMStandard with a ConsistencyCheck object.\"\"\"\n    self.consistency_check = ConsistencyCheck()\n</code></pre>"},{"location":"API_reference/processing/bin_by_time/","title":"Bin by time","text":""},{"location":"API_reference/processing/bin_by_time/#el_paso.processing.bin_by_time","title":"<code>el_paso.processing.bin_by_time</code>","text":""},{"location":"API_reference/processing/bin_by_time/#el_paso.processing.bin_by_time.TimeBinMethod","title":"<code>TimeBinMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for time binning methods.</p> <p>Attributes:</p> Name Type Description <code>Mean</code> <code>str</code> <p>Calculates the mean of the data.</p> <code>NanMean</code> <code>str</code> <p>Calculates the mean of the data, ignoring NaNs.</p> <code>Median</code> <code>str</code> <p>Calculates the median of the data.</p> <code>NanMedian</code> <code>str</code> <p>Calculates the median of the data, ignoring NaNs.</p> <code>Merge</code> <code>str</code> <p>Concatenates the data.</p> <code>NanMax</code> <code>str</code> <p>Calculates the maximum of the data, ignoring NaNs.</p> <code>NanMin</code> <code>str</code> <p>Calculates the minimum of the data, ignoring NaNs.</p> <code>NoBinning</code> <code>str</code> <p>Applies no binning.</p> <code>Repeat</code> <code>str</code> <p>Repeats the data.</p> <code>Unique</code> <code>str</code> <p>Returns unique values from the data.</p> Source code in <code>el_paso/processing/bin_by_time.py</code> <pre><code>class TimeBinMethod(Enum):\n    \"\"\"Enum for time binning methods.\n\n    Attributes:\n        Mean (str): Calculates the mean of the data.\n        NanMean (str): Calculates the mean of the data, ignoring NaNs.\n        Median (str): Calculates the median of the data.\n        NanMedian (str): Calculates the median of the data, ignoring NaNs.\n        Merge (str): Concatenates the data.\n        NanMax (str): Calculates the maximum of the data, ignoring NaNs.\n        NanMin (str): Calculates the minimum of the data, ignoring NaNs.\n        NoBinning (str): Applies no binning.\n        Repeat (str): Repeats the data.\n        Unique (str): Returns unique values from the data.\n    \"\"\"\n\n    Mean = \"Mean\"\n    NanMean = \"NanMean\"\n    Median = \"Median\"\n    NanMedian = \"NanMedian\"\n    Merge = \"Merge\"\n    NanMax = \"NanMax\"\n    NanMin = \"NanMin\"\n    NoBinning = \"NoBinning\"\n    Repeat = \"Repeat\"\n    Unique = \"Unique\"\n\n    def __call__(self, data:NDArray[np.generic], drop_percent:float=0) -&gt; NDArray[np.generic]:  # noqa: C901, PLR0912\n        \"\"\"Applies the binning method to the provided data.\n\n        Args:\n            data (NDArray[np.generic]): The input data array to be binned or aggregated.\n            drop_percent (float, optional): The percentage of the lowest and highest\n                values to drop before performing a statistical aggregation.\n                Defaults to 0.\n\n        Returns:\n            NDArray[np.generic]: The resulting array after applying the selected\n                binning or aggregation method.\n\n        Raises:\n            TypeError: If the selected binning method requires numeric types and the\n                input data is not numeric.\n        \"\"\"\n        binned_array:NDArray[np.generic]\n\n        if self.value in [\"Mean\", \"NanMean\", \"Median\", \"NanMedian\", \"NanMax\", \"NanMin\"] \\\n            and not np.issubdtype(data.dtype, np.number):\n                msg = f\"{self.value} time bin method is only supported for numeric types!\"\n                raise TypeError(msg)\n\n        num_to_remove = int(len(data) * drop_percent / 100)\n        if num_to_remove &gt; 0 and np.issubdtype(data.dtype, np.number):\n            data = np.sort(data, axis=0)\n            data = data[num_to_remove:-num_to_remove]\n\n        match self.value:\n            case \"Mean\":\n                data = typing.cast(\"NDArray[np.floating]\", data)\n                binned_array = np.mean(data, axis=0)\n            case \"NanMean\":\n                data = typing.cast(\"NDArray[np.floating]\", data)\n                binned_array = np.nanmean(data, axis=0)\n            case \"Median\":\n                data = typing.cast(\"NDArray[np.floating]\", data)\n                binned_array = np.nanmedian(data, axis=0)\n            case \"NanMedian\":\n                data = typing.cast(\"NDArray[np.floating]\", data)\n                binned_array = np.nanmedian(data, axis=0)\n            case \"Merge\":\n                binned_array = np.concatenate(data, axis=0)\n            case \"NanMax\":\n                binned_array = np.nanmax(data, axis=0)\n            case \"NanMin\":\n                binned_array = np.nanmin(data, axis=0)\n            case \"NoBinning\":\n                binned_array = data\n            case \"Repeat\":\n                binned_array = data\n            case \"Unique\":\n                binned_array = np.unique(data, axis=0)\n\n                if data.dtype.kind in {\"U\", \"S\"}:\n                    binned_array = np.asarray([\"\".join(binned_array)])\n\n        return binned_array\n</code></pre>"},{"location":"API_reference/processing/bin_by_time/#el_paso.processing.bin_by_time.TimeBinMethod.__call__","title":"<code>__call__</code>","text":"<p>Applies the binning method to the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[generic]</code> <p>The input data array to be binned or aggregated.</p> required <code>drop_percent</code> <code>float</code> <p>The percentage of the lowest and highest values to drop before performing a statistical aggregation. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>NDArray[generic]</code> <p>NDArray[np.generic]: The resulting array after applying the selected binning or aggregation method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the selected binning method requires numeric types and the input data is not numeric.</p> Source code in <code>el_paso/processing/bin_by_time.py</code> <pre><code>def __call__(self, data:NDArray[np.generic], drop_percent:float=0) -&gt; NDArray[np.generic]:  # noqa: C901, PLR0912\n    \"\"\"Applies the binning method to the provided data.\n\n    Args:\n        data (NDArray[np.generic]): The input data array to be binned or aggregated.\n        drop_percent (float, optional): The percentage of the lowest and highest\n            values to drop before performing a statistical aggregation.\n            Defaults to 0.\n\n    Returns:\n        NDArray[np.generic]: The resulting array after applying the selected\n            binning or aggregation method.\n\n    Raises:\n        TypeError: If the selected binning method requires numeric types and the\n            input data is not numeric.\n    \"\"\"\n    binned_array:NDArray[np.generic]\n\n    if self.value in [\"Mean\", \"NanMean\", \"Median\", \"NanMedian\", \"NanMax\", \"NanMin\"] \\\n        and not np.issubdtype(data.dtype, np.number):\n            msg = f\"{self.value} time bin method is only supported for numeric types!\"\n            raise TypeError(msg)\n\n    num_to_remove = int(len(data) * drop_percent / 100)\n    if num_to_remove &gt; 0 and np.issubdtype(data.dtype, np.number):\n        data = np.sort(data, axis=0)\n        data = data[num_to_remove:-num_to_remove]\n\n    match self.value:\n        case \"Mean\":\n            data = typing.cast(\"NDArray[np.floating]\", data)\n            binned_array = np.mean(data, axis=0)\n        case \"NanMean\":\n            data = typing.cast(\"NDArray[np.floating]\", data)\n            binned_array = np.nanmean(data, axis=0)\n        case \"Median\":\n            data = typing.cast(\"NDArray[np.floating]\", data)\n            binned_array = np.nanmedian(data, axis=0)\n        case \"NanMedian\":\n            data = typing.cast(\"NDArray[np.floating]\", data)\n            binned_array = np.nanmedian(data, axis=0)\n        case \"Merge\":\n            binned_array = np.concatenate(data, axis=0)\n        case \"NanMax\":\n            binned_array = np.nanmax(data, axis=0)\n        case \"NanMin\":\n            binned_array = np.nanmin(data, axis=0)\n        case \"NoBinning\":\n            binned_array = data\n        case \"Repeat\":\n            binned_array = data\n        case \"Unique\":\n            binned_array = np.unique(data, axis=0)\n\n            if data.dtype.kind in {\"U\", \"S\"}:\n                binned_array = np.asarray([\"\".join(binned_array)])\n\n    return binned_array\n</code></pre>"},{"location":"API_reference/processing/bin_by_time/#el_paso.processing.bin_by_time.bin_by_time","title":"<code>bin_by_time</code>","text":"<p>Bins one or more variables by time according to specified methods and cadence.</p> <p>This function takes a time variable and a dictionary of other variables, then bins these variables over time. Each variable can have a specific binning method applied (e.g., mean, median, sum). The binning is performed over defined time intervals (cadence) with a specified alignment.</p> <p>Parameters:</p> Name Type Description Default <code>time_variable</code> <code>Variable</code> <p>The master time variable that defines the time basis for all other variables. Its data should be in a time unit (e.g., <code>ep.units.posixtime</code> or <code>ep.units.datenum</code>).</p> required <code>variables</code> <code>dict[str, Variable]</code> <p>A dictionary where keys are variable names (str) and values are the <code>ep.Variable</code> objects to be binned.</p> required <code>time_bin_method_dict</code> <code>dict[str, TimeBinMethod]</code> <p>A dictionary mapping variable names (str) to <code>ep.TimeBinMethod</code> enums, specifying how each variable should be binned within each time window. If a variable is not present in this dictionary, it will be skipped.</p> required <code>time_binning_cadence</code> <code>timedelta</code> <p>A <code>datetime.timedelta</code> object specifying the duration of each time bin.</p> required <code>window_alignement</code> <code>Literal['center', 'left', 'right']</code> <p>Determines how the time windows are aligned. Defaults to \"center\". * \"center\": The time bin represents the center of the window. * \"left\": The time bin represents the left (start) of the window. * \"right\": The time bin represents the right (end) of the window.</p> <code>'center'</code> <code>start_time</code> <code>datetime | None</code> <p>Optional. A <code>datetime.datetime</code> object specifying the start time for binning. If None, the start time of <code>time_variable</code> is used.</p> <code>None</code> <code>end_time</code> <code>datetime | None</code> <p>Optional. A <code>datetime.datetime</code> object specifying the end time for binning. If None, the end time of <code>time_variable</code> is used.</p> <code>None</code> <code>drop_percent</code> <code>float</code> <p>Optional. The percentage of the lowest and highest values to drop from each time bin before calculating statistical aggregates like mean or median. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Variable</code> <p>ep.Variable: An <code>ep.Variable</code> object representing the new binned time axis. The</p> <code>Variable</code> <p><code>variables</code> dictionary passed as an argument is modified in place, with</p> <code>Variable</code> <p>each variables's data updated to its binned values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the first dimension size of any variable's data does not match the length of the <code>time_variable</code> data.</p> Source code in <code>el_paso/processing/bin_by_time.py</code> <pre><code>@timed_function()\ndef bin_by_time(  # noqa: C901\n    time_variable: ep.Variable,\n    variables: dict[str, ep.Variable],\n    time_bin_method_dict: dict[str, TimeBinMethod],\n    time_binning_cadence: timedelta,\n    window_alignement: Literal[\"center\", \"left\", \"right\"] = \"center\",\n    start_time: datetime|None=None,\n    end_time: datetime|None=None,\n    drop_percent: float = 0,\n) -&gt; ep.Variable:\n    \"\"\"Bins one or more variables by time according to specified methods and cadence.\n\n    This function takes a time variable and a dictionary of other variables, then\n    bins these variables over time. Each variable can have a specific binning\n    method applied (e.g., mean, median, sum). The binning is performed over\n    defined time intervals (cadence) with a specified alignment.\n\n    Args:\n        time_variable (ep.Variable): The master time variable that defines the\n            time basis for all other variables. Its data should be in a time\n            unit (e.g., `ep.units.posixtime` or `ep.units.datenum`).\n        variables (dict[str, ep.Variable]): A dictionary where keys are variable names (str) and values\n            are the `ep.Variable` objects to be binned.\n        time_bin_method_dict (dict[str, ep.TimeBinMethod]): A dictionary mapping variable names (str) to\n            `ep.TimeBinMethod` enums, specifying how each variable should be\n            binned within each time window. If a variable is not present in\n            this dictionary, it will be skipped.\n        time_binning_cadence (timedelta): A `datetime.timedelta` object specifying the\n            duration of each time bin.\n        window_alignement (Literal[\"center\", \"left\", \"right\"]): Determines how the time windows are aligned.\n            Defaults to \"center\".\n            * \"center\": The time bin represents the center of the window.\n            * \"left\": The time bin represents the left (start) of the window.\n            * \"right\": The time bin represents the right (end) of the window.\n        start_time (datetime | None): Optional. A `datetime.datetime` object specifying the\n            start time for binning. If None, the start time of `time_variable`\n            is used.\n        end_time (datetime | None): Optional. A `datetime.datetime` object specifying the end\n            time for binning. If None, the end time of `time_variable` is used.\n        drop_percent (float): Optional. The percentage of the lowest and highest values to\n            drop from each time bin before calculating statistical aggregates\n            like mean or median. Defaults to 0.\n\n    Returns:\n        ep.Variable: An `ep.Variable` object representing the new binned time axis. The\n        `variables` dictionary passed as an argument is modified in place, with\n        each variables's data updated to its binned values.\n\n    Raises:\n        ValueError: If the first dimension size of any variable's data does not\n            match the length of the `time_variable` data.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Binning by time...\")\n\n    start_time = start_time or datenum_to_datetime(time_variable.get_data(ep.units.datenum)[0])\n    end_time   = end_time or datenum_to_datetime(time_variable.get_data(ep.units.datenum)[-1])\n\n    original_cadence = float(np.nanmedian(np.diff(time_variable.get_data(ep.units.posixtime))))\n\n    binned_time, time_bins = _create_binned_time_and_bins(start_time, end_time, time_binning_cadence, window_alignement)\n\n    # Cache digitized indices for every time variable\n    index_iterables = None\n\n    for key, var in variables.items():\n\n        if key not in time_bin_method_dict:\n            continue\n\n        # Just repeat in case of no time dependency\n        if time_bin_method_dict[key] == ep.TimeBinMethod.Repeat:\n            var.set_data(np.repeat(var.get_data()[np.newaxis, ...], len(binned_time), axis=0), \"same\")\n            var.metadata.original_cadence_seconds = 0\n            continue\n\n        # check if time variable and data content sizes match\n        if var.get_data().shape[0] != len(time_variable.get_data()):\n            msg = f\"Variable {key}: size of dimension 0 does not match length of time variable!\"\n            raise ValueError(msg)\n\n        # calculate bin indices for given time array if it has not been calculated before\n        if not index_iterables:\n            timestamps = typing.cast(\"NDArray[np.floating]\", time_variable.get_data(ep.units.posixtime))\n            index_iterables = _calculate_index_iterables(timestamps, time_bins)\n\n        unique_indices, indices_separation = index_iterables\n\n        # Initialize binned_data as an array of np.nans with the same shape as self._data,\n        # but with the length of the first dimension matching the length of time_array\n        if var.get_data().dtype.kind in {\"U\", \"S\", \"O\"}:  # Check if the data is string or object type\n            binned_data = np.full((len(binned_time),), \"\", dtype=var.get_data().dtype)\n        else:\n            binned_data_shape = (len(binned_time), *var.get_data().shape[1:])\n            binned_data = np.full(binned_data_shape, np.nan)\n\n        # Iterate over unique indices\n        for i, unique_index in enumerate(unique_indices):\n            bin_data = var.get_data()[indices_separation[i] : indices_separation[i + 1]]\n            if len(bin_data) == 0:\n                continue  # no data found\n            if bin_data.dtype.kind in {\"i\", \"f\"} and not np.any(np.isfinite(bin_data)):\n                continue  # no finite data found\n            binned_value = time_bin_method_dict[key](bin_data, drop_percent=drop_percent)\n\n            # Update the relevant slice of binned_data\n            binned_data[unique_index, ...] = binned_value\n\n        # Update relevant metadata fields\n        # Ensure binned_data works for both numeric and string data\n        if isinstance(binned_data[0], str):\n            var.set_data(np.array(binned_data, dtype=object), \"same\")\n        else:\n            var.set_data(np.array(binned_data) , \"same\")\n\n        # update metadata\n        var.metadata.original_cadence_seconds = original_cadence\n        var.metadata.add_processing_note(f\"Time binned with method {time_bin_method_dict[key].value}\"\n                                         f\" and cadence of {time_binning_cadence.total_seconds()/60} minutes\")\n\n    new_time_var = ep.Variable(data=binned_time, original_unit=ep.units.posixtime)\n    new_time_var.metadata.add_processing_note(\"Created while time binning.\")\n\n    return new_time_var\n</code></pre>"},{"location":"API_reference/processing/compute_invariant_K/","title":"compute invariant K","text":""},{"location":"API_reference/processing/compute_invariant_K/#el_paso.processing.compute_invariant_K","title":"<code>el_paso.processing.compute_invariant_K</code>","text":""},{"location":"API_reference/processing/compute_invariant_K/#el_paso.processing.compute_invariant_K.compute_invariant_K","title":"<code>compute_invariant_K</code>","text":"<p>Computes the invariant K from mirror magnetic field and the second adiabatic invariant (I).</p> <p>The invariant K is calculated as the square root of the mirror magnetic field (\\(B_{mirr}\\)) multiplied by the second adiabatic invariant (\\(X_J\\)). The unit of the resulting invariant K is \\(R_E \\cdot G^{0.5}\\).</p> <p>Parameters:</p> Name Type Description Default <code>bmirr</code> <code>Variable</code> <p>A Variable object containing the mirror magnetic field data. The data is expected to be convertible to Gauss (u.G).</p> required <code>xj</code> <code>Variable</code> <p>A Variable object containing the second adiabativ invariant (I).</p> required <p>Returns:</p> Type Description <code>Variable</code> <p>ep.Variable: A new Variable object containing the computed invariant K data with unit \\(R_E \\cdot G^{0.5}\\).</p> Source code in <code>el_paso/processing/compute_invariant_K.py</code> <pre><code>def compute_invariant_K(bmirr:ep.Variable,  # noqa: N802\n                        xj:ep.Variable) -&gt; ep.Variable:\n    r\"\"\"Computes the invariant K from mirror magnetic field and the second adiabatic invariant (I).\n\n    The invariant K is calculated as the square root of the mirror magnetic\n    field ($B_{mirr}$) multiplied by the second adiabatic invariant ($X_J$).\n    The unit of the resulting invariant K is $R_E \\cdot G^{0.5}$.\n\n    Args:\n        bmirr (ep.Variable): A Variable object containing the mirror magnetic\n            field data. The data is expected to be convertible to Gauss (u.G).\n        xj (ep.Variable): A Variable object containing the second adiabativ invariant (I).\n\n    Returns:\n        ep.Variable: A new Variable object containing the computed invariant K\n            data with unit $R_E \\cdot G^{0.5}$.\n    \"\"\"\n    bmirr_data = bmirr.get_data(u.G).astype(np.float64)\n\n    xj_data = xj.get_data(ep.units.RE).astype(np.float64)\n\n    inv_K_var = ep.Variable(data = np.sqrt(bmirr_data) * xj_data,  # noqa: N806\n                            original_unit=ep.units.RE * u.G**0.5)  # type: ignore[reportUnknownArgumentType]\n\n    inv_K_var.metadata.add_processing_note(\"Created with compute_invariant_K from B_mirr and XJ.\")\n\n    return inv_K_var\n</code></pre>"},{"location":"API_reference/processing/compute_invariant_mu/","title":"Compute invariant mu","text":""},{"location":"API_reference/processing/compute_invariant_mu/#el_paso.processing.compute_invariant_mu","title":"<code>el_paso.processing.compute_invariant_mu</code>","text":""},{"location":"API_reference/processing/compute_invariant_mu/#el_paso.processing.compute_invariant_mu.compute_invariant_mu","title":"<code>compute_invariant_mu</code>","text":"<p>Computes the first adiabatic invariant (mu) for given particle species.</p> <p>The first adiabatic invariant (\\(\\mu\\)) is calculated using the formula: \\(\\mu = \\frac{p_{\\perp}^2}{2mB}\\), where \\(p_{\\perp}\\) is the perpendicular momentum, \\(m\\) is the particle's rest mass, and \\(B\\) is the local magnetic field strength.</p> <p>The momentum is derived from the total energy and local pitch angle. The result is in units of \\(MeV/G\\).</p> <p>Parameters:</p> Name Type Description Default <code>energy_var</code> <code>Variable</code> <p>A Variable object containing the total energy of the particles in MeV. Expected to be a 2D array (time, energy_bins).</p> required <code>alpha_local_var</code> <code>Variable</code> <p>A Variable object containing the local pitch angles in radians. Expected to be a 2D array (time, angle_bins).</p> required <code>B_local_var</code> <code>Variable</code> <p>A Variable object containing the local magnetic field strength in nT. Expected to be a 1D array (time).</p> required <code>particle_species</code> <code>ParticleLiteral</code> <p>The species of the particles (e.g., \"electron\", \"proton\").</p> required <p>Returns:</p> Type Description <code>Variable</code> <p>ep.Variable: A new Variable object containing the computed invariant mu data, with dimensions (time, energy_bins, angle_bins) and unit \\(MeV/G\\).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input variables do not have the correct dimensions (energy, alpha_local must be 2D; B_local must be 1D) or if their time dimensions do not match.</p> Notes <p>Values of invariant mu that are less than or equal to zero are replaced with <code>NaN</code>.</p> Source code in <code>el_paso/processing/compute_invariant_mu.py</code> <pre><code>def compute_invariant_mu(energy_var:ep.Variable,\n                         alpha_local_var:ep.Variable,\n                         B_local_var:ep.Variable,  # noqa: N803\n                         particle_species:ParticleLiteral) -&gt; ep.Variable:\n    r\"\"\"Computes the first adiabatic invariant (mu) for given particle species.\n\n    The first adiabatic invariant ($\\mu$) is calculated using the formula:\n    $\\mu = \\frac{p_{\\perp}^2}{2mB}$, where $p_{\\perp}$ is the perpendicular\n    momentum, $m$ is the particle's rest mass, and $B$ is the local magnetic\n    field strength.\n\n    The momentum is derived from the total energy and local pitch angle.\n    The result is in units of $MeV/G$.\n\n    Args:\n        energy_var (ep.Variable): A Variable object containing the total energy\n            of the particles in MeV. Expected to be a 2D array (time, energy_bins).\n        alpha_local_var (ep.Variable): A Variable object containing the local\n            pitch angles in radians. Expected to be a 2D array (time, angle_bins).\n        B_local_var (ep.Variable): A Variable object containing the local magnetic\n            field strength in nT. Expected to be a 1D array (time).\n        particle_species (ParticleLiteral): The species of the particles\n            (e.g., \"electron\", \"proton\").\n\n    Returns:\n        ep.Variable: A new Variable object containing the computed invariant mu\n            data, with dimensions (time, energy_bins, angle_bins) and unit $MeV/G$.\n\n    Raises:\n        ValueError: If input variables do not have the correct dimensions\n            (energy, alpha_local must be 2D; B_local must be 1D)\n            or if their time dimensions do not match.\n\n    Notes:\n        Values of invariant mu that are less than or equal to zero are replaced with `NaN`.\n    \"\"\"\n    energy = energy_var.get_data(u.MeV)\n    alpha_local = alpha_local_var.get_data(u.radian)\n    magnetic_field = B_local_var.get_data(u.nT)\n\n    if energy.ndim != 2 or alpha_local.ndim != 2 or magnetic_field.ndim != 1:  # noqa: PLR2004\n        msg = (\"Input variables must have the correct dimensions: \"\n               \"energy (2D), alpha_local (2D), and magnetic_field (1D).\")\n        raise ValueError(msg)\n\n    if energy.shape[0] != alpha_local.shape[0] or energy.shape[0] != magnetic_field.shape[0]:\n        msg = (\"Input variables must have matching time dimensions: \"\n               f\"energy ({energy.shape[0]}), alpha_local ({alpha_local.shape[0]}), \"\n               f\"and magnetic_field ({magnetic_field.shape[0]}).\")\n        raise ValueError(msg)\n\n    mc2 = rest_energy(particle_species)\n    pct = en2pc(energy, particle_species)  # Relativistic energy for the particles\n\n    # Calculate InvMu using broadcasting\n    sin_alpha_eq = np.sin(alpha_local)\n\n    inv_mu = (pct[:, :, np.newaxis] * sin_alpha_eq[:, np.newaxis, :]) ** 2 / (\n            magnetic_field[:, np.newaxis, np.newaxis] * 2 * mc2)  # MeV/G\n\n    inv_mu[inv_mu &lt;= 0] = np.nan\n\n    inv_mu_var = ep.Variable(data=inv_mu,\n                             original_unit=u.MeV / u.G) # type: ignore[reportUnknownArgumentType]\n\n    inv_mu_var.metadata.add_processing_note(f\"Created with compute_invariant_mu for {particle_species} particles\")\n\n    return inv_mu_var\n</code></pre>"},{"location":"API_reference/processing/compute_magnetic_field_variables/","title":"Compute magnetic field variables","text":"<p>This function serves as a wrapper to calculate a suite of magnetic field and related invariants (like L-star, MLT, B_local, B_eq, invariant Mu, and invariant K) based on provided time and geocentric coordinates. It leverages the IRBEM library for the underlying computations.</p> <p>var_names_to_compute must be a list of strings containing the requested variable names followed by the magnetic field identifier string.</p> <p>Supported variable names:</p> <ul> <li>B_local</li> <li>B_fofl</li> <li>MLT</li> <li>R_eq</li> <li>Lstar</li> <li>PA_eq</li> <li>invMu</li> <li>invK</li> </ul> <p>Supported magnetic field strings:</p> <ul> <li>OP77Q</li> <li>T89</li> <li>T96</li> <li>T01</li> <li>T01s</li> <li>TS04|TS05|T04s</li> </ul> <p>Examples of valid entries: B_local_T89, Lstar_TS04</p>"},{"location":"API_reference/processing/compute_magnetic_field_variables/#el_paso.processing.compute_magnetic_field_variables","title":"<code>el_paso.processing.compute_magnetic_field_variables</code>","text":""},{"location":"API_reference/processing/compute_magnetic_field_variables/#el_paso.processing.compute_magnetic_field_variables.MagFieldVar","title":"<code>MagFieldVar</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A named tuple to represent a request for a magnetic field variable.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>MagFieldVarTypes</code> <p>The type of magnetic field variable to compute (e.g., \"B_local\", \"Lstar\").</p> <code>mag_field</code> <code>str | MagneticField</code> <p>The magnetic field model to use for the computation .</p> Source code in <code>el_paso/processing/compute_magnetic_field_variables.py</code> <pre><code>class MagFieldVar(NamedTuple):\n    \"\"\"A named tuple to represent a request for a magnetic field variable.\n\n    Attributes:\n        type (mag_utils.MagFieldVarTypes): The type of magnetic field variable to compute (e.g., \"B_local\", \"Lstar\").\n        mag_field (str | mag_utils.MagneticField): The magnetic field model to use for the computation .\n    \"\"\"\n    type: mag_utils.MagFieldVarTypes\n    mag_field: str|mag_utils.MagneticField\n</code></pre>"},{"location":"API_reference/processing/compute_magnetic_field_variables/#el_paso.processing.compute_magnetic_field_variables.compute_magnetic_field_variables","title":"<code>compute_magnetic_field_variables</code>","text":"<p>Computes various magnetic field-related variables using the IRBEM library.</p> <p>This function serves as a wrapper to calculate a suite of magnetic field and related invariants (like L-star, MLT, B_local, B_eq, invariant Mu, and invariant K) based on provided time and geocentric coordinates. It leverages the IRBEM library for the underlying computations.</p> <p>Parameters:</p> Name Type Description Default <code>time_var</code> <code>Variable</code> <p>A Variable object containing time data. The data should be a 1D array of timestamps.</p> required <code>xgeo_var</code> <code>Variable</code> <p>A Variable object containing geocentric (XGEO) coordinates. Expected to be a 2D array (time, 3) where the last dimension represents X, Y, Z coordinates.</p> required <code>variables_to_compute</code> <code>Sequence[tuple[MagFieldVarTypes, str | MagneticField]]</code> <p>A sequence of tuples, where each tuple specifies a variable to compute. The first element is the variable type (e.g., \"Lstar\"), and the second is the magnetic field model to use (e.g., \"IGRF\").</p> required <code>irbem_lib_path</code> <code>str</code> <p>The file path to the compiled IRBEM library object (a <code>.so</code> or <code>.dll</code> file).</p> required <code>irbem_options</code> <code>list[int]</code> <p>A list of 5 integer options for the IRBEM library calls, controlling aspects like model selection, bounce tracing, etc.</p> required <code>num_cores</code> <code>int</code> <p>The number of CPU cores to use for parallel processing within IRBEM calls.</p> required <code>indices_solar_wind</code> <code>dict[str, Variable] | None</code> <p>Optional. A dictionary containing solar wind indices (e.g., \"Kp\", \"Dst\") as <code>Variable</code> objects. Defaults to None.</p> <code>None</code> <code>pa_local_var</code> <code>Variable | None</code> <p>Optional. A Variable object containing local pitch angle data in degrees. Required if any pitch-angle dependent variables (e.g., \"PA_eq\", \"Lstar\", \"invMu\", \"invK\") are requested. Defaults to None.</p> <code>None</code> <code>energy_var</code> <code>Variable | None</code> <p>Optional. A Variable object containing particle energy data in MeV. Required if \"invMu\" is requested. Defaults to None.</p> <code>None</code> <code>particle_species</code> <code>Literal['electron', 'proton'] | None</code> <p>Optional. The species of particle (e.g., \"electron\", \"proton\"). Required if \"invMu\" is requested. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Variable]</code> <p>dict[str, Variable]: A dictionary where keys are the computed variable</p> <code>dict[str, Variable]</code> <p>names and values are their corresponding <code>Variable</code> objects containing</p> <code>dict[str, Variable]</code> <p>the calculated data and metadata.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no IRBEM library object is found at the provided <code>irbem_lib_path</code>.</p> <code>ValueError</code> <ul> <li>If <code>irbem_options</code> does not contain exactly 5 entries.</li> <li>If a pitch-angle dependent variable is requested but <code>pa_local_var</code> is not provided.</li> <li>If an energy-dependent variable (\"invMu\") is requested but <code>energy_var</code> is not provided.</li> <li>If a particle-species dependent variable (\"invMu\") is requested but <code>particle_species</code> is not provided.</li> </ul> <code>NotImplementedError</code> <p>If a requested variable name in <code>variables_to_compute</code> is not supported by this function.</p> Notes <ul> <li>The function internally constructs an <code>IrbemInput</code> object for each unique   magnetic field model encountered in <code>variables_to_compute</code>.</li> <li>Intermediate computed variables (e.g., <code>B_eq</code>, <code>B_local</code>) are cached   within the function to avoid redundant IRBEM calls when multiple   dependent variables are requested.</li> </ul> Source code in <code>el_paso/processing/compute_magnetic_field_variables.py</code> <pre><code>def compute_magnetic_field_variables(\n    time_var:Variable,\n    xgeo_var:Variable,\n    variables_to_compute: VariableRequest,\n    irbem_lib_path: str,\n    irbem_options: list[int],\n    num_cores: int,\n    indices_solar_wind: dict[str, Variable]|None = None,\n    pa_local_var: Variable|None = None,\n    energy_var: Variable|None = None,\n    particle_species:Literal[\"electron\", \"proton\"]|None = None,\n) -&gt; dict[str, Variable]:\n    \"\"\"Computes various magnetic field-related variables using the IRBEM library.\n\n    This function serves as a wrapper to calculate a suite of magnetic field\n    and related invariants (like L-star, MLT, B_local, B_eq, invariant Mu,\n    and invariant K) based on provided time and geocentric coordinates. It\n    leverages the IRBEM library for the underlying computations.\n\n    Args:\n        time_var (Variable): A Variable object containing time data. The data should be a 1D array of timestamps.\n        xgeo_var (Variable): A Variable object containing geocentric (XGEO)\n            coordinates. Expected to be a 2D array (time, 3) where the last\n            dimension represents X, Y, Z coordinates.\n        variables_to_compute (Sequence[tuple[mag_utils.MagFieldVarTypes, str | mag_utils.MagneticField]]):\n            A sequence of tuples, where each tuple specifies a variable to compute. The first element is the\n            variable type (e.g., \"Lstar\"), and the second is the magnetic field model to use (e.g., \"IGRF\").\n        irbem_lib_path (str): The file path to the compiled IRBEM library object\n            (a `.so` or `.dll` file).\n        irbem_options (list[int]): A list of 5 integer options for the IRBEM library\n            calls, controlling aspects like model selection, bounce tracing, etc.\n        num_cores (int): The number of CPU cores to use for parallel processing\n            within IRBEM calls.\n        indices_solar_wind (dict[str, Variable] | None): Optional. A dictionary\n            containing solar wind indices (e.g., \"Kp\", \"Dst\") as `Variable` objects.\n            Defaults to None.\n        pa_local_var (Variable | None): Optional. A Variable object containing\n            local pitch angle data in degrees. Required if any pitch-angle dependent variables\n            (e.g., \"PA_eq\", \"Lstar\", \"invMu\", \"invK\") are requested. Defaults to None.\n        energy_var (Variable | None): Optional. A Variable object containing\n            particle energy data in MeV. Required if \"invMu\" is requested. Defaults to None.\n        particle_species (Literal[\"electron\", \"proton\"] | None): Optional. The\n            species of particle (e.g., \"electron\", \"proton\"). Required if \"invMu\"\n            is requested. Defaults to None.\n\n    Returns:\n        dict[str, Variable]: A dictionary where keys are the computed variable\n        names and values are their corresponding `Variable` objects containing\n        the calculated data and metadata.\n\n    Raises:\n        FileNotFoundError: If no IRBEM library object is found at the provided `irbem_lib_path`.\n        ValueError:\n            - If `irbem_options` does not contain exactly 5 entries.\n            - If a pitch-angle dependent variable is requested but `pa_local_var` is not provided.\n            - If an energy-dependent variable (\"invMu\") is requested but `energy_var` is not provided.\n            - If a particle-species dependent variable (\"invMu\") is requested but `particle_species` is not provided.\n        NotImplementedError: If a requested variable name in `variables_to_compute`\n            is not supported by this function.\n\n    Notes:\n        - The function internally constructs an `IrbemInput` object for each unique\n          magnetic field model encountered in `variables_to_compute`.\n        - Intermediate computed variables (e.g., `B_eq`, `B_local`) are cached\n          within the function to avoid redundant IRBEM calls when multiple\n          dependent variables are requested.\n    \"\"\"\n    if not Path(irbem_lib_path).is_file():\n        msg = f\"No library object found under the provided irbem_lib_path: {irbem_lib_path}\"\n        raise FileNotFoundError(msg)\n\n    if len(irbem_options) != 5:  # noqa: PLR2004\n        msg = f\"irbem_options must be a list with exactly 5 entries! Got: {irbem_options}\"\n        raise ValueError(msg)\n\n    mag_variables_to_compute = [MagFieldVar(mag_field_var[0], mag_field_var[1])\n                                for mag_field_var in variables_to_compute]\n\n    if _requires_pa_var(mag_variables_to_compute) and pa_local_var is None:\n        msg = \"Pitch-angle dependent variable is requested but local pitch angles are not provided!\"\n        raise ValueError(msg)\n    pa_local_var = typing.cast(\"Variable\", pa_local_var)\n\n    if _requires_energy_var(mag_variables_to_compute) and energy_var is None:\n            msg = \"Energy dependent variable is requested but energies are not provided!\"\n            raise ValueError(msg)\n    energy_var = typing.cast(\"Variable\", energy_var)\n\n    if _requires_particle_species(mag_variables_to_compute) and particle_species is None:\n            msg = \"Particle-species dependent variable is requested but particle_species is not provided!\"\n            raise ValueError(msg)\n    particle_species = typing.cast(\"Literal['electron', 'proton']\", particle_species)\n\n    # collect magnetic_field results in this dictionary\n    computed_variables:dict[str, Variable] = {}\n    var_names_to_compute:list[str] = []\n\n    for mag_field_var in mag_variables_to_compute:\n\n        var_type = mag_field_var.type\n        mag_field = mag_field_var.mag_field\n\n        if isinstance(mag_field, str):\n            mag_field = mag_utils.MagneticField(mag_field)\n\n        variable_name = mag_utils.create_var_name(var_type, mag_field)\n        var_names_to_compute.append(variable_name)\n\n        # check if the value has been calculated already\n        if variable_name in computed_variables:\n            continue\n\n        indices_solar_wind_hashable = make_dict_hashable(indices_solar_wind)\n\n        maginput = mag_utils.construct_maginput(time_var, mag_field, indices_solar_wind_hashable)\n\n        irbem_input = mag_utils.IrbemInput(irbem_lib_path, mag_field, maginput, irbem_options, num_cores)\n\n        computed_variables |= _get_result(var_type,\n                                         xgeo_var,\n                                         time_var,\n                                         pa_local_var,\n                                         energy_var,\n                                         computed_variables,\n                                         irbem_input,\n                                         particle_species)\n\n    # only return the requested variables\n    computed_variables = {\n        var_name: computed_variables[var_name]\n        for var_name in computed_variables\n        if var_name in var_names_to_compute\n    }\n\n    return computed_variables\n</code></pre>"},{"location":"API_reference/processing/compute_phase_space_density/","title":"Compute phase space density","text":""},{"location":"API_reference/processing/compute_phase_space_density/#el_paso.processing.compute_phase_space_density.compute_phase_space_density","title":"<code>el_paso.processing.compute_phase_space_density.compute_phase_space_density</code>","text":"<p>Computes the Phase Space Density (PSD) from differential flux.</p> <p>This function calculates the Phase Space Density (PSD) for a given differential flux and energy spectrum, based on the particle species. The PSD is a fundamental quantity in space plasma physics. The units of the resulting PSD are typically \\((m \\cdot kg \\cdot m/s)^{-3}\\).</p> <p>The formula used is: \\(PSD = \\frac{j}{p^2}\\) where \\(j\\) is the differential flux, and \\(p\\) is the relativistic momentum.</p> <p>Parameters:</p> Name Type Description Default <code>flux_var</code> <code>Variable</code> <p>A Variable object containing the differential flux data. Expected units are inverse of (cm^2 s keV sr).</p> required <code>energy_var</code> <code>Variable</code> <p>A Variable object containing the energy spectrum data in MeV.</p> required <code>particle_species</code> <code>Literal['electron', 'proton']</code> <p>The species of the particles (e.g., \"electron\", \"proton\") for which the PSD is computed.</p> required <p>Returns:</p> Type Description <code>Variable</code> <p>ep.Variable: A new Variable object containing the computed Phase Space Density (PSD) data, with unit \\((m \\cdot kg \\cdot m/s)^{-3}\\).</p> Notes <ul> <li>The constant <code>(1e3 / 2.997e10)</code> converts units appropriately (e.g., cm to m, keV to J, etc.)   and accounts for the speed of light.</li> </ul> Source code in <code>el_paso/processing/compute_phase_space_density.py</code> <pre><code>def compute_phase_space_density(flux_var: ep.Variable,\n                                energy_var: ep.Variable,\n                                particle_species: Literal[\"electron\", \"proton\"]) -&gt; ep.Variable:\n    r\"\"\"Computes the Phase Space Density (PSD) from differential flux.\n\n    This function calculates the Phase Space Density (PSD) for a given\n    differential flux and energy spectrum, based on the particle species.\n    The PSD is a fundamental quantity in space plasma physics.\n    The units of the resulting PSD are typically $(m \\cdot kg \\cdot m/s)^{-3}$.\n\n    The formula used is:\n    $PSD = \\frac{j}{p^2}$\n    where $j$ is the differential flux, and $p$ is the relativistic momentum.\n\n    Args:\n        flux_var (ep.Variable): A Variable object containing the differential\n            flux data. Expected units are inverse of (cm^2 s keV sr).\n        energy_var (ep.Variable): A Variable object containing the energy\n            spectrum data in MeV.\n        particle_species (Literal[\"electron\", \"proton\"]): The species of the\n            particles (e.g., \"electron\", \"proton\") for which the PSD is computed.\n\n    Returns:\n        ep.Variable: A new Variable object containing the computed Phase Space\n            Density (PSD) data, with unit $(m \\cdot kg \\cdot m/s)^{-3}$.\n\n    Notes:\n        - The constant `(1e3 / 2.997e10)` converts units appropriately (e.g., cm to m, keV to J, etc.)\n          and accounts for the speed of light.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Computing PSD...\")\n\n    flux_data = flux_var.get_data(typing.cast(\"u.UnitBase\", (u.cm**2 * u.s * u.keV * u.sr) ** (-1)))\n    energies = energy_var.get_data(u.MeV)\n\n    # Calculate pct for each energy value\n    pct = ep.physics.en2pc(energies, particle_species)  # Relativistic energy for electrons\n    psd_data = (1e3 / 2.997e10) * flux_data / (pct[:, :, np.newaxis] ** 2)\n\n    var = ep.Variable(data=psd_data, original_unit=typing.cast(\"u.UnitBase\", (u.m * u.kg * u.m / u.s)**(-3)))\n\n    var.metadata.add_processing_note(\"Created with compute_PSD\")\n\n    return var\n</code></pre>"},{"location":"API_reference/processing/convert_string_to_datetime/","title":"Convert string to datetime","text":""},{"location":"API_reference/processing/convert_string_to_datetime/#el_paso.processing.convert_string_to_datetime.convert_string_to_datetime","title":"<code>el_paso.processing.convert_string_to_datetime.convert_string_to_datetime</code>","text":"<p>Converts a Variable's string-based time data to UTC datetime objects.</p> <p>Parameters:</p> Name Type Description Default <code>time_var</code> <code>Variable</code> <p>The variable containing string-based time data to be converted.</p> required <p>Returns:</p> Type Description <code>NDArray[generic]</code> <p>NDArray[np.generic]: A NumPy array of <code>datetime</code> objects in the UTC timezone.</p> Source code in <code>el_paso/processing/convert_string_to_datetime.py</code> <pre><code>def convert_string_to_datetime(time_var:Variable) -&gt; NDArray[np.generic]:\n    \"\"\"Converts a Variable's string-based time data to UTC datetime objects.\n\n    Args:\n        time_var (Variable): The variable containing string-based time data to be\n            converted.\n\n    Returns:\n        NDArray[np.generic]: A NumPy array of `datetime` objects in the UTC timezone.\n    \"\"\"\n    time_var.metadata.add_processing_note(\"Converting string-time to datetime\")\n\n    return np.asarray([parser.parse(t).replace(tzinfo=timezone.utc) for t in time_var.get_data()])\n</code></pre>"},{"location":"API_reference/processing/fold_pitch_angles_and_flux/","title":"Fold pitch angles and flux","text":""},{"location":"API_reference/processing/fold_pitch_angles_and_flux/#el_paso.processing.fold_pitch_angles_and_flux","title":"<code>el_paso.processing.fold_pitch_angles_and_flux</code>","text":""},{"location":"API_reference/processing/fold_pitch_angles_and_flux/#el_paso.processing.fold_pitch_angles_and_flux.fold_pitch_angles_and_flux","title":"<code>fold_pitch_angles_and_flux</code>","text":"<p>Folds pitch angles and corresponding flux values around 90 degrees.</p> <p>This function modifies the input <code>flux_var</code> and <code>pa_local_var</code> in place by folding the pitch angle data and averaging the corresponding flux values around 90 degrees. It assumes that pitch angles are symmetric around 90 degrees.</p> <p>Parameters:</p> Name Type Description Default <code>flux_var</code> <code>Variable</code> <p>A Variable object containing the flux data. This object will be modified in place with the folded flux data. Expected to have a shape compatible with (time, energy_bins, pitch_angle_bins).</p> required <code>pa_local_var</code> <code>Variable</code> <p>A Variable object containing the local pitch angle data. This object will be modified in place with the unique folded pitch angles in degrees. Expected to have a shape compatible with (time, pitch_angle_bins).</p> required <code>produce_statistic_plot</code> <code>bool</code> <p>If True, a statistical plot related to the folding process will be produced. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If local pitch angles are found to change over time (they are assumed constant).</li> <li>If the time dimensions of <code>flux_var</code> and <code>pa_local_var</code> do not match.</li> <li>If the pitch angle dimension of <code>flux_var</code> does not match the   second dimension of <code>pa_local_var</code>.</li> </ul> Source code in <code>el_paso/processing/fold_pitch_angles_and_flux.py</code> <pre><code>@timed_function()\ndef fold_pitch_angles_and_flux(flux_var:Variable, pa_local_var:Variable, *, produce_statistic_plot:bool=False) -&gt; None:\n    \"\"\"Folds pitch angles and corresponding flux values around 90 degrees.\n\n    This function modifies the input `flux_var` and `pa_local_var` in place\n    by folding the pitch angle data and averaging the corresponding flux values\n    around 90 degrees. It assumes that pitch angles are symmetric around 90 degrees.\n\n    Args:\n        flux_var (Variable): A Variable object containing the flux data.\n            This object will be modified in place with the folded flux data.\n            Expected to have a shape compatible with (time, energy_bins, pitch_angle_bins).\n        pa_local_var (Variable): A Variable object containing the local pitch angle data.\n            This object will be modified in place with the unique folded pitch angles in degrees.\n            Expected to have a shape compatible with (time, pitch_angle_bins).\n        produce_statistic_plot (bool, optional): If True, a statistical plot\n            related to the folding process will be produced. Defaults to False.\n\n    Raises:\n        ValueError:\n            - If local pitch angles are found to change over time (they are assumed constant).\n            - If the time dimensions of `flux_var` and `pa_local_var` do not match.\n            - If the pitch angle dimension of `flux_var` does not match the\n              second dimension of `pa_local_var`.\n    \"\"\"\n    logger.info(\"Folding pitch angles and flux ...\")\n\n    flux     = flux_var.get_data().astype(np.float64)\n    pa_local = pa_local_var.get_data(u.deg).astype(np.float64)\n\n    if np.all(np.repeat(pa_local[0,:][np.newaxis,:], pa_local.shape[0], axis=0) != pa_local):\n        msg = \"We assume that local pitch angles do not change in time!\"\n        raise ValueError(msg)\n\n    if len(flux) != len(pa_local):\n        msg = f\"Size of time dimensions of flux ({len(flux)}) and local pitch angles ({len(pa_local)}) do not match!\"\n        raise ValueError(msg)\n\n    if flux.shape[2] != pa_local.shape[1]:\n        msg = f\"Dimension mismatch: flux: {flux.shape}, pitch angle: {pa_local.shape}\"\n        raise ValueError(msg)\n\n    folded_flux, unique_angles = _fold_pitch_angles_and_flux(pa_local,\n                                                             flux,\n                                                             produce_statistic_plot=produce_statistic_plot)\n\n    flux_var.set_data(folded_flux, \"same\")\n    flux_var.metadata.add_processing_note(\"Folded around 90 degrees local pitch angle\")\n\n    pa_local_var.set_data(unique_angles, u.deg)\n    pa_local_var.metadata.add_processing_note(\"Folded around 90 degrees local pitch angle\")\n</code></pre>"},{"location":"API_reference/saving_strategies/data_org/","title":"DataOrg Strategy","text":""},{"location":"API_reference/saving_strategies/data_org/#el_paso.saving_strategies.data_org_strategy.DataOrgStrategy","title":"<code>el_paso.saving_strategies.data_org_strategy.DataOrgStrategy</code>","text":"<p>               Bases: <code>SavingStrategy</code></p> <p>A concrete saving strategy for saving data based on the satellite mission into separate monthly files.</p> <p>This strategy implements the data standard used at GFZ in the past. It organizes the output files into a specific directory structure (e.g., <code>base_path/MISSION/SATELLITE/Processed_Mat_Files/</code>) and standardizes variables to specific units and dimensions before saving. The data is saved in either <code>.mat</code> or <code>.pickle</code> format, depending on user preference.</p> <p>Attributes:</p> Name Type Description <code>output_files</code> <code>list[OutputFile]</code> <p>Pre-defined list of files to be saved, each with a specific set of variables.</p> <code>base_data_path</code> <code>Path</code> <p>The root directory for all saved data.</p> <code>mission</code> <code>str</code> <p>The name of the space mission (e.g., \"MMS\").</p> <code>satellite</code> <code>str</code> <p>The name of the satellite (e.g., \"MMS1\").</p> <code>instrument</code> <code>str</code> <p>The name of the instrument.</p> <code>kext</code> <code>str</code> <p>A model-related identifier, with \"TS04\" being mapped to \"T04s\" for backward compatibility.</p> <code>file_format</code> <code>Literal['.mat', '.pickle']</code> <p>The file extension for the output files.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the strategy with file paths and metadata.</p> <code>standardize_variable</code> <p>Standardizes variables to specific units and dimensions based on their name.</p> <code>get_time_intervals_to_save</code> <p>Splits the given time range into a list of monthly intervals.</p> <code>get_file_path</code> <p>Generates a complete file path based on the mission, satellite, and date.</p> <code>append_data</code> <p>Appends new data to an existing file by concatenating NumPy arrays based on time.</p> Source code in <code>el_paso/saving_strategies/data_org_strategy.py</code> <pre><code>class DataOrgStrategy(SavingStrategy):\n    \"\"\"A concrete saving strategy for saving data based on the satellite mission into separate monthly files.\n\n    This strategy implements the data standard used at GFZ in the past.\n    It organizes the output files into a specific directory structure\n    (e.g., `base_path/MISSION/SATELLITE/Processed_Mat_Files/`) and standardizes\n    variables to specific units and dimensions before saving. The data is saved\n    in either `.mat` or `.pickle` format, depending on user preference.\n\n    Attributes:\n        output_files (list[OutputFile]): Pre-defined list of files to be saved,\n            each with a specific set of variables.\n        base_data_path (Path): The root directory for all saved data.\n        mission (str): The name of the space mission (e.g., \"MMS\").\n        satellite (str): The name of the satellite (e.g., \"MMS1\").\n        instrument (str): The name of the instrument.\n        kext (str): A model-related identifier, with \"TS04\" being mapped to \"T04s\"\n            for backward compatibility.\n        file_format (Literal[\".mat\", \".pickle\"]): The file extension for the output files.\n\n    Methods:\n        __init__: Initializes the strategy with file paths and metadata.\n        standardize_variable: Standardizes variables to specific units and dimensions based on their name.\n        get_time_intervals_to_save: Splits the given time range into a list of monthly intervals.\n        get_file_path: Generates a complete file path based on the mission, satellite, and date.\n        append_data: Appends new data to an existing file by concatenating NumPy arrays based on time.\n    \"\"\"\n\n    output_files: list[OutputFile]\n\n    file_path: Path\n\n    def __init__(self,\n                 base_data_path: str | Path,\n                 mission: str,\n                 satellite: str,\n                 instrument: str,\n                 kext: str,\n                 file_format: Literal[\".mat\", \".pickle\"] = \".mat\") -&gt; None:\n        \"\"\"Initializes the data organization strategy.\n\n        Parameters:\n            base_data_path (str | Path): The base directory for saving all data.\n            mission (str): The mission name.\n            satellite (str): The satellite name.\n            instrument (str): The instrument name.\n            kext (str): The model extension type. \"TS04\" is remapped to \"T04s\".\n            file_format (Literal[\".mat\", \".pickle\"]): The desired format for the output files.\n        \"\"\"\n        self.base_data_path = Path(base_data_path)\n        self.mission = mission\n        self.satellite = satellite\n        self.instrument = instrument\n\n        # for backwards compatibility\n        if kext == \"TS04\":\n            kext = \"T04s\"\n        self.kext = kext\n\n        self.file_format = file_format\n\n        self.output_files = [\n            OutputFile(\"flux\", [\"time\", \"Flux\"]),\n            OutputFile(\"alpha_and_energy\", [\"time\", \"alpha_local\", \"alpha_eq_model\", \"energy_channels\"]),\n            OutputFile(\"mlt\", [\"time\", \"MLT\"]),\n            OutputFile(\"lstar\", [\"time\", \"Lstar\"]),\n            OutputFile(\"lm\", [\"time\", \"Lm\"]),\n            OutputFile(\"psd\", [\"time\", \"PSD\"]),\n            OutputFile(\"xGEO\", [\"time\", \"xGEO\"]),\n            OutputFile(\"invmu_and_invk\", [\"time\", \"InvMu\", \"InvK\"]),\n            OutputFile(\"bfield\", [\"time\", \"B_eq\", \"B_local\"]),\n            OutputFile(\"R0\", [\"time\", \"R0\"]),\n            OutputFile(\"density\", [\"time\", \"density\"]),\n        ]\n\n        self.data_standard = DataOrgStandard()\n\n    def standardize_variable(self, variable: Variable, name_in_file: str) -&gt; Variable:\n        \"\"\"Standardizes a variable's units and dimensions based on its predefined name.\n\n        This method uses a `match` statement to apply specific unit conversions and\n        dimension checks for a known set of variable names.\n\n        Parameters:\n            variable (Variable): The variable instance to be standardized.\n            name_in_file (str): The predefined name of the variable to use for standardization rules.\n\n        Returns:\n            Variable: The standardized variable instance.\n\n        Raises:\n            ValueError: If an unknown `name_in_file` is encountered.\n        \"\"\"\n        return self.data_standard.standardize_variable(name_in_file, variable)\n\n    def get_time_intervals_to_save(self,\n                                   start_time: datetime | None,\n                                   end_time: datetime | None) -&gt; list[tuple[datetime, datetime]]:\n        \"\"\"Splits the time range into a list of full-month intervals.\n\n        This method iterates from the start month to the end month, creating a new\n        (start, end) tuple for each calendar month.\n\n        Parameters:\n            start_time (datetime | None): The start of the time range.\n            end_time (datetime | None): The end of the time range.\n\n        Returns:\n            list[tuple[datetime, datetime]]: A list of tuples, where each tuple represents a\n                monthly time interval.\n\n        Raises:\n            ValueError: If either `start_time` or `end_time` is not provided.\n        \"\"\"\n        time_intervals: list[tuple[datetime, datetime]] = []\n\n        if start_time is None or end_time is None:\n            msg = \"start_time and end_time must be provided for DataOrgStrategy!\"\n            raise ValueError(msg)\n\n        current_time = start_time.replace(day=1)\n        while current_time &lt;= end_time:\n            year = current_time.year\n            month = current_time.month\n            eom_day = calendar.monthrange(year, month)[1]\n\n            month_start = datetime(year, month, 1, 0, 0, 0, tzinfo=timezone.utc)\n            month_end = datetime(year, month, eom_day, 23, 59, 59, tzinfo=timezone.utc)\n            time_intervals.append((month_start, month_end))\n            current_time = datetime(year + 1, 1, 1, tzinfo=timezone.utc) if month == 12 \\\n                else datetime(year, month + 1, 1, tzinfo=timezone.utc)  # noqa: PLR2004\n\n        return time_intervals\n\n    def get_file_path(self, interval_start: datetime, interval_end: datetime, output_file: OutputFile) -&gt; Path:\n        \"\"\"Generates a structured file path for the given time interval and output file.\n\n        The path follows a specific format:\n        `base_path/MISSION/SATELLITE/Processed_Mat_Files/satellite_instrument_YYYYMMDDtoYYYYMMDD_filename_ver4.mat`\n\n        Parameters:\n            interval_start (datetime): The start of the time interval.\n            interval_end (datetime): The end of the time interval.\n            output_file (OutputFile): The output file configuration.\n\n        Returns:\n            Path: The generated file path.\n        \"\"\"\n        start_year_month_day = interval_start.strftime(\"%Y%m%d\")\n        end_year_month_day = interval_end.strftime(\"%Y%m%d\")\n\n        file_name = f\"{self.satellite.lower()}_{self.instrument.lower()}_{start_year_month_day}to{end_year_month_day}_{output_file.name}\"\n\n        if output_file.name in [\"alpha_and_energy\", \"lstar\", \"lm\", \"invmu_and_invk\", \"mlt\", \"bfield\", \"R0\"]:\n            file_name += f\"_n4_4_{self.kext}\"\n\n        file_name += \"_ver4\" + self.file_format\n\n        return self.base_data_path / self.mission.upper() / self.satellite.lower() / \"Processed_Mat_Files\" / file_name\n\n    def append_data(self, file_path: Path, data_dict_to_save: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Appends new data to an existing file by combining the new and old data dictionaries.\n\n        This method handles `pickle` files specifically, loading the old data, merging it with the\n        new data based on time, and then returning the merged dictionary. It raises an error if\n        the time values are not unique after concatenation.\n\n        Parameters:\n            file_path (Path): The path to the existing file to append to.\n            data_dict_to_save (dict[str, Any]): The dictionary with new data to be added.\n\n        Returns:\n            dict[str, Any]: A new dictionary containing the merged old and new data.\n\n        Raises:\n            ValueError: If a key mismatch occurs between the dictionaries or if the concatenated\n                time array contains non-unique values.\n        \"\"\"\n        with file_path.open(\"rb\") as file:\n            data_dict_old = pickle.load(file)  # noqa: S301\n\n            time_1 = np.squeeze(data_dict_old[\"time\"])\n            time_2 = np.squeeze(data_dict_to_save[\"time\"])\n\n            idx_to_insert = np.searchsorted(time_1, time_2[0])\n\n            time_1_in_2 = np.squeeze(np.isin(time_1, time_2))\n\n            for key, value_1 in data_dict_old.items():\n\n                if key not in data_dict_to_save:\n                    msg = \"Key missmatch when concatenating data dicts!\"\n                    raise ValueError(msg)\n\n                if isinstance(value_1, np.ndarray):\n                    value_1_truncated = value_1[~time_1_in_2]\n\n                    value_2 = data_dict_to_save[key]\n\n                    concatenated_value = value_2 if value_1_truncated.size == 0 \\\n                        else np.insert(value_1_truncated, idx_to_insert, value_2, axis=0)\n\n                    if key == \"time\" and len(np.unique(concatenated_value)) != len(concatenated_value):\n                        msg = \"Time values were not unique when concatinating arrays!\"\n                        raise ValueError(msg)\n                    data_dict_to_save[key] = concatenated_value\n\n                elif isinstance(value_1, dict): # this is the metadata dict\n                    continue\n\n            return data_dict_to_save\n</code></pre>"},{"location":"API_reference/saving_strategies/data_org/#el_paso.saving_strategies.data_org_strategy.DataOrgStrategy.__init__","title":"<code>__init__</code>","text":"<p>Initializes the data organization strategy.</p> <p>Parameters:</p> Name Type Description Default <code>base_data_path</code> <code>str | Path</code> <p>The base directory for saving all data.</p> required <code>mission</code> <code>str</code> <p>The mission name.</p> required <code>satellite</code> <code>str</code> <p>The satellite name.</p> required <code>instrument</code> <code>str</code> <p>The instrument name.</p> required <code>kext</code> <code>str</code> <p>The model extension type. \"TS04\" is remapped to \"T04s\".</p> required <code>file_format</code> <code>Literal['.mat', '.pickle']</code> <p>The desired format for the output files.</p> <code>'.mat'</code> Source code in <code>el_paso/saving_strategies/data_org_strategy.py</code> <pre><code>def __init__(self,\n             base_data_path: str | Path,\n             mission: str,\n             satellite: str,\n             instrument: str,\n             kext: str,\n             file_format: Literal[\".mat\", \".pickle\"] = \".mat\") -&gt; None:\n    \"\"\"Initializes the data organization strategy.\n\n    Parameters:\n        base_data_path (str | Path): The base directory for saving all data.\n        mission (str): The mission name.\n        satellite (str): The satellite name.\n        instrument (str): The instrument name.\n        kext (str): The model extension type. \"TS04\" is remapped to \"T04s\".\n        file_format (Literal[\".mat\", \".pickle\"]): The desired format for the output files.\n    \"\"\"\n    self.base_data_path = Path(base_data_path)\n    self.mission = mission\n    self.satellite = satellite\n    self.instrument = instrument\n\n    # for backwards compatibility\n    if kext == \"TS04\":\n        kext = \"T04s\"\n    self.kext = kext\n\n    self.file_format = file_format\n\n    self.output_files = [\n        OutputFile(\"flux\", [\"time\", \"Flux\"]),\n        OutputFile(\"alpha_and_energy\", [\"time\", \"alpha_local\", \"alpha_eq_model\", \"energy_channels\"]),\n        OutputFile(\"mlt\", [\"time\", \"MLT\"]),\n        OutputFile(\"lstar\", [\"time\", \"Lstar\"]),\n        OutputFile(\"lm\", [\"time\", \"Lm\"]),\n        OutputFile(\"psd\", [\"time\", \"PSD\"]),\n        OutputFile(\"xGEO\", [\"time\", \"xGEO\"]),\n        OutputFile(\"invmu_and_invk\", [\"time\", \"InvMu\", \"InvK\"]),\n        OutputFile(\"bfield\", [\"time\", \"B_eq\", \"B_local\"]),\n        OutputFile(\"R0\", [\"time\", \"R0\"]),\n        OutputFile(\"density\", [\"time\", \"density\"]),\n    ]\n\n    self.data_standard = DataOrgStandard()\n</code></pre>"},{"location":"API_reference/saving_strategies/monthly_h5/","title":"Monthly H5 Strategy","text":""},{"location":"API_reference/saving_strategies/monthly_h5/#el_paso.saving_strategies.monthly_h5_strategy.MonthlyH5Strategy","title":"<code>el_paso.saving_strategies.monthly_h5_strategy.MonthlyH5Strategy</code>","text":"<p>               Bases: <code>SavingStrategy</code></p> <p>A saving strategy that organizes and saves data into a series of monthly HDF5 files.</p> <p>This strategy partitions data by month, with each month's data being saved to a separate HDF5 file. It standardizes variables to a consistent set of units and dimensions before saving and performs consistency checks to ensure data integrity. The file name is constructed from a user-defined stem, a date range, and a magnetic field model identifier.</p> <p>Attributes:</p> Name Type Description <code>output_files</code> <code>list[OutputFile]</code> <p>Pre-defined list of files to be saved, each containing a comprehensive list of variables to be included.</p> <code>base_data_path</code> <code>Path</code> <p>The root directory for all saved <code>.h5</code> files.</p> <code>file_name_stem</code> <code>str</code> <p>The base name for the output files.</p> <code>mag_field</code> <code>MagneticFieldLiteral</code> <p>A string specifying the magnetic field model used.</p> <code>data_standard</code> <code>DataStandard</code> <p>An instance of a data standard class that handles the standardization of variables.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the strategy with file paths, names, and a magnetic field model.</p> <code>get_time_intervals_to_save</code> <p>Splits a given time range into a list of monthly intervals.</p> <code>get_file_path</code> <p>Generates the file path for a monthly HDF5 file.</p> <code>standardize_variable</code> <p>Standardizes a variable's units, dimensions, and shape.</p> Source code in <code>el_paso/saving_strategies/monthly_h5_strategy.py</code> <pre><code>class MonthlyH5Strategy(SavingStrategy):\n    \"\"\"A saving strategy that organizes and saves data into a series of monthly HDF5 files.\n\n    This strategy partitions data by month, with each month's data being saved to\n    a separate HDF5 file. It standardizes variables to a consistent set of units\n    and dimensions before saving and performs consistency checks to ensure data\n    integrity. The file name is constructed from a user-defined stem, a date range,\n    and a magnetic field model identifier.\n\n    Attributes:\n        output_files (list[OutputFile]): Pre-defined list of files to be saved,\n            each containing a comprehensive list of variables to be included.\n        base_data_path (Path): The root directory for all saved `.h5` files.\n        file_name_stem (str): The base name for the output files.\n        mag_field (ep.processing.magnetic_field_utils.MagneticFieldLiteral):\n            A string specifying the magnetic field model used.\n        data_standard (DataStandard): An instance of a data standard class\n            that handles the standardization of variables.\n\n    Methods:\n        __init__: Initializes the strategy with file paths, names, and a magnetic field model.\n        get_time_intervals_to_save: Splits a given time range into a list of monthly intervals.\n        get_file_path: Generates the file path for a monthly HDF5 file.\n        standardize_variable: Standardizes a variable's units, dimensions, and shape.\n    \"\"\"\n\n    output_files:list[OutputFile]\n\n    file_path:Path\n\n    def __init__(self,\n                 base_data_path:str|Path,\n                 file_name_stem:str,\n                 mag_field:ep.processing.magnetic_field_utils.MagneticFieldLiteral,\n                 data_standard: DataStandard|None = None) -&gt; None:\n        \"\"\"Initializes the MonthlyH5Strategy.\n\n        Parameters:\n            base_data_path (str | Path): The base directory for saving all data.\n            file_name_stem (str): The base name for the output files.\n            mag_field (ep.processing.magnetic_field_utils.MagneticFieldLiteral):\n                The magnetic field model used, e.g., 'TS04'.\n            data_standard (DataStandard | None): An optional data standard instance.\n                If `None`, `ep.data_standards.PRBEMStandard` is used by default.\n        \"\"\"\n        self.base_data_path = Path(base_data_path)\n        self.file_name_stem = file_name_stem\n        self.mag_field = mag_field\n\n        if data_standard is None:\n            data_standard = ep.data_standards.PRBEMStandard()\n        self.data_standard = data_standard\n\n        self.output_files = [\n            OutputFile(\"full\", [\"time\",\n                                \"flux/FEDU\", \"flux/FEDO\", \"flux/alpha_eq\", \"flux/energy\", \"flux/alpha_local\",\n                                \"position/xGEO\", f\"position/{mag_field}/MLT\", f\"position/{mag_field}/R0\",\n                                f\"position/{mag_field}/Lstar\", f\"position/{mag_field}/Lm\",\n                                f\"mag_field/{mag_field}/B_eq\", f\"mag_field/{mag_field}/B_local\",\n                                \"psd/PSD\", f\"psd/{mag_field}/inv_mu\", f\"psd/{mag_field}/inv_K\",\n                                \"density/density_local\", f\"density/{mag_field}/density_eq\",\n            ], save_incomplete=True),\n        ]\n\n    def get_time_intervals_to_save(self,\n                                   start_time:datetime|None,\n                                   end_time:datetime|None) -&gt; list[tuple[datetime, datetime]]:\n        \"\"\"Splits the provided time range into a list of full-month intervals.\n\n        This method generates a list of (start_datetime, end_datetime) tuples, where each tuple\n        represents a single calendar month.\n\n        Parameters:\n            start_time (datetime | None): The start time of the data range.\n            end_time (datetime | None): The end time of the data range.\n\n        Returns:\n            list[tuple[datetime, datetime]]: A list of tuples, each defining a monthly interval.\n\n        Raises:\n            ValueError: If either `start_time` or `end_time` is not provided.\n        \"\"\"\n        time_intervals:list[tuple[datetime, datetime]] = []\n\n        if start_time is None or end_time is None:\n            msg = \"start_time and end_time must be provided for MonthlyH5Strategy!\"\n            raise ValueError(msg)\n\n        current_time = start_time.replace(day=1)\n        while current_time &lt;= end_time:\n            year = current_time.year\n            month = current_time.month\n            eom_day = calendar.monthrange(year, month)[1]\n\n            month_start = datetime(year, month, 1, 0, 0, 0, tzinfo=timezone.utc)\n            month_end = datetime(year, month, eom_day, 23, 59, 59, tzinfo=timezone.utc)\n            time_intervals.append((month_start, month_end))\n            current_time = datetime(year + 1, 1, 1, tzinfo=timezone.utc) if month == 12 else \\\n                datetime(year, month + 1, 1, tzinfo=timezone.utc)  # noqa: PLR2004\n\n        return time_intervals\n\n    def get_file_path(self, interval_start:datetime, interval_end:datetime, output_file:OutputFile) -&gt; Path:  # noqa: ARG002\n        \"\"\"Generates a structured file path for the HDF5 file.\n\n        The file name is constructed from a predefined stem, the date range, and the magnetic\n        field model, with a `.h5` extension.\n\n        Parameters:\n            interval_start (datetime): The start of the time interval.\n            interval_end (datetime): The end of the time interval.\n            output_file (OutputFile): The configuration for the output file. (ignored)\n\n        Returns:\n            Path: The full file path for the HDF5 file.\n        \"\"\"\n        start_year_month_day = interval_start.strftime(\"%Y%m%d\")\n        end_year_month_day = interval_end.strftime(\"%Y%m%d\")\n\n        file_name = f\"{self.file_name_stem}_{start_year_month_day}to{end_year_month_day}_{self.mag_field}.h5\"\n\n        return self.base_data_path / file_name\n\n    def standardize_variable(self, variable: ep.Variable, name_in_file: str) -&gt; ep.Variable:\n        \"\"\"Standardizes a variable's units and dimensions by delegating to a DataStandard instance.\n\n        This method acts as a wrapper, passing the variable and its file name to the\n        `standardize_variable` method of the `data_standard` attribute.\n        Parameters:\n            variable (ep.Variable): The variable instance to be standardized.\n            name_in_file (str): The name of the variable as it appears in the file.\n\n        Returns:\n            ep.Variable: The standardized variable.\n        \"\"\"\n        return self.data_standard.standardize_variable(name_in_file, variable)\n</code></pre>"},{"location":"API_reference/saving_strategies/monthly_h5/#el_paso.saving_strategies.monthly_h5_strategy.MonthlyH5Strategy.__init__","title":"<code>__init__</code>","text":"<p>Initializes the MonthlyH5Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>base_data_path</code> <code>str | Path</code> <p>The base directory for saving all data.</p> required <code>file_name_stem</code> <code>str</code> <p>The base name for the output files.</p> required <code>mag_field</code> <code>MagneticFieldLiteral</code> <p>The magnetic field model used, e.g., 'TS04'.</p> required <code>data_standard</code> <code>DataStandard | None</code> <p>An optional data standard instance. If <code>None</code>, <code>ep.data_standards.PRBEMStandard</code> is used by default.</p> <code>None</code> Source code in <code>el_paso/saving_strategies/monthly_h5_strategy.py</code> <pre><code>def __init__(self,\n             base_data_path:str|Path,\n             file_name_stem:str,\n             mag_field:ep.processing.magnetic_field_utils.MagneticFieldLiteral,\n             data_standard: DataStandard|None = None) -&gt; None:\n    \"\"\"Initializes the MonthlyH5Strategy.\n\n    Parameters:\n        base_data_path (str | Path): The base directory for saving all data.\n        file_name_stem (str): The base name for the output files.\n        mag_field (ep.processing.magnetic_field_utils.MagneticFieldLiteral):\n            The magnetic field model used, e.g., 'TS04'.\n        data_standard (DataStandard | None): An optional data standard instance.\n            If `None`, `ep.data_standards.PRBEMStandard` is used by default.\n    \"\"\"\n    self.base_data_path = Path(base_data_path)\n    self.file_name_stem = file_name_stem\n    self.mag_field = mag_field\n\n    if data_standard is None:\n        data_standard = ep.data_standards.PRBEMStandard()\n    self.data_standard = data_standard\n\n    self.output_files = [\n        OutputFile(\"full\", [\"time\",\n                            \"flux/FEDU\", \"flux/FEDO\", \"flux/alpha_eq\", \"flux/energy\", \"flux/alpha_local\",\n                            \"position/xGEO\", f\"position/{mag_field}/MLT\", f\"position/{mag_field}/R0\",\n                            f\"position/{mag_field}/Lstar\", f\"position/{mag_field}/Lm\",\n                            f\"mag_field/{mag_field}/B_eq\", f\"mag_field/{mag_field}/B_local\",\n                            \"psd/PSD\", f\"psd/{mag_field}/inv_mu\", f\"psd/{mag_field}/inv_K\",\n                            \"density/density_local\", f\"density/{mag_field}/density_eq\",\n        ], save_incomplete=True),\n    ]\n</code></pre>"},{"location":"API_reference/saving_strategies/monthly_netcdf/","title":"Monthly NetCDF Strategy","text":""},{"location":"API_reference/saving_strategies/monthly_netcdf/#el_paso.saving_strategies.monthly_netcdf_strategy.MonthlyNetCDFStrategy","title":"<code>el_paso.saving_strategies.monthly_netcdf_strategy.MonthlyNetCDFStrategy</code>","text":"<p>               Bases: <code>MonthlyH5Strategy</code></p> <p>A saving strategy that saves data to monthly NetCDF files.</p> <p>This strategy organizes and saves processed scientific data into a series of NetCDF files, partitioned by month. It inherits from <code>MonthlyH5Strategy</code> but overrides the file saving logic to use the NetCDF format, which is widely used in climate and earth science for storing array-oriented scientific data.</p> <p>The strategy standardizes variables based on a provided <code>DataStandard</code> and structures the output files with a consistent naming convention that includes the file stem, date range, and magnetic field models used. It supports multiple magnetic field models and automatically configures the output files and their dependencies.</p> Source code in <code>el_paso/saving_strategies/monthly_netcdf_strategy.py</code> <pre><code>class MonthlyNetCDFStrategy(MonthlyH5Strategy):\n    \"\"\"A saving strategy that saves data to monthly NetCDF files.\n\n    This strategy organizes and saves processed scientific data into a series of\n    NetCDF files, partitioned by month. It inherits from `MonthlyH5Strategy` but\n    overrides the file saving logic to use the NetCDF format, which is widely used\n    in climate and earth science for storing array-oriented scientific data.\n\n    The strategy standardizes variables based on a provided `DataStandard` and\n    structures the output files with a consistent naming convention that includes\n    the file stem, date range, and magnetic field models used. It supports\n    multiple magnetic field models and automatically configures the output files\n    and their dependencies.\n    \"\"\"\n\n    output_files:list[OutputFile]\n\n    file_path:Path\n    dependency_dict: dict[str,list[str]]\n\n    def __init__(self,\n                 base_data_path:str|Path,\n                 file_name_stem:str,\n                 mag_field:MagneticFieldLiteral|list[MagneticFieldLiteral],\n                 data_standard:DataStandard|None = None) -&gt; None:\n        \"\"\"Initializes the monthly NetCDF saving strategy.\n\n        Parameters:\n            base_data_path (str | Path): The base directory where the output NetCDF files will be saved.\n            file_name_stem (str): The base name for the output files (e.g., \"my_data\").\n            mag_field (MagneticFieldLiteral | list[MagneticFieldLiteral]):\n                A string or list of strings specifying the magnetic field models used.\n            data_standard (DataStandard | None):\n                An optional `DataStandard` instance to use for standardizing variables.\n                If `None`, `ep.data_standards.PRBEMStandard` is used by default.\n        \"\"\"\n        if not isinstance(mag_field, list):\n            mag_field = [mag_field]\n\n        if data_standard is None:\n            data_standard = ep.data_standards.PRBEMStandard()\n\n        self.base_data_path = Path(base_data_path)\n        self.file_name_stem = file_name_stem\n        self.mag_field = mag_field\n        self.standard = data_standard\n\n        output_file_entries = [\"time\", \"flux/FEDU\", \"flux/FEDO\", \"flux/alpha_eq\", \"flux/energy\", \"flux/alpha_local\",\n                               \"position/xGEO\", \"density/density_local\"]\n\n        for single_mag_field in mag_field:\n            output_file_entries.extend([f\"position/{single_mag_field}/MLT\", f\"position/{single_mag_field}/R0\",\n                                        f\"position/{single_mag_field}/Lstar\", f\"position/{single_mag_field}/Lm\",\n                                        f\"mag_field/{single_mag_field}/B_eq\", f\"mag_field/{single_mag_field}/B_local\",\n                                        f\"psd/{single_mag_field}/inv_mu\", f\"psd/{single_mag_field}/inv_K\",\n                                        f\"density/{single_mag_field}/density_eq\"])\n        self.output_files = [\n            OutputFile(\"full\", output_file_entries, save_incomplete=True),\n        ]\n\n\n        self.dependency_dict = {\n            \"time\": [\"time\"],\n            \"flux/FEDU\": [\"time\", \"energy\", \"alpha\"],\n            \"flux/FEDO\": [\"time\", \"energy\"],\n            \"flux/alpha_eq\": [\"time\", \"alpha\"],\n            \"flux/energy\": [\"time\", \"energy\"],\n            \"flux/alpha_local\": [\"time\", \"alpha\"],\n            \"position/xGEO\": [\"time\", \"xGEO_components\"],\n            \"psd/PSD\": [\"time\", \"energy\", \"alpha\"],\n            \"density/density_local\": [\"time\"],\n        }\n\n        for single_mag_field in mag_field:\n            self.dependency_dict |= {\n                f\"position/{single_mag_field}/MLT\": [\"time\"],\n                f\"position/{single_mag_field}/R0\": [\"time\"],\n                f\"position/{single_mag_field}/Lstar\": [\"time\", \"alpha\"],\n                f\"position/{single_mag_field}/Lm\": [\"time\", \"alpha\"],\n                f\"mag_field/{single_mag_field}/B_eq\": [\"time\"],\n                f\"mag_field/{single_mag_field}/B_local\": [\"time\"],\n                f\"psd/{single_mag_field}/inv_mu\": [\"time\", \"energy\", \"alpha\"],\n                f\"psd/{single_mag_field}/inv_K\": [\"time\", \"alpha\"],\n                f\"density/{single_mag_field}/density_eq\": [\"time\"],\n            }\n\n    def get_file_path(self, interval_start:datetime, interval_end:datetime, output_file:OutputFile) -&gt; Path:  # noqa: ARG002\n        \"\"\"Generates the file path for a monthly NetCDF file.\n\n        The file name is constructed from the `file_name_stem`, the date range of the interval,\n        and the specified magnetic field models, with a `.nc` extension.\n\n        Parameters:\n            interval_start (datetime): The start of the time interval.\n            interval_end (datetime): The end of the time interval.\n            output_file (OutputFile): The configuration for the output file.\n\n        Returns:\n            Path: The full file path for the NetCDF file.\n        \"\"\"\n        start_year_month_day = interval_start.strftime(\"%Y%m%d\")\n        end_year_month_day = interval_end.strftime(\"%Y%m%d\")\n\n        file_name = f\"{self.file_name_stem}_{start_year_month_day}to{end_year_month_day}\"\n\n        for mag_field in self.mag_field:\n            file_name += f\"_{mag_field}\"\n\n        file_name += \".nc\"\n\n        return self.base_data_path / file_name\n\n    def standardize_variable(self, variable: ep.Variable, name_in_file: str) -&gt; ep.Variable:\n        \"\"\"Standardizes a variable based on the configured `DataStandard`.\n\n        This method delegates the standardization process to a `DataStandard` instance,\n        ensuring that the variable's units and dimensions are consistent with the\n        defined standard.\n\n        Parameters:\n            variable (ep.Variable): The variable instance to be standardized.\n            name_in_file (str): The name of the variable as it will appear in the file.\n\n        Returns:\n            ep.Variable: The standardized variable.\n        \"\"\"\n        return self.standard.standardize_variable(name_in_file, variable)\n\n    def save_single_file(self, file_path:Path, dict_to_save:dict[str,Any], *, append:bool=False) -&gt; None:  # noqa: C901\n        \"\"\"Saves a dictionary of variables to a single NetCDF file.\n\n        This method creates a new NetCDF4 file, defines dimensions based on the data,\n        and writes each variable as a dataset. It also attaches metadata as attributes\n        to the datasets.\n\n        Parameters:\n            file_path (Path): The path to the file where the data will be saved.\n            dict_to_save (dict[str, Any]): The dictionary containing variable data.\n            append (bool, optional): If `True`, attempts to append data to an existing file.\n                Currently, this functionality is not fully implemented for NetCDF,\n                so it defaults to creating a new file.\n\n        Note:\n            This method only supports creating new files (`append=False`) and does not\n            handle appending to an existing NetCDF file.\n        \"\"\"\n        logger.info(f\"Saving file {file_path.name}...\")\n\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n\n        if file_path.exists() and append:\n            dict_to_save = self.append_data(file_path, dict_to_save)\n\n        with nC.Dataset(file_path, \"w\", format=\"NETCDF4\") as file:\n\n            size_time = dict_to_save[\"time\"].shape[0]\n            size_pitch_angle:int = 0\n            size_energy:int = 0\n\n            if \"flux/alpha_eq\" in dict_to_save and dict_to_save[\"flux/alpha_eq\"].size &gt; 0:\n                size_pitch_angle = dict_to_save[\"flux/alpha_eq\"].shape[1]\n            elif \"flux/alpha_local\" in dict_to_save and dict_to_save[\"flux/alpha_local\"].size &gt; 0:\n                size_pitch_angle = dict_to_save[\"flux/alpha_local\"].shape[1]\n\n            if \"flux/energy\" in dict_to_save and dict_to_save[\"flux/energy\"].size &gt; 0:\n                size_energy = dict_to_save[\"flux/energy\"].shape[1]\n\n            file.createDimension(\"time\", size_time)\n            file.createDimension(\"alpha\", size_pitch_angle)\n            file.createDimension(\"energy\", size_energy)\n\n            if \"position/xGEO\" in dict_to_save and dict_to_save[\"position/xGEO\"].size &gt; 0:\n                file.createDimension(\"xGEO_components\", 3)\n\n            for path, value in dict_to_save.items():\n\n                if path == \"metadata\":\n                    continue\n\n                if value.size == 0:\n                    continue\n\n                path_parts = path.split(\"/\")\n                groups = path_parts[:-1]\n                dataset_name = path_parts[-1]\n\n                curr_hierachy = file\n                for group in groups:\n                    if group not in curr_hierachy.groups:\n                        curr_hierachy = curr_hierachy.createGroup(group) # type: ignore[reportUnknownVariableType]\n                    else:\n                        curr_hierachy = typing.cast(\"nC.Group\", curr_hierachy[group])\n\n                data_set = typing.cast(\"nC.Variable[Any]\", curr_hierachy.createVariable( # type: ignore[reportUnknownMemberType]\n                    dataset_name,\n                    \"f4\",\n                    self.dependency_dict[path],\n                    zlib=True, complevel=5, shuffle=True))\n\n                data_set[:,...] = value\n\n                if path in dict_to_save[\"metadata\"]:\n                    metadata = dict_to_save[\"metadata\"][path]\n                    data_set.units = metadata[\"unit\"]\n                    data_set.source = metadata[\"source_files\"]\n                    data_set.history = metadata[\"processing_notes\"]\n                    data_set.description = metadata[\"description\"]\n</code></pre>"},{"location":"API_reference/saving_strategies/monthly_netcdf/#el_paso.saving_strategies.monthly_netcdf_strategy.MonthlyNetCDFStrategy.__init__","title":"<code>__init__</code>","text":"<p>Initializes the monthly NetCDF saving strategy.</p> <p>Parameters:</p> Name Type Description Default <code>base_data_path</code> <code>str | Path</code> <p>The base directory where the output NetCDF files will be saved.</p> required <code>file_name_stem</code> <code>str</code> <p>The base name for the output files (e.g., \"my_data\").</p> required <code>mag_field</code> <code>MagneticFieldLiteral | list[MagneticFieldLiteral]</code> <p>A string or list of strings specifying the magnetic field models used.</p> required <code>data_standard</code> <code>DataStandard | None</code> <p>An optional <code>DataStandard</code> instance to use for standardizing variables. If <code>None</code>, <code>ep.data_standards.PRBEMStandard</code> is used by default.</p> <code>None</code> Source code in <code>el_paso/saving_strategies/monthly_netcdf_strategy.py</code> <pre><code>def __init__(self,\n             base_data_path:str|Path,\n             file_name_stem:str,\n             mag_field:MagneticFieldLiteral|list[MagneticFieldLiteral],\n             data_standard:DataStandard|None = None) -&gt; None:\n    \"\"\"Initializes the monthly NetCDF saving strategy.\n\n    Parameters:\n        base_data_path (str | Path): The base directory where the output NetCDF files will be saved.\n        file_name_stem (str): The base name for the output files (e.g., \"my_data\").\n        mag_field (MagneticFieldLiteral | list[MagneticFieldLiteral]):\n            A string or list of strings specifying the magnetic field models used.\n        data_standard (DataStandard | None):\n            An optional `DataStandard` instance to use for standardizing variables.\n            If `None`, `ep.data_standards.PRBEMStandard` is used by default.\n    \"\"\"\n    if not isinstance(mag_field, list):\n        mag_field = [mag_field]\n\n    if data_standard is None:\n        data_standard = ep.data_standards.PRBEMStandard()\n\n    self.base_data_path = Path(base_data_path)\n    self.file_name_stem = file_name_stem\n    self.mag_field = mag_field\n    self.standard = data_standard\n\n    output_file_entries = [\"time\", \"flux/FEDU\", \"flux/FEDO\", \"flux/alpha_eq\", \"flux/energy\", \"flux/alpha_local\",\n                           \"position/xGEO\", \"density/density_local\"]\n\n    for single_mag_field in mag_field:\n        output_file_entries.extend([f\"position/{single_mag_field}/MLT\", f\"position/{single_mag_field}/R0\",\n                                    f\"position/{single_mag_field}/Lstar\", f\"position/{single_mag_field}/Lm\",\n                                    f\"mag_field/{single_mag_field}/B_eq\", f\"mag_field/{single_mag_field}/B_local\",\n                                    f\"psd/{single_mag_field}/inv_mu\", f\"psd/{single_mag_field}/inv_K\",\n                                    f\"density/{single_mag_field}/density_eq\"])\n    self.output_files = [\n        OutputFile(\"full\", output_file_entries, save_incomplete=True),\n    ]\n\n\n    self.dependency_dict = {\n        \"time\": [\"time\"],\n        \"flux/FEDU\": [\"time\", \"energy\", \"alpha\"],\n        \"flux/FEDO\": [\"time\", \"energy\"],\n        \"flux/alpha_eq\": [\"time\", \"alpha\"],\n        \"flux/energy\": [\"time\", \"energy\"],\n        \"flux/alpha_local\": [\"time\", \"alpha\"],\n        \"position/xGEO\": [\"time\", \"xGEO_components\"],\n        \"psd/PSD\": [\"time\", \"energy\", \"alpha\"],\n        \"density/density_local\": [\"time\"],\n    }\n\n    for single_mag_field in mag_field:\n        self.dependency_dict |= {\n            f\"position/{single_mag_field}/MLT\": [\"time\"],\n            f\"position/{single_mag_field}/R0\": [\"time\"],\n            f\"position/{single_mag_field}/Lstar\": [\"time\", \"alpha\"],\n            f\"position/{single_mag_field}/Lm\": [\"time\", \"alpha\"],\n            f\"mag_field/{single_mag_field}/B_eq\": [\"time\"],\n            f\"mag_field/{single_mag_field}/B_local\": [\"time\"],\n            f\"psd/{single_mag_field}/inv_mu\": [\"time\", \"energy\", \"alpha\"],\n            f\"psd/{single_mag_field}/inv_K\": [\"time\", \"alpha\"],\n            f\"density/{single_mag_field}/density_eq\": [\"time\"],\n        }\n</code></pre>"},{"location":"API_reference/saving_strategies/single_file/","title":"Single File Strategy","text":""},{"location":"API_reference/saving_strategies/single_file/#el_paso.saving_strategies.single_file_strategy.SingleFileStrategy","title":"<code>el_paso.saving_strategies.single_file_strategy.SingleFileStrategy</code>","text":"<p>               Bases: <code>SavingStrategy</code></p> <p>A concrete saving strategy that saves all data to a single file.</p> <p>This strategy implements the <code>SavingStrategy</code> abstract methods to manage saving all variables for the entire time range into a single output file. It is a simple, non-partitioning approach.</p> <p>Attributes:</p> Name Type Description <code>file_path</code> <code>Path</code> <p>The path to the single output file where all data will be saved.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the strategy with the file path and sets up the single output file configuration.</p> <code>get_time_intervals_to_save</code> <p>Returns the entire time range as a single interval.</p> <code>get_file_path</code> <p>Always returns the pre-defined single file path.</p> <code>standardize_variable</code> <p>Passes the variable through without any standardization.</p> Source code in <code>el_paso/saving_strategies/single_file_strategy.py</code> <pre><code>class SingleFileStrategy(SavingStrategy):\n    \"\"\"A concrete saving strategy that saves all data to a single file.\n\n    This strategy implements the `SavingStrategy` abstract methods to manage saving all variables\n    for the entire time range into a single output file. It is a simple, non-partitioning approach.\n\n    Attributes:\n        file_path (Path): The path to the single output file where all data will be saved.\n\n    Methods:\n        __init__(file_path): Initializes the strategy with the file path and sets up the single output file configuration.\n        get_time_intervals_to_save: Returns the entire time range as a single interval.\n        get_file_path: Always returns the pre-defined single file path.\n        standardize_variable: Passes the variable through without any standardization.\n    \"\"\"\n\n    map_standard_name: dict[str, str]\n    output_files: list[OutputFile]\n\n    file_path: Path\n\n    def __init__(self, file_path: str | Path) -&gt; None:\n        \"\"\"Initializes the SingleFileStrategy with the specified file path.\n\n        Parameters:\n            file_path (str | Path): The full path to the output file.\n        \"\"\"\n        self.file_path = Path(file_path)\n        self.output_files = [OutputFile(self.file_path.name, [])]\n\n        self.map_standard_name = {}\n\n    def get_time_intervals_to_save(self,\n                                   start_time: datetime|None,\n                                   end_time: datetime|None) -&gt; list[tuple[datetime, datetime]]:\n        \"\"\"Returns the entire time range as a single interval.\n\n        This strategy does not split data by time; it saves everything in one go.\n\n        Parameters:\n            start_time (datetime): The start time of the data range.\n            end_time (datetime): The end time of the data range.\n\n        Returns:\n            list[tuple[datetime, datetime]]: A list containing a single tuple with the start and end times.\n        \"\"\"\n        if start_time is None or end_time is None:\n            msg = \"Both start_time and end_time must be provided.\"\n            raise ValueError(msg)\n\n        return [(start_time, end_time)]\n\n    def get_file_path(self, interval_start: datetime, interval_end: datetime, output_file: OutputFile) -&gt; Path:\n        \"\"\"Returns the pre-defined single file path, ignoring the interval.\n\n        This method ensures all data is saved to the same file, regardless of the time interval.\n\n        Parameters:\n            interval_start (datetime): The start of the time interval (ignored).\n            interval_end (datetime): The end of the time interval (ignored).\n            output_file (OutputFile): The output file configuration (ignored).\n\n        Returns:\n            Path: The `file_path` of this strategy instance.\n        \"\"\"\n        return self.file_path\n\n    def standardize_variable(self, variable: Variable, name_in_file: str) -&gt; Variable:\n        \"\"\"Does not modify the variable.\n\n        This strategy does not perform any specific standardization on the variables before saving.\n\n        Parameters:\n            variable (Variable): The variable instance to be standardized.\n            name_in_file (str): The name of the variable as it appears in the file (ignored).\n\n        Returns:\n            Variable: The original variable instance, unchanged.\n        \"\"\"\n        return variable\n</code></pre>"},{"location":"API_reference/saving_strategies/single_file/#el_paso.saving_strategies.single_file_strategy.SingleFileStrategy.__init__","title":"<code>__init__</code>","text":"<p>Initializes the SingleFileStrategy with the specified file path.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>The full path to the output file.</p> required Source code in <code>el_paso/saving_strategies/single_file_strategy.py</code> <pre><code>def __init__(self, file_path: str | Path) -&gt; None:\n    \"\"\"Initializes the SingleFileStrategy with the specified file path.\n\n    Parameters:\n        file_path (str | Path): The full path to the output file.\n    \"\"\"\n    self.file_path = Path(file_path)\n    self.output_files = [OutputFile(self.file_path.name, [])]\n\n    self.map_standard_name = {}\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/","title":"General Utilities","text":""},{"location":"API_reference/utilities/general_utilities/#el_paso.utils","title":"<code>el_paso.utils</code>","text":""},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.Hashabledict","title":"<code>Hashabledict</code>","text":"<p>               Bases: <code>dict[Any, Any]</code></p> <p>A dictionary subclass that is hashable.</p> <p>This class enables a dictionary to be used in sets or as keys in other dictionaries by providing a custom hash implementation based on its contents.</p> Source code in <code>el_paso/utils.py</code> <pre><code>class Hashabledict(dict[Any,Any]):\n    \"\"\"A dictionary subclass that is hashable.\n\n    This class enables a dictionary to be used in sets or as keys in other dictionaries\n    by providing a custom hash implementation based on its contents.\n    \"\"\"\n    def __hash__(self) -&gt; int:  # type: ignore[reportIncompatibleVariableOverride]\n        \"\"\"Computes a hash value for the dictionary.\n\n        The hash is computed based on the frozensets of the dictionary's keys\n        and values. This ensures that two `Hashabledict` instances with the same\n        key-value pairs will have the same hash, regardless of the order of\n        insertion.\n\n        Returns:\n            int: The hash value of the dictionary.\n        \"\"\"\n        return hash((frozenset(self), frozenset(self.itervalues()))) # type: ignore[reportAttributeAccessIssue]\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.Hashabledict.__hash__","title":"<code>__hash__</code>","text":"<p>Computes a hash value for the dictionary.</p> <p>The hash is computed based on the frozensets of the dictionary's keys and values. This ensures that two <code>Hashabledict</code> instances with the same key-value pairs will have the same hash, regardless of the order of insertion.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The hash value of the dictionary.</p> Source code in <code>el_paso/utils.py</code> <pre><code>def __hash__(self) -&gt; int:  # type: ignore[reportIncompatibleVariableOverride]\n    \"\"\"Computes a hash value for the dictionary.\n\n    The hash is computed based on the frozensets of the dictionary's keys\n    and values. This ensures that two `Hashabledict` instances with the same\n    key-value pairs will have the same hash, regardless of the order of\n    insertion.\n\n    Returns:\n        int: The hash value of the dictionary.\n    \"\"\"\n    return hash((frozenset(self), frozenset(self.itervalues()))) # type: ignore[reportAttributeAccessIssue]\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.assert_n_dim","title":"<code>assert_n_dim</code>","text":"<p>Asserts that a variable's data has a specific number of dimensions.</p> <p>Raises a <code>ValueError</code> if the provided variable's data does not match the expected number of dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Variable</code> <p>The variable instance to check.</p> required <code>n_dims</code> <code>int</code> <p>The expected number of dimensions.</p> required <code>name_in_file</code> <code>str</code> <p>The name of the variable, used in the error message.</p> required Source code in <code>el_paso/utils.py</code> <pre><code>def assert_n_dim(var: ep.Variable, n_dims:int, name_in_file:str) -&gt; None:\n    \"\"\"Asserts that a variable's data has a specific number of dimensions.\n\n    Raises a `ValueError` if the provided variable's data does not match the\n    expected number of dimensions.\n\n    Parameters:\n        var (ep.Variable): The variable instance to check.\n        n_dims (int): The expected number of dimensions.\n        name_in_file (str): The name of the variable, used in the error message.\n    \"\"\"\n    provided = var.get_data().ndim\n\n    if provided != n_dims:\n        msg = (f\"Encountered dimension missmatch for variable with name {name_in_file}:\"\n               f\"should be {n_dims}, got: {provided}!\")\n        raise ValueError(msg)\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.datenum_to_datetime","title":"<code>datenum_to_datetime</code>","text":"<p>Converts a MATLAB datenum value to a timezone-aware datetime object.</p> <p>This function leverages pandas to convert the datenum (days since year 0) into a UTC-aware datetime object.</p> <p>Parameters:</p> Name Type Description Default <code>datenum_val</code> <code>float</code> <p>The MATLAB datenum value.</p> required <p>Returns:</p> Name Type Description <code>datetime</code> <code>datetime</code> <p>The converted datetime object with UTC timezone.</p> Source code in <code>el_paso/utils.py</code> <pre><code>def datenum_to_datetime(datenum_val: float) -&gt; datetime:\n    \"\"\"Converts a MATLAB datenum value to a timezone-aware datetime object.\n\n    This function leverages pandas to convert the datenum (days since year 0)\n    into a UTC-aware datetime object.\n\n    Parameters:\n        datenum_val (float): The MATLAB datenum value.\n\n    Returns:\n        datetime: The converted datetime object with UTC timezone.\n    \"\"\"\n    return pd.to_datetime(datenum_val-719529, unit=\"D\", origin=pd.Timestamp(\"1970-01-01\")).to_pydatetime().replace(tzinfo=timezone.utc)\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.datetime_to_datenum","title":"<code>datetime_to_datenum</code>","text":"<p>Converts a datetime object to a MATLAB datenum value.</p> <p>This function calculates the datenum value, which represents the number of days since year 0, including a fractional component for the time of day.</p> <p>Parameters:</p> Name Type Description Default <code>datetime_val</code> <code>datetime</code> <p>The datetime object to convert.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The corresponding MATLAB datenum value.</p> Source code in <code>el_paso/utils.py</code> <pre><code>def datetime_to_datenum(datetime_val: datetime) -&gt; float:\n    \"\"\"Converts a datetime object to a MATLAB datenum value.\n\n    This function calculates the datenum value, which represents the number of days\n    since year 0, including a fractional component for the time of day.\n\n    Parameters:\n        datetime_val (datetime): The datetime object to convert.\n\n    Returns:\n        float: The corresponding MATLAB datenum value.\n    \"\"\"\n    mdn = datetime_val + timedelta(days = 366)\n    dt = datetime(datetime_val.year, datetime_val.month, datetime_val.day, 0, 0, 0, tzinfo=timezone.utc)\n    frac = (datetime_val - dt).seconds / (24.0 * 60.0 * 60.0)\n\n    return mdn.toordinal() + round(frac, 6)\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.enforce_utc_timezone","title":"<code>enforce_utc_timezone</code>","text":"<p>Ensures a datetime object has UTC timezone information.</p> <p>If the provided datetime object is naive (lacks timezone info), it is assigned the UTC timezone. If it already has a timezone, it is returned unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>datetime</code> <p>The datetime object to process.</p> required <p>Returns:</p> Name Type Description <code>datetime</code> <code>datetime</code> <p>The datetime object with <code>timezone.utc</code> assigned.</p> Source code in <code>el_paso/utils.py</code> <pre><code>def enforce_utc_timezone(time:datetime) -&gt; datetime:\n    \"\"\"Ensures a datetime object has UTC timezone information.\n\n    If the provided datetime object is naive (lacks timezone info), it is assigned\n    the UTC timezone. If it already has a timezone, it is returned unchanged.\n\n    Parameters:\n        time (datetime): The datetime object to process.\n\n    Returns:\n        datetime: The datetime object with `timezone.utc` assigned.\n    \"\"\"\n    if time.tzinfo is None:\n        time = time.replace(tzinfo=timezone.utc)\n    return time\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.extract_version","title":"<code>extract_version</code>","text":"<p>Extracts the version string from a file name.</p> <p>The function looks for a version string pattern <code>_v*</code> (e.g., '_v1.2.3' or '_v1_2-3') located just before the file extension. It returns the base file name and a parsed version object. If no version is found, it returns the original file name and a default version '0'.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str | Path</code> <p>The name or path of the file.</p> required <p>Returns:</p> Type Description <code>tuple[str, Version]</code> <p>tuple[str, version_pkg.Version]: A tuple containing: - The base file name without the version string. - The parsed version object (<code>packaging.version.Version</code>).</p> Source code in <code>el_paso/utils.py</code> <pre><code>def extract_version(file_name:str|Path) -&gt; tuple[str, version_pkg.Version]:\n    \"\"\"Extracts the version string from a file name.\n\n    The function looks for a version string pattern `_v*` (e.g., '_v1.2.3' or '_v1_2-3')\n    located just before the file extension. It returns the base file name and a\n    parsed version object. If no version is found, it returns the original file name\n    and a default version '0'.\n\n    Parameters:\n        file_name (str | Path): The name or path of the file.\n\n    Returns:\n        tuple[str, version_pkg.Version]: A tuple containing:\n            - The base file name without the version string.\n            - The parsed version object (`packaging.version.Version`).\n    \"\"\"\n    # convert to str in case of Path object\n    file_name = str(file_name)\n\n    # Regular expression to find the version part (_v* or _v*.*-*.*) before the file extension\n    match = re.search(r\"_(v[\\d._-]+)(?=\\.\\w+$)\", file_name)\n    if match:\n        base_name = file_name[:match.start()]\n        ver_str = match.group(1)\n        # Normalize the version string by replacing separators with dots\n        normalized_ver_str = re.sub(r\"[_-]\", \".\", ver_str.replace(\"v\", \"\"))\n        return base_name, version_pkg.parse(normalized_ver_str)\n    else:\n        return file_name, version_pkg.parse(\"0\")\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.fill_str_template_with_time","title":"<code>fill_str_template_with_time</code>","text":"<p>Fills a string template with time-based placeholders.</p> <p>This function replaces common time-based placeholders in a string with the corresponding values from a <code>datetime</code> object. The placeholders are case-sensitive.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input string containing placeholders like 'yyyymmdd', 'YYYYMMDD',          'YYYY', 'MM', and 'DD'.</p> required <code>time</code> <code>datetime</code> <p>The datetime object to use for filling the template.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string with all placeholders replaced by their time values.</p> Source code in <code>el_paso/utils.py</code> <pre><code>def fill_str_template_with_time(input:str, time:datetime) -&gt; str:\n    \"\"\"Fills a string template with time-based placeholders.\n\n    This function replaces common time-based placeholders in a string with\n    the corresponding values from a `datetime` object. The placeholders\n    are case-sensitive.\n\n    Parameters:\n        input (str): The input string containing placeholders like 'yyyymmdd', 'YYYYMMDD',\n                     'YYYY', 'MM', and 'DD'.\n        time (datetime): The datetime object to use for filling the template.\n\n    Returns:\n        str: The string with all placeholders replaced by their time values.\n    \"\"\"\n    yyyymmdd_str = time.strftime('%Y%m%d')\n    yyyy_str = time.strftime('%Y')\n    MM_str = time.strftime('%m')\n    DD_str = time.strftime('%d')\n\n    return input.replace(\"yyyymmdd\", yyyymmdd_str) \\\n                 .replace(\"YYYYMMDD\", yyyymmdd_str) \\\n                 .replace(\"YYYY\", yyyy_str) \\\n                 .replace(\"MM\", MM_str) \\\n                 .replace(\"DD\", DD_str)\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.get_file_by_version","title":"<code>get_file_by_version</code>","text":"<p>Filters a list of file paths to find a specific version or the latest one.</p> <p>If a specific version string (e.g., 'v1.2.3') is provided, the function returns the file that matches exactly. If the <code>version</code> parameter is 'latest', it returns the file with the highest version number among all provided file paths.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>Iterable[T]</code> <p>An iterable of file paths (as strings or <code>Path</code> objects).</p> required <code>version</code> <code>str</code> <p>The specific version string to match (e.g., 'v1.2.3') or 'latest'            to retrieve the most recent version.</p> required <p>Returns:</p> Type Description <code>T | None</code> <p>T | None: The file path that matches the criteria, or <code>None</code> if no matching       file is found.</p> Source code in <code>el_paso/utils.py</code> <pre><code>def get_file_by_version(file_paths:Iterable[T], version:str) -&gt; T|None:\n    \"\"\"Filters a list of file paths to find a specific version or the latest one.\n\n    If a specific version string (e.g., 'v1.2.3') is provided, the function returns\n    the file that matches exactly. If the `version` parameter is 'latest', it\n    returns the file with the highest version number among all provided file paths.\n\n    Parameters:\n        file_paths (Iterable[T]): An iterable of file paths (as strings or `Path` objects).\n        version (str): The specific version string to match (e.g., 'v1.2.3') or 'latest'\n                       to retrieve the most recent version.\n\n    Returns:\n        T | None: The file path that matches the criteria, or `None` if no matching\n                  file is found.\n    \"\"\"\n    latest_file = None\n\n    if version != \"latest\":\n        normalized_version = re.sub(r\"[_-]\", \".\", version.replace(\"v\", \"\"))\n        target_version = version_pkg.parse(normalized_version)\n    else:\n        target_version = None\n\n    for file in file_paths:\n        _, ver_obj = extract_version(file)\n\n        # Check if the current file matches the target version if specified\n        if target_version and ver_obj == target_version:\n            return file\n\n        # If no specific version is targeted, find the highest version\n        if latest_file is None or ver_obj &gt; extract_version(latest_file)[1]:\n            latest_file = file\n\n    # Extract the file names from the dictionary\n    return latest_file\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.make_dict_hashable","title":"<code>make_dict_hashable</code>","text":"<p>Converts a standard dictionary into a hashable one.</p> <p>If the input is <code>None</code>, it is returned as is. Otherwise, a new <code>Hashabledict</code> instance is created and returned.</p> <p>Parameters:</p> Name Type Description Default <code>dict_input</code> <code>dict | None</code> <p>The dictionary to convert.</p> required <p>Returns:</p> Type Description <code>Hashabledict | None</code> <p>Hashabledict | None: The new hashable dictionary, or <code>None</code> if the input was <code>None</code>.</p> Source code in <code>el_paso/utils.py</code> <pre><code>def make_dict_hashable(dict_input:dict[Any,Any]|None) -&gt; Hashabledict|None:\n    \"\"\"Converts a standard dictionary into a hashable one.\n\n    If the input is `None`, it is returned as is. Otherwise, a new `Hashabledict`\n    instance is created and returned.\n\n    Parameters:\n        dict_input (dict | None): The dictionary to convert.\n\n    Returns:\n        Hashabledict | None: The new hashable dictionary, or `None` if the input was `None`.\n    \"\"\"\n    if dict_input is None:\n        return dict_input\n\n    return Hashabledict(dict_input)\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.show_process_bar_for_map_async","title":"<code>show_process_bar_for_map_async</code>","text":"<p>Displays a progress bar for a <code>multiprocessing.pool.MapResult</code> object.</p> <p>This function creates a <code>tqdm</code> progress bar that tracks the completion of a parallel map operation. It polls the <code>MapResult</code>'s internal state to update the progress bar until the operation is complete.</p> <p>Parameters:</p> Name Type Description Default <code>map_result</code> <code>MapResult</code> <p>The result object from <code>Pool.map_async()</code>.</p> required <code>chunksize</code> <code>int</code> <p>The chunk size used in the <code>map_async</code> call.</p> required Source code in <code>el_paso/utils.py</code> <pre><code>def show_process_bar_for_map_async(map_result:MapResult[Any], chunksize:int) -&gt; None:\n    \"\"\"Displays a progress bar for a `multiprocessing.pool.MapResult` object.\n\n    This function creates a `tqdm` progress bar that tracks the completion of\n    a parallel map operation. It polls the `MapResult`'s internal state to\n    update the progress bar until the operation is complete.\n\n    Parameters:\n        map_result (MapResult): The result object from `Pool.map_async()`.\n        chunksize (int): The chunk size used in the `map_async` call.\n    \"\"\"\n    init = typing.cast(\"int\", map_result._number_left) * chunksize  # type: ignore[reportUnknownMemberType] # noqa: SLF001\n    with tqdm.tqdm(total=init) as t:\n        while (True):\n            if map_result.ready():\n                break\n            t.n = (init-map_result._number_left*chunksize)  # type: ignore[reportUnknownMemberType] # noqa: SLF001\n            t.refresh()\n            time.sleep(1)\n</code></pre>"},{"location":"API_reference/utilities/general_utilities/#el_paso.utils.timed_function","title":"<code>timed_function</code>","text":"<p>A decorator that logs the execution time of a function.</p> <p>This decorator measures the time it takes for a decorated function to execute and logs the result to a logger at the INFO level. The log message can be prefixed with an optional function name.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str | None</code> <p>An optional name to use in the log message. If <code>None</code>,                     a generic message is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[[Callable[P, R]], Callable[P, R]]</code> <p>A decorator that wraps the target function with timing logic.</p> Source code in <code>el_paso/utils.py</code> <pre><code>def timed_function(func_name:str|None=None) -&gt; Callable[[Callable[P, R]], Callable[P, R]]:\n    \"\"\"A decorator that logs the execution time of a function.\n\n    This decorator measures the time it takes for a decorated function to execute\n    and logs the result to a logger at the INFO level. The log message can be\n    prefixed with an optional function name.\n\n    Parameters:\n        func_name (str | None): An optional name to use in the log message. If `None`,\n                                a generic message is used.\n\n    Returns:\n        Callable: A decorator that wraps the target function with timing logic.\n    \"\"\"\n    def timed_function_(f: Callable[P, R]) -&gt; Callable[P, R]:\n        @wraps(f)\n        def wrap(*args: P.args, **kwargs: P.kwargs) -&gt; R:\n            tic = timeit.default_timer()\n            result = f(*args, **kwargs)\n            toc = timeit.default_timer()\n            if func_name:\n                logger.info(f\"\\t\\t{func_name} finished in {toc-tic:0.3f} seconds\")\n            else:\n                logger.info(f\"\\t\\tFinished in {toc-tic:0.3f} seconds\")\n\n            return result\n        return wrap\n    return timed_function_\n</code></pre>"},{"location":"API_reference/utilities/load_indices_solar_wind_parameters/","title":"Loading Geomagnetic Indices and Solar Wind Parameters","text":""},{"location":"API_reference/utilities/load_indices_solar_wind_parameters/#el_paso.load_indices_solar_wind_parameters.load_indices_solar_wind_parameters","title":"<code>el_paso.load_indices_solar_wind_parameters.load_indices_solar_wind_parameters</code>","text":"<pre><code>load_indices_solar_wind_parameters\n</code></pre><pre><code>load_indices_solar_wind_parameters\n</code></pre> <p>Loads a variety of space weather indices and solar wind parameters.</p> <p>This function fetches and processes data for several common space weather and solar wind indices, including Kp, Dst, solar wind plasma properties, and Tsyganenko model parameters (G1, G2, G3, W_params).</p> <p>Data is downloaded and cached locally to a <code>.elpaso</code> directory in the user's home directory. The function can either return the data with its original timestamps or interpolate the data to a new set of timestamps provided by a <code>target_time_variable</code>.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>datetime</code> <p>The start time for the data retrieval.</p> required <code>end_time</code> <code>datetime</code> <p>The end time for the data retrieval.</p> required <code>requested_outputs</code> <code>Iterable[SW_Index]</code> <p>A list of space weather indices to load.                                     Supported values are defined by the <code>SW_Index</code> Literal.</p> required <code>target_time_variable</code> <code>Variable | None</code> <p>An optional <code>ep.Variable</code> containing the                                         target timestamps for interpolation. If <code>None</code>,                                         the raw data and its timestamps are returned.</p> <code>None</code> <code>w_parameter_method</code> <code>TsyWebsite | Calculation</code> <p>The method to use for obtaining                                                 W_params. 'TsyWebsite' fetches data from                                                 the Tsyganenko website (only available until 2023),                                                 while 'Calculation' computes them from other                                                 solar wind data. Defaults to 'Calculation'.</p> <code>'Calculation'</code> <p>Returns:</p> Type Description <code>dict[SW_Index, tuple[Variable, Variable]] | dict[SW_Index, Variable]</code> <p>dict[SW_Index, ep.Variable | tuple[ep.Variable, ep.Variable]]: A dictionary where each key                                                             is a requested index and the value                                                             is the corresponding <code>ep.Variable</code>                                                             object(s). If <code>target_time_variable</code>                                                             is provided, the value is a single                                                             <code>ep.Variable</code>. Otherwise, it is a                                                             tuple of <code>(data_variable, time_variable)</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>requested_outputs</code> is not an iterable of strings.</p> <code>OSError</code> <p>If the HOME environment variable is not set.</p> <code>ValueError</code> <p>If a requested output is not a supported <code>SW_Index</code> or if         an unsupported method is requested.</p> <code>FileNotFoundError</code> <p>If the data file from the Tsyganenko website is not found.</p> Source code in <code>el_paso/load_indices_solar_wind_parameters.py</code> <pre><code>def load_indices_solar_wind_parameters(start_time:datetime,  # noqa: C901, PLR0912, PLR0915\n                                       end_time:datetime,\n                                       requested_outputs:Iterable[SW_Index],\n                                       target_time_variable:ep.Variable|None=None,\n                                       *,\n                                       w_parameter_method:Literal[\"TsyWebsite\", \"Calculation\"] = \"Calculation\",\n                                    ) -&gt; dict[SW_Index, tuple[ep.Variable, ep.Variable]] | dict[SW_Index, ep.Variable]:\n    \"\"\"Loads a variety of space weather indices and solar wind parameters.\n\n    This function fetches and processes data for several common space weather\n    and solar wind indices, including Kp, Dst, solar wind plasma properties,\n    and Tsyganenko model parameters (G1, G2, G3, W_params).\n\n    Data is downloaded and cached locally to a `.elpaso` directory in the user's home\n    directory. The function can either return the data with its original timestamps\n    or interpolate the data to a new set of timestamps provided by a `target_time_variable`.\n\n    Parameters:\n        start_time (datetime): The start time for the data retrieval.\n        end_time (datetime): The end time for the data retrieval.\n        requested_outputs (Iterable[SW_Index]): A list of space weather indices to load.\n                                                Supported values are defined by the `SW_Index` Literal.\n        target_time_variable (ep.Variable | None): An optional `ep.Variable` containing the\n                                                    target timestamps for interpolation. If `None`,\n                                                    the raw data and its timestamps are returned.\n        w_parameter_method ('TsyWebsite' | 'Calculation'): The method to use for obtaining\n                                                            W_params. 'TsyWebsite' fetches data from\n                                                            the Tsyganenko website (only available until 2023),\n                                                            while 'Calculation' computes them from other\n                                                            solar wind data. Defaults to 'Calculation'.\n\n    Returns:\n        dict[SW_Index, ep.Variable | tuple[ep.Variable, ep.Variable]]: A dictionary where each key\n                                                                        is a requested index and the value\n                                                                        is the corresponding `ep.Variable`\n                                                                        object(s). If `target_time_variable`\n                                                                        is provided, the value is a single\n                                                                        `ep.Variable`. Otherwise, it is a\n                                                                        tuple of `(data_variable, time_variable)`.\n\n    Raises:\n        TypeError: If `requested_outputs` is not an iterable of strings.\n        OSError: If the HOME environment variable is not set.\n        ValueError: If a requested output is not a supported `SW_Index` or if\n                    an unsupported method is requested.\n        FileNotFoundError: If the data file from the Tsyganenko website is not found.\n    \"\"\"\n    start_time = enforce_utc_timezone(start_time)\n    end_time = enforce_utc_timezone(end_time)\n\n    if not isinstance(requested_outputs, list):\n        msg = \"requested_outputs must be a list of strings!\"\n        raise TypeError(msg)\n\n    result_dict:dict[SW_Index, tuple[ep.Variable, ep.Variable]] | dict[SW_Index, ep.Variable] = {}\n\n    home_path = os.getenv(\"HOME\")\n    if home_path is None:\n        msg = \"HOME environment variable is not set!\"\n        raise OSError(msg)\n\n    base_data_path = Path(home_path) / \".elpaso\"\n\n    for requested_output in requested_outputs:\n\n        match requested_output:\n\n            case \"Kp\":\n                kp_model_order = [swvo_io.kp.KpOMNI(base_data_path / \"OMNI_low_res\"), swvo_io.kp.KpNiemegk(base_data_path/\"KpNiemegk\")]\n                output_df = swvo_io.kp.read_kp_from_multiple_models(start_time, end_time, model_order=kp_model_order, download=True)\n\n                result = _create_variables_from_data_frame(output_df,\n                                                           \"kp\",\n                                                           u.dimensionless_unscaled,\n                                                           target_time_variable,\n                                                           \"previous\")\n\n            case \"Dst\":\n                output_df = swvo_io.dst.DSTOMNI(base_data_path / \"OMNI_low_res\").read(start_time, end_time, download=True)\n\n                result = _create_variables_from_data_frame(output_df, \"dst\", u.nT, target_time_variable, \"linear\")\n\n            case \"Pdyn\":\n                sw_model_order = [swvo_io.solar_wind.SWOMNI(base_data_path / \"OMNI_high_res\")]\n                output_df = swvo_io.solar_wind.read_solar_wind_from_multiple_models(start_time-timedelta(hours=1), end_time+timedelta(hours=1), model_order=sw_model_order, download=True)\n                output_df[\"pdyn\"] = output_df[\"pdyn\"].interpolate(method=\"spline\", order=3).ffill().bfill()\n\n                result = _create_variables_from_data_frame(output_df, \"pdyn\", u.nPa, target_time_variable, \"linear\")\n\n            case \"IMF_Bz\":\n                sw_model_order = [swvo_io.solar_wind.SWOMNI(base_data_path / \"OMNI_high_res\")]\n                # we request two additional hours for interpolation \n                output_df = swvo_io.solar_wind.read_solar_wind_from_multiple_models(start_time-timedelta(hours=1),\n                                                                                  end_time+timedelta(hours=1),\n                                                                                  model_order=sw_model_order,\n                                                                                  download=True)\n                output_df[\"bz_gsm\"] = output_df[\"bz_gsm\"].interpolate(method=\"spline\", order=3).ffill().bfill()\n\n                result = _create_variables_from_data_frame(output_df, \"bz_gsm\", u.nT, target_time_variable, \"linear\")\n\n            case \"IMF_By\":\n                # we request two additional hours for interpolation\n                sw_model_order = [swvo_io.solar_wind.SWOMNI(base_data_path / \"OMNI_high_res\")]\n                output_df = swvo_io.solar_wind.read_solar_wind_from_multiple_models(start_time-timedelta(hours=1),\n                                                                                  end_time+timedelta(hours=1),\n                                                                                  model_order=sw_model_order,\n                                                                                  download=True)\n                output_df[\"by_gsm\"] = output_df[\"by_gsm\"].interpolate(method=\"spline\", order=3).ffill().bfill()\n\n                result = _create_variables_from_data_frame(output_df, \"by_gsm\", u.nT, target_time_variable, \"linear\")\n\n            case \"SW_speed\":\n                # we request two additional hours for interpolation\n                sw_model_order = [swvo_io.solar_wind.SWOMNI(base_data_path / \"OMNI_high_res\")]\n                output_df = swvo_io.solar_wind.read_solar_wind_from_multiple_models(start_time-timedelta(hours=1),\n                                                                                  end_time+timedelta(hours=1),\n                                                                                  model_order=sw_model_order,\n                                                                                  download=True)\n                output_df[\"speed\"] = output_df[\"speed\"].interpolate(method=\"spline\", order=3).ffill().bfill()\n\n                result = _create_variables_from_data_frame(output_df,\n                                                           \"speed\",\n                                                           u.km * u.s**-1,\n                                                           target_time_variable,\n                                                           \"linear\")\n\n            case \"SW_density\":\n                # we request two additional hours for interpolation\n                sw_model_order = [swvo_io.solar_wind.SWOMNI(base_data_path / \"OMNI_high_res\")]\n                output_df = swvo_io.solar_wind.read_solar_wind_from_multiple_models(start_time-timedelta(hours=1),\n                                                                                  end_time+timedelta(hours=1),\n                                                                                  model_order=sw_model_order,\n                                                                                  download=True)\n                output_df[\"proton_density\"] = output_df[\"proton_density\"].interpolate(method=\"spline\", order=3)\n\n                result = _create_variables_from_data_frame(output_df,\n                                                           \"proton_density\",\n                                                           u.cm**-3,\n                                                           target_time_variable,\n                                                           \"linear\")\n\n            case \"G1\":\n\n                g1_var, time_var = _calculate_g1(start_time, end_time, target_time_variable)\n                result = (g1_var, time_var) if target_time_variable is None else g1_var\n\n            case \"G2\":\n\n                g2_var, time_var = _calculate_g2(start_time, end_time, target_time_variable)\n                result = (g2_var, time_var) if target_time_variable is None else g2_var\n\n            case \"G3\":\n\n                g3_var, time_var = _calculate_g3(start_time, end_time, target_time_variable)\n                result = (g3_var, time_var) if target_time_variable is None else g3_var\n\n            case \"W_params\":\n\n                w_var, time_var = _get_w_parameters(start_time,\n                                          end_time,\n                                          target_time_variable,\n                                          w_parameter_method)\n\n                result = (w_var, time_var) if target_time_variable is None else w_var\n\n            case _:\n                msg = f\"Requested invalid output: {requested_output}!\"\n                raise ValueError(msg)\n\n        result_dict[requested_output] = result  # type: ignore[reportArgumentType]\n\n    return result_dict\n</code></pre>"},{"location":"API_reference/utilities/release_mode/","title":"Release mode","text":"<p>For publishing a data product, a release mode ensures reproducibility by associating the product with a specific Git commit hash. This commit hash serves as a unique identifier for the exact version of the EL-PASO code used to generate the data.</p> <p>By linking the data to a precise point in the Git history, anyone can retrieve the identical code version and replicate the results, guaranteeing that the data product is traceable and verifiable.</p> <p>Before release mode can be activated, the EL-PASO repository must not contain any changes which are not commited to the repository.</p> <p>Under release mode, additional metadata of the author who processed the data is also storred as metadata.</p>"},{"location":"API_reference/utilities/release_mode/#el_paso.release_mode","title":"<code>el_paso.release_mode</code>","text":""},{"location":"API_reference/utilities/release_mode/#el_paso.release_mode.activate_release_mode","title":"<code>activate_release_mode</code>","text":"<p>Activates the package's release mode and validates the repository state.</p> <p>This function enables a special mode for data processing that records key information about the execution environment. It checks if the package is installed and validates that the Git repository is in a clean state (no uncommitted changes) unless <code>dirty_ok</code> is set to <code>True</code>. Upon successful activation, it stores metadata about the user, version, and commit hash. This information is appended to the metadata of any processed variables.</p> <p>Parameters:</p> Name Type Description Default <code>user_name</code> <code>str</code> <p>The name of the user activating release mode.</p> required <code>email_address</code> <code>str</code> <p>The email address of the user.</p> required <code>el_paso_repository_path</code> <code>str | Path</code> <p>The path to the EL-PASO Git repository.</p> required <code>dirty_ok</code> <code>bool</code> <p>If <code>True</code>, allows activation even if the repository has uncommitted changes. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>PackageNotFoundError</code> <p>If the 'el_paso' package is not found in the Python environment, indicating it's not properly installed.</p> <code>ValueError</code> <p>If the Git repository is not clean and <code>dirty_ok</code> is <code>False</code>.</p> Source code in <code>el_paso/release_mode.py</code> <pre><code>def activate_release_mode(user_name:str,\n                          email_address:str,\n                          el_paso_repository_path:str|Path,\n                          *,\n                          dirty_ok:bool=False) -&gt; None:\n    \"\"\"Activates the package's release mode and validates the repository state.\n\n    This function enables a special mode for data processing that records key\n    information about the execution environment. It checks if the package is\n    installed and validates that the Git repository is in a clean state (no\n    uncommitted changes) unless `dirty_ok` is set to `True`. Upon successful\n    activation, it stores metadata about the user, version, and commit hash.\n    This information is appended to the metadata of any processed variables.\n\n    Args:\n        user_name (str): The name of the user activating release mode.\n        email_address (str): The email address of the user.\n        el_paso_repository_path (str | Path): The path to the EL-PASO Git repository.\n        dirty_ok (bool, optional): If `True`, allows activation even if the\n            repository has uncommitted changes. Defaults to `False`.\n\n    Raises:\n        importlib.metadata.PackageNotFoundError: If the 'el_paso' package is not\n            found in the Python environment, indicating it's not properly installed.\n        ValueError: If the Git repository is not clean and `dirty_ok` is `False`.\n    \"\"\"\n    try:\n        el_paso_version = importlib.metadata.version(\"el_paso\")\n    except importlib.metadata.PackageNotFoundError:\n        logger.exception(\"EL-PASO has to be installed when release mode is used!\")\n        raise\n\n    el_paso_repo = git.Repo(el_paso_repository_path)\n    commit_hash = el_paso_repo.head.commit.hexsha\n\n    if el_paso_repo.is_dirty(index=True,\n                            working_tree=True,\n                            untracked_files=True):\n        if dirty_ok:\n            logger.warning(\"Dirty repository used for processing data in release mode!\")\n        else:\n            msg = \"Your EL-PASO repository contains changes! Please push your changes to process data in release mode!\"\n            raise ValueError(msg)\n\n    date_now = datetime.now(timezone.utc).now()\n    date_now_str = date_now.strftime(\"%d-%b-%Y\")\n\n    ep._release_msg = (  # noqa: SLF001 # type: ignore[Private]\n        f\"This variable was processed using the EL-PASO release mode on {date_now_str}.\\n\"\n        f\"User name: {user_name}, email address: {email_address}.\\n\"\n        f\"EL-PASO version: {el_paso_version}, git commit: {commit_hash}.\"\n    )\n\n    ep._release_mode = True  # noqa: SLF001 # type: ignore[Private]\n</code></pre>"},{"location":"API_reference/utilities/release_mode/#el_paso.release_mode.get_release_msg","title":"<code>get_release_msg</code>","text":"<p>Retrieves the message associated with the package's release mode.</p> <p>This message contains metadata about the execution environment, including user information, version, and commit hash, if release mode is active.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The release mode message if active, otherwise an empty string.</p> Source code in <code>el_paso/release_mode.py</code> <pre><code>def get_release_msg() -&gt; str:\n    \"\"\"Retrieves the message associated with the package's release mode.\n\n    This message contains metadata about the execution environment, including\n    user information, version, and commit hash, if release mode is active.\n\n    Returns:\n        str: The release mode message if active, otherwise an empty string.\n    \"\"\"\n    return ep._release_msg  # noqa: SLF001 # type: ignore[Private]\n</code></pre>"},{"location":"API_reference/utilities/release_mode/#el_paso.release_mode.is_in_release_mode","title":"<code>is_in_release_mode</code>","text":"<p>Checks if the package's release mode is currently active.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if release mode is active, <code>False</code> otherwise.</p> Source code in <code>el_paso/release_mode.py</code> <pre><code>def is_in_release_mode() -&gt; bool:\n    \"\"\"Checks if the package's release mode is currently active.\n\n    Returns:\n        bool: `True` if release mode is active, `False` otherwise.\n    \"\"\"\n    return ep._release_mode  # noqa: SLF001 # type: ignore[Private]\n</code></pre>"},{"location":"API_reference/utilities/scripts/","title":"Scripts","text":""},{"location":"API_reference/utilities/scripts/#scripts.inspect_cdf_file.inspect_cdf_file","title":"<code>scripts.inspect_cdf_file.inspect_cdf_file</code>","text":"<p>Prints a formatted table of metadata for all variables in a CDF file.</p> <p>This function opens a CDF (Common Data Format) file, retrieves key metadata for each variable, and presents it in a clear, human-readable table. The table includes the variable name, data type, units, data shape, fill value, and a description. This is useful for quickly understanding the contents and structure of a CDF file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the CDF file.</p> required <p>Raises:</p> Type Description <code>CDFError</code> <p>If the file is not a valid CDF file or an error occurs              while reading it.</p> Source code in <code>scripts/inspect_cdf_file.py</code> <pre><code>def inspect_cdf_file(file_path:str) -&gt; None:\n    \"\"\"Prints a formatted table of metadata for all variables in a CDF file.\n\n    This function opens a CDF (Common Data Format) file, retrieves key metadata\n    for each variable, and presents it in a clear, human-readable table. The table\n    includes the variable name, data type, units, data shape, fill value, and a\n    description. This is useful for quickly understanding the contents and\n    structure of a CDF file.\n\n    Parameters:\n        file_path (str): The path to the CDF file.\n\n    Raises:\n        cdflib.CDFError: If the file is not a valid CDF file or an error occurs\n                         while reading it.\n    \"\"\"\n    cdf_file = cdflib.CDF(file_path)\n\n    variable_names = cdf_file.cdf_info().zVariables\n\n    var_attrs_to_print = []\n\n    for var in variable_names:\n        var_attrs_full = cdf_file.varattsget(var)\n        vdr_info       = cdf_file.varinq(var)\n        var_data       = cdf_file.varget(var)\n\n        var_shape = var_data.shape  # type: ignore[reportAttributeAccessIssue]\n\n        units = var_attrs_full.get(\"UNITS\", \"\")\n\n        desc = var_attrs_full.get(\"CATDESC\", \"\")\n\n        fillvall = var_attrs_full.get(\"FILLVAL\", \"\")\n\n        data_type = vdr_info.Data_Type_Description\n\n        var_attrs_to_print.append([var, data_type, units, var_shape, fillvall, desc])\n\n    print(tabulate(var_attrs_to_print,  # noqa: T201\n                   headers=[\"Variable name\", \"Data Type\", \"Units\", \"Data Shape\", \"Fill value\", \"Description\"]))\n</code></pre>"},{"location":"API_reference/utilities/scripts/#scripts.submit_slurm_jobs.submit_slurm_jobs_in_chunks","title":"<code>scripts.submit_slurm_jobs.submit_slurm_jobs_in_chunks</code>","text":"<p>Submits HPC jobs in time-based chunks.</p> <p>This function divides a specified time range into smaller intervals (daily, monthly, or yearly) and submits a separate job for each interval to an HPC cluster using the <code>sbatch</code> command. It assumes a job script template named <code>job_script_template.sh</code> exists in the same directory. The chunk start and end times are passed to the job script as command-line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>start_time_str</code> <code>str</code> <p>The start of the time range, in a format parsable                   by <code>dateutil.parser</code>. Example: '2023-01-01T00:00:00'.</p> required <code>end_time_str</code> <code>str</code> <p>The end of the time range, in a format parsable                 by <code>dateutil.parser</code>. Example: '2023-03-31T23:59:59'.</p> required <code>chunk_type</code> <code>ChunkType</code> <p>The type of time chunk to use for job submission.                     Valid options are <code>ChunkType.DAILY</code>, <code>ChunkType.MONTHLY</code>,                     or <code>ChunkType.YEARLY</code>.</p> required <p>Raises:</p> Type Description <code>CalledProcessError</code> <p>If an <code>sbatch</code> command fails to execute                             with a non-zero exit code.</p> Source code in <code>scripts/submit_slurm_jobs.py</code> <pre><code>def submit_slurm_jobs_in_chunks(start_time_str:str,\n                          end_time_str:str,\n                          chunk_type:ChunkType,\n                          job_script_path:str = \"job_script_template.sh\") -&gt; None:\n    \"\"\"Submits HPC jobs in time-based chunks.\n\n    This function divides a specified time range into smaller intervals (daily,\n    monthly, or yearly) and submits a separate job for each interval to an HPC\n    cluster using the `sbatch` command. It assumes a job script template named\n    `job_script_template.sh` exists in the same directory. The chunk start and\n    end times are passed to the job script as command-line arguments.\n\n    Parameters:\n        start_time_str (str): The start of the time range, in a format parsable\n                              by `dateutil.parser`. Example: '2023-01-01T00:00:00'.\n        end_time_str (str): The end of the time range, in a format parsable\n                            by `dateutil.parser`. Example: '2023-03-31T23:59:59'.\n        chunk_type (ChunkType): The type of time chunk to use for job submission.\n                                Valid options are `ChunkType.DAILY`, `ChunkType.MONTHLY`,\n                                or `ChunkType.YEARLY`.\n\n    Raises:\n        subprocess.CalledProcessError: If an `sbatch` command fails to execute\n                                        with a non-zero exit code.\n    \"\"\"\n    # Convert string times to datetime objects\n    start_time = dateutil.parser.parse(start_time_str).replace(tzinfo=timezone.utc)\n    end_time = dateutil.parser.parse(end_time_str).replace(tzinfo=timezone.utc)\n\n    time_intervals = _get_time_intervals(start_time, end_time, chunk_type)\n\n    for (start_interval, end_interval) in time_intervals:\n\n        # Format the times for the command line arguments\n        chunk_start_str = start_interval.strftime(\"%Y-%m-%dT%H:%M:%S\")\n        chunk_end_str = end_interval.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n        print(f\"Submitting job for time range: {chunk_start_str} to {chunk_end_str}\")\n\n        # Construct the sbatch command\n        command = [\n            \"sbatch\",\n            job_script_path,\n            chunk_start_str,\n            chunk_end_str,\n        ]\n\n        try:\n            # Execute the sbatch command and check for errors\n            subprocess.run(command, check=True)  # noqa: S603\n        except subprocess.CalledProcessError as e:\n            print(f\"Error submitting job: {e}\")\n            break\n</code></pre>"},{"location":"API_reference/utilities/units/","title":"Units","text":"<p>EL-PASO defines custom astropy units for unit conversion. This is useful for converting between different time formats, as tt2000 and posixtime.</p> Unit name Description el_paso.units.cdf_epoch Time unit representing milliseconds since 0000-01-01. el_paso.units.tt2000 Time unit representing nanoseconds since 2000-01-01. el_paso.units.posixtime Time unit representing seconds since 1970-01-01. el_paso.units.datenum Time unit representing days since 0000-01-01, as used in MATLAB. el_paso.units.RE Unit of distance representing Earth radii. <p>There are eqivalences enabled for all custom units, meaning that conversion works as simple as:</p> <pre><code>from astropy import units as u\nimport el_paso as ep\n\nposixtime_q = u.Quantity(1362265200.0, ep.units.posixtime)\ncdf_tt2000_q = posixtime_q.to(ep.units.cdf_tt2000)\n</code></pre>"},{"location":"getting_started/basic_workflow/","title":"Basic Workflow","text":""},{"location":"getting_started/basic_workflow/#basic-workflow","title":"Basic Workflow","text":"<p>The workflow of EL-PASO consists of four main steps:</p> <pre><code>1. Download of raw data files\n2. Extract data from downloaded files and store it as 'Variables'\n3. Process the extracted variables and compute derived values\n4. Save variables in a pre-defined data standard and file format\n</code></pre>"},{"location":"getting_started/basic_workflow/#1-download-of-raw-data-files","title":"1. Download of raw data files","text":"<p>Raw data files are usually downloaded from an online repository hosted by an agency. This can be achieved by the internal download routine (download) or by using external tools such as SPEDAS or an API such as HAPI.</p> <p>The download using the EL-PASO routine can be setup using a few lines of code:</p> <p></p> <p>You can learn more about the EL-PASO download routine in tutorial #1 located in tutorials/1_download_data_and_extracting_variables.ipynb.</p> <p>Note</p> <p>The file containing all the code snippets is located at examples/minimal_example.py</p>"},{"location":"getting_started/basic_workflow/#2-extract-data-from-downloaded-files-and-store-it-as-variables","title":"2. Extract data from downloaded files and store it as 'Variables'","text":"<p>A Variable in EL-PASO is a custom class which holds a numpy-array as data and metadata (unit, processing notes, etc).</p> <p>We can turn the downloaded files into variables by calling the EL-PASO extract_variables_from_files routine.</p> <p>The user has to provide information about under which name or column the variable is storred and its unit:</p> <p></p> <p>You can learn more about the extraction of variables in tutorial #1 located in tutorials/1_download_data_and_extracting_variables.ipynb.</p>"},{"location":"getting_started/basic_workflow/#3-process-the-extracted-variables-and-compute-derived-values","title":"3. Process the extracted variables and compute derived values","text":"<p>The next step will consist of multiple processing steps, such as time binning or applying thresholds:</p> <p></p> <p>We can also calculate derived quantities such as adiabatic invariants (mu, K, L*). The calculations use the IRBEM library as a backend and runs parallelized:</p> <p></p> <p>You can learn more about processing in tutorials #4 and #5 located in tutorials.</p>"},{"location":"getting_started/basic_workflow/#4-save-variables-in-a-pre-defined-data-standard-and-file-format","title":"4. Save variables in a pre-defined data standard and file format","text":"<p>The last step saves the variables in a pre-defined format utizing a data standard.  </p> <p>The pre-defined format specifies under which names the variables are saved, if the variables are split up in multiple files, and if daily, monthly, or yearly files are created.</p> <p>The data standard describes the dimensions, units, and names of the data set. For example, the standard describes that a measurement of the unidirectional differential electron flux has always dimensions of (Time, energy, pitch angle), a unit of 1/(keV str s cm^2), and is called FEDU. In this example, we are applying the PRBEM standard.</p> <p></p> <p>You can learn more about different saving file formats and standars in tutorials #3 located in tutorials.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#installing-el-paso","title":"Installing EL-PASO","text":"<p>After cloning the repository, the main package can be installed using a virtual environment and pip. Make sure your current directory is set to the EL-PASO repository:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install .\n</code></pre>"},{"location":"getting_started/installation/#preparing-and-installing-irbem","title":"Preparing and installing IRBEM","text":"<p>The IRBEM library is the backend for magnetic field calculations. It is already included as a submodule and can be cloned by calling:</p> <pre><code>git submodule init\ngit submodule update\n</code></pre> <p>Now, the IRBEM repository has been cloned to the IRBEM folder. Next, we have to compile the IRBEM library. You can follow their compilation instructions at on github but on Linux, you should be able to simple call:</p> <pre><code>cd IRBEM\nmake\nmake install .\n</code></pre> <p>These commands compile the libirbem.so file and copy it to the root of the IRBEM directory.</p> <p>Before installing the python bindings, we have to apply a custom patch to the python wrapper, since some necessary functionalities are missing in the official IRBEM python wrapper. For this, copy the IRBEM.py file from the root of the EL-PASO repository to the IRBEM python wrapper:</p> <pre><code>cp ../IRBEM.py python/IRBEM\n</code></pre> <p>Now we can install the wrapper:</p> <pre><code>pip install python/\n</code></pre>"},{"location":"getting_started/installation/#validation-of-installation","title":"Validation of installation","text":"<p>You can validate your installation by running the minimal example located in examples:</p> <pre><code>python3 examples/minimal_example.py\n</code></pre>"},{"location":"getting_started/tutorials/","title":"Tutorials","text":"<p>The tutorials located in tutorials serve as a starting point for new users to get familiar with the workflow of EL-PASO.</p> <p>It is recommended to follow the numbering of files since the tutorials gradually build up one processing pipeline.</p> <p>Tutorials covering more advanced topics are marked as advanced. These contents should not be necessary for every-day-use.</p>"}]}